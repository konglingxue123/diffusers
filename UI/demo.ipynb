{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录为 /home/ubunto/Project/konglx/generate/diffusers/UI\n",
      "目录修改成功 /home/ubunto/Project/konglx/generate/diffusers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# 查看当前工作目录\n",
    "retval = os.getcwd()\n",
    "print(\"当前工作目录为 %s\" % retval)\n",
    "os.chdir('/home/ubunto/Project/konglx/generate/diffusers')\n",
    "\n",
    "# 查看修改后的工作目录\n",
    "retval = os.getcwd()\n",
    "\n",
    "print(\"目录修改成功 %s\" % retval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## only depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'annotator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# from pytorch_lightning import seed_everything\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mannotator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m resize_image, HWC3\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mannotator\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmidas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MidasDetector\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# from cldm.model import create_model, load_state_dict\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# from cldm.ddim_hacked import DDIMSampler\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'annotator'"
     ]
    }
   ],
   "source": [
    "# from share import *\n",
    "# import config\n",
    "\n",
    "import cv2\n",
    "import einops\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# from pytorch_lightning import seed_everything\n",
    "from annotator.util import resize_image, HWC3\n",
    "from annotator.midas import MidasDetector\n",
    "# from cldm.model import create_model, load_state_dict\n",
    "# from cldm.ddim_hacked import DDIMSampler\n",
    "from diffusers import DDPMPipeline, DDIMPipeline, DDIMScheduler\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import PIL\n",
    "\n",
    "######################\n",
    "# rough深度的onnx推理准备\n",
    "import onnxruntime as ort\n",
    "\n",
    "import cv2\n",
    "from typing import Tuple, Dict, List\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "onnx_model_dir=\"/home/ubunto/Project/konglx/mono_depth/Metric3D-main/onnx/metric3d-vit-small/onnx/model.onnx\"\n",
    "\n",
    "def prepare_input(\n",
    "    rgb_image: np.ndarray, input_size: Tuple[int, int]\n",
    ") -> Tuple[Dict[str, np.ndarray], List[int]]:\n",
    "\n",
    "    h, w = rgb_image.shape[:2] # 原图尺寸\n",
    "    scale = min(input_size[0] / h, input_size[1] / w)\n",
    "    rgb = cv2.resize(\n",
    "        rgb_image, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_LINEAR\n",
    "    )\n",
    "\n",
    "    padding = [123.675, 116.28, 103.53]\n",
    "    h, w = rgb.shape[:2]\n",
    "    pad_h = input_size[0] - h\n",
    "    pad_w = input_size[1] - w\n",
    "    pad_h_half = pad_h // 2  # h方向，每边padding的尺寸\n",
    "    pad_w_half = pad_w // 2  # w方向，每边padding的尺寸\n",
    "    rgb: np.ndarray = cv2.copyMakeBorder(\n",
    "        rgb,\n",
    "        pad_h_half,\n",
    "        pad_h - pad_h_half,\n",
    "        pad_w_half,\n",
    "        pad_w - pad_w_half,\n",
    "        cv2.BORDER_CONSTANT,\n",
    "        value=padding,\n",
    "    )\n",
    "    pad_info = [pad_h_half, pad_h - pad_h_half, pad_w_half, pad_w - pad_w_half]\n",
    "\n",
    "    onnx_input = {\n",
    "        # \"image\": np.ascontiguousarray(\n",
    "        \"pixel_values\": np.ascontiguousarray(\n",
    "            np.transpose(rgb, (2, 0, 1))[None], dtype=np.float32\n",
    "        ),  # 1, 3, H, W\n",
    "    }\n",
    "    return onnx_input, pad_info\n",
    "\n",
    "def onnx_inference(onnx_model_dir: str, input_image: np):\n",
    "\n",
    "    ## Dummy Test\n",
    "    B = 1\n",
    "    if \"vit\" in onnx_model_dir:\n",
    "        input_size = (616, 1064)  # [H, W]\n",
    "        if 'fp16' in onnx_model_dir:\n",
    "            dummy_image = np.zeros([B, 3, input_size[0], input_size[1]], dtype=np.float16)\n",
    "        else:\n",
    "            dummy_image = np.zeros([B, 3, input_size[0], input_size[1]], dtype=np.float32)\n",
    "    else:\n",
    "        input_size = (544, 1216)  # [H, W]\n",
    "        dummy_image = np.zeros([B, 3, input_size[0], input_size[1]], dtype=np.float32)\n",
    "\n",
    "    providers = [\n",
    "        (\n",
    "            \"CUDAExecutionProvider\",\n",
    "            {\"cudnn_conv_use_max_workspace\": \"0\", \"device_id\": str(0)},\n",
    "        )\n",
    "    ]\n",
    "    # providers = [(\"TensorrtExecutionProvider\", {'trt_engine_cache_enable': True, 'trt_fp16_enable': True, 'device_id': 0, 'trt_dla_enable': False})]\n",
    "    ort_session = ort.InferenceSession(onnx_model_dir, providers=providers)\n",
    "\n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "    out_name = ort_session.get_outputs()[0].name\n",
    "    print(input_name, out_name, sep='\\n')\n",
    "    # outputs = ort_session.run(None, {\"image\": dummy_image})\n",
    "    outputs = ort_session.run(None, {input_name: dummy_image})\n",
    "\n",
    "    print(\n",
    "        f\"The actual output of onnxruntime session for the dummy set: outputs[0].shape={outputs[0].shape}\"\n",
    "    )\n",
    "    # input_iamge = input_image\n",
    "    # print(np.unique(input_image))\n",
    "\n",
    "    original_shape = input_image.shape[:2]\n",
    "    onnx_input, pad_info = prepare_input(input_image, input_size)  # rgb_image是原图形状[512,512], input_size = (616, 1064)\n",
    "    outputs = ort_session.run(None, onnx_input)\n",
    "    depth = outputs[0].squeeze()  # [H, W]\n",
    "    # plt.imshow(depth)\n",
    "    # Reshape the depth to the original size\n",
    "    depth = depth[\n",
    "        pad_info[0] : input_size[0] - pad_info[1],\n",
    "        pad_info[2] : input_size[1] - pad_info[3],\n",
    "    ]\n",
    "    depth = cv2.resize(\n",
    "        depth, (original_shape[1], original_shape[0]), interpolation=cv2.INTER_LINEAR\n",
    "    )\n",
    "    return depth\n",
    "################################\n",
    "\n",
    "\n",
    "def show_image(img, detect_resolution=512):\n",
    "\n",
    "    # Convert To PIL Image\n",
    "    # image = Image.open(img)\n",
    "    # print(type(image))\n",
    "\n",
    "    # # Convert the image to a NumPy array\n",
    "    # image_array = np.array(image)\n",
    "    # print(type(image_array))\n",
    "    # input_image_np = (img.astype(np.float32) / 255.0) * 2 - 1\n",
    "    input_image_np = img\n",
    "\n",
    "    rgb_image =resize_image(input_image_np, detect_resolution)\n",
    "    tiff_np = onnx_inference(onnx_model_dir=onnx_model_dir, input_image=rgb_image)\n",
    "    vmin = np.percentile(tiff_np, 2)\n",
    "    vmax = np.percentile(tiff_np, 85)\n",
    "    # print(vmin, vmax)\n",
    "\n",
    "    tiff_np -= vmin\n",
    "    tiff_np /= vmax - vmin\n",
    "    tiff_np = 1.0 - tiff_np\n",
    "    tiff_np_uint8 = (tiff_np * 255.0).clip(0, 255).astype(np.uint8)\n",
    "    tiff_pil_uint8 = Image.fromarray(tiff_np_uint8)\n",
    "    # print(np.unique(tiff_np_uint8))\n",
    "    # print(type(tiff_np_uint8))\n",
    "\n",
    "    return tiff_np_uint8,tiff_pil_uint8\n",
    "\n",
    "\n",
    "inputs_np=gr.Image(type=\"numpy\")\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=show_image, \n",
    "    inputs=gr.Image(type=\"numpy\"), \n",
    "    outputs=[gr.Image(type=\"numpy\"),\n",
    "             gr.Image(type=\"pil\"),\n",
    "            ]\n",
    ")\n",
    "    \n",
    "# demo.launch(share=True)\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## depth + corrosion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前工作目录为 /home/ubunto/Project/konglx/generate/diffusers\n",
      "目录修改成功 /home/ubunto/Project/konglx/generate/diffusers\n",
      "['rgb(128, 0, 0)', 'rgb(0, 128, 0)', 'rgb(128, 128, 0)']\n",
      "<class 'gradio.components.radio.Radio'> <gradio.components.radio.Radio object at 0x7fb92c6e91f0>\n",
      "<class 'gradio.components.radio.Radio'> <gradio.components.radio.Radio object at 0x7fb92c6eb370>\n",
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from share import *\n",
    "# import config\n",
    "import imgviz\n",
    "import os\n",
    "# 查看当前工作目录\n",
    "retval = os.getcwd()\n",
    "print(\"当前工作目录为 %s\" % retval)\n",
    "os.chdir('/home/ubunto/Project/konglx/generate/diffusers')\n",
    "\n",
    "# 查看修改后的工作目录\n",
    "retval = os.getcwd()\n",
    "\n",
    "print(\"目录修改成功 %s\" % retval)\n",
    "\n",
    "\n",
    "import cv2\n",
    "import einops\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "# from pytorch_lightning import seed_everything\n",
    "from annotator.util import resize_image, HWC3\n",
    "from annotator.midas import MidasDetector\n",
    "# from cldm.model import create_model, load_state_dict\n",
    "# from cldm.ddim_hacked import DDIMSampler\n",
    "from diffusers import DDPMPipeline, DDIMPipeline, DDIMScheduler, DDPMScheduler\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import PIL\n",
    "\n",
    "######################\n",
    "# rough深度的onnx推理准备\n",
    "import onnxruntime as ort\n",
    "\n",
    "import cv2\n",
    "from typing import Tuple, Dict, List\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "onnx_model_dir=\"/home/ubunto/Project/konglx/mono_depth/Metric3D-main/onnx/metric3d-vit-small/onnx/model.onnx\"\n",
    "\n",
    "def prepare_input(\n",
    "    rgb_image: np.ndarray, input_size: Tuple[int, int]\n",
    ") -> Tuple[Dict[str, np.ndarray], List[int]]:\n",
    "\n",
    "    h, w = rgb_image.shape[:2] # 原图尺寸\n",
    "    scale = min(input_size[0] / h, input_size[1] / w)\n",
    "    rgb = cv2.resize(\n",
    "        rgb_image, (int(w * scale), int(h * scale)), interpolation=cv2.INTER_LINEAR\n",
    "    )\n",
    "\n",
    "    padding = [123.675, 116.28, 103.53]\n",
    "    h, w = rgb.shape[:2]\n",
    "    pad_h = input_size[0] - h\n",
    "    pad_w = input_size[1] - w\n",
    "    pad_h_half = pad_h // 2  # h方向，每边padding的尺寸\n",
    "    pad_w_half = pad_w // 2  # w方向，每边padding的尺寸\n",
    "    rgb: np.ndarray = cv2.copyMakeBorder(\n",
    "        rgb,\n",
    "        pad_h_half,\n",
    "        pad_h - pad_h_half,\n",
    "        pad_w_half,\n",
    "        pad_w - pad_w_half,\n",
    "        cv2.BORDER_CONSTANT,\n",
    "        value=padding,\n",
    "    )\n",
    "    pad_info = [pad_h_half, pad_h - pad_h_half, pad_w_half, pad_w - pad_w_half]\n",
    "\n",
    "    onnx_input = {\n",
    "        # \"image\": np.ascontiguousarray(\n",
    "        \"pixel_values\": np.ascontiguousarray(\n",
    "            np.transpose(rgb, (2, 0, 1))[None], dtype=np.float32\n",
    "        ),  # 1, 3, H, W\n",
    "    }\n",
    "    return onnx_input, pad_info\n",
    "\n",
    "def onnx_inference(onnx_model_dir: str, input_image: np):\n",
    "\n",
    "    ## Dummy Test\n",
    "    B = 1\n",
    "    if \"vit\" in onnx_model_dir:\n",
    "        input_size = (616, 1064)  # [H, W]\n",
    "        if 'fp16' in onnx_model_dir:\n",
    "            dummy_image = np.zeros([B, 3, input_size[0], input_size[1]], dtype=np.float16)\n",
    "        else:\n",
    "            dummy_image = np.zeros([B, 3, input_size[0], input_size[1]], dtype=np.float32)\n",
    "    else:\n",
    "        input_size = (544, 1216)  # [H, W]\n",
    "        dummy_image = np.zeros([B, 3, input_size[0], input_size[1]], dtype=np.float32)\n",
    "\n",
    "    providers = [\n",
    "        (\n",
    "            \"CUDAExecutionProvider\",\n",
    "            {\"cudnn_conv_use_max_workspace\": \"0\", \"device_id\": str(0)},\n",
    "        )\n",
    "    ]\n",
    "    # providers = [(\"TensorrtExecutionProvider\", {'trt_engine_cache_enable': True, 'trt_fp16_enable': True, 'device_id': 0, 'trt_dla_enable': False})]\n",
    "    ort_session = ort.InferenceSession(onnx_model_dir, providers=providers)\n",
    "\n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "    out_name = ort_session.get_outputs()[0].name\n",
    "    print(input_name, out_name, sep='\\n')\n",
    "    # outputs = ort_session.run(None, {\"image\": dummy_image})\n",
    "    outputs = ort_session.run(None, {input_name: dummy_image})\n",
    "\n",
    "    print(\n",
    "        f\"The actual output of onnxruntime session for the dummy set: outputs[0].shape={outputs[0].shape}\"\n",
    "    )\n",
    "    # input_iamge = input_image\n",
    "    # print(np.unique(input_image))\n",
    "\n",
    "    original_shape = input_image.shape[:2]\n",
    "    onnx_input, pad_info = prepare_input(input_image, input_size)  # rgb_image是原图形状[512,512], input_size = (616, 1064)\n",
    "    outputs = ort_session.run(None, onnx_input)\n",
    "    depth = outputs[0].squeeze()  # [H, W]\n",
    "    # plt.imshow(depth)\n",
    "    # Reshape the depth to the original size\n",
    "    depth = depth[\n",
    "        pad_info[0] : input_size[0] - pad_info[1],\n",
    "        pad_info[2] : input_size[1] - pad_info[3],\n",
    "    ]\n",
    "    depth = cv2.resize(\n",
    "        depth, (original_shape[1], original_shape[0]), interpolation=cv2.INTER_LINEAR\n",
    "    )\n",
    "    return depth\n",
    "################################\n",
    "\n",
    "######################\n",
    "#  controlnet模型准备\n",
    "def controlnet_diff(corrosion_with_bg_pil, mask_depth_pil, prompt = '',device = 'cuda',seed = 2024,use_freeu = False,controlnet_conditioning_scale_list=[1.0,1.0],\n",
    "                    config_dir = '/home/ubunto/Project/konglx/generate/ControlNet/models/stable-diffusion-v1-5',\n",
    "                    control_corrosion_with_bg_trained_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_corrosion_with_background_merge_inpainting_h-512_w-512_2024-07-29_13:19:18_seeds-2024/checkpoint-5700/controlnet',\n",
    "                    control_depth_trained_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_h-512_w-512_2024-07-16_17:42:27_seeds-2024/checkpoint-2000/controlnet',\n",
    "                    scheduler_select = 'PSNR',\n",
    "                    inference_steps=20,\n",
    "                    guidance_scale=7.5,\n",
    "                    ):\n",
    "    with torch.no_grad():\n",
    "        print('scheduler_select:', scheduler_select)\n",
    "        controlnet_corrosion_with_bg = ControlNetModel.from_pretrained(control_corrosion_with_bg_trained_dir)\n",
    "        controlnet_depth = ControlNetModel.from_pretrained(control_depth_trained_dir)\n",
    "        if scheduler_select == 'PSNR' or scheduler_select == None:\n",
    "            pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=[controlnet_corrosion_with_bg, controlnet_depth], safety_checker=None).to(device)\n",
    "        elif scheduler_select == 'DDPM':\n",
    "            # scheduler DDPM\n",
    "            scheduler = DDPMScheduler.from_pretrained(config_dir, subfolder=\"scheduler\")\n",
    "            pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=[controlnet_corrosion_with_bg, controlnet_depth], safety_checker=None, scheduler=scheduler).to(device)\n",
    "        elif scheduler_select == 'DDIM':\n",
    "            # scheduler DDIM\n",
    "            scheduler = DDIMScheduler.from_pretrained(config_dir, subfolder=\"scheduler\")\n",
    "            pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=[controlnet_corrosion_with_bg, controlnet_depth], safety_checker=None, scheduler=scheduler).to(device)\n",
    "        generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "        pipeline.unet.use_freeu = use_freeu \n",
    "        \n",
    "\n",
    "        mask_corrosion_with_bg_pil = load_image(corrosion_with_bg_pil)\n",
    "        mask_depth_pil = load_image(mask_depth_pil)\n",
    "        validation_image = [mask_corrosion_with_bg_pil, mask_depth_pil]\n",
    "        image = pipeline(prompt, validation_image, num_inference_steps=inference_steps, generator=generator, controlnet_conditioning_scale=controlnet_conditioning_scale_list,\n",
    "                        guidance_scale=guidance_scale,\n",
    "                            ).images[0]\n",
    "    torch.cuda.empty_cache()\n",
    "    return image\n",
    "#######################\n",
    "\n",
    "################################\n",
    "# gradio的功能接口定义\n",
    "def show_image(org_img, img, prompt='', image_resolution=512, scheduler_select='PSNR',\n",
    "               inference_steps=20, guidance_scale=7.5,seed=2024):\n",
    "\n",
    "    # Convert To PIL Image\n",
    "    # image = Image.open(img)\n",
    "    # print(type(image))\n",
    "\n",
    "    # # Convert the image to a NumPy array\n",
    "    # image_array = np.array(image)\n",
    "    # print(type(image_array))\n",
    "    # input_image_np = (img.astype(np.float32) / 255.0) * 2 - 1\n",
    "    # print('scheduler_select:', scheduler_select)\n",
    "    input_image_np = img\n",
    "    org_rgb_image =resize_image(org_img, image_resolution)\n",
    "    rgb_image =resize_image(input_image_np, image_resolution)\n",
    "    rgb_image_pil = Image.fromarray(rgb_image)\n",
    "    tiff_np = onnx_inference(onnx_model_dir=onnx_model_dir, input_image=org_rgb_image)\n",
    "    vmin = np.percentile(tiff_np, 2)\n",
    "    vmax = np.percentile(tiff_np, 85)\n",
    "    # print(vmin, vmax)\n",
    "\n",
    "    tiff_np -= vmin\n",
    "    tiff_np /= vmax - vmin\n",
    "    tiff_np = 1.0 - tiff_np\n",
    "    tiff_np_uint8 = (tiff_np * 255.0).clip(0, 255).astype(np.uint8)\n",
    "    tiff_pil_uint8 = Image.fromarray(tiff_np_uint8)\n",
    "    # print(np.unique(tiff_np_uint8))\n",
    "    # print(type(tiff_np_uint8))\n",
    "\n",
    "    out_control_diff_pil = controlnet_diff(corrosion_with_bg_pil=rgb_image_pil, mask_depth_pil=tiff_pil_uint8, \n",
    "                                           prompt=prompt,scheduler_select=scheduler_select, \n",
    "                                           inference_steps=inference_steps, guidance_scale=guidance_scale,\n",
    "                                           seed=seed,)\n",
    "    \n",
    "    return [tiff_pil_uint8,out_control_diff_pil]\n",
    "    # return tiff_pil_uint8,out_control_diff_pil\n",
    "\n",
    "\n",
    "def show_image_draw(org_img, img, prompt='', image_resolution=512, scheduler_select='PSNR',\n",
    "               inference_steps=20, guidance_scale=7.5,seed=2024):\n",
    "\n",
    "    # Convert To PIL Image\n",
    "    # image = Image.open(img)\n",
    "    # print(type(image))\n",
    "\n",
    "    # # Convert the image to a NumPy array\n",
    "    # image_array = np.array(image)\n",
    "    # print(type(image_array))\n",
    "    # input_image_np = (img.astype(np.float32) / 255.0) * 2 - 1\n",
    "    # print('scheduler_select:', scheduler_select)\n",
    "    input_image_np = img['composite']\n",
    "    org_rgb_image =resize_image(org_img, image_resolution)\n",
    "    rgb_image =resize_image(input_image_np, image_resolution)\n",
    "    rgb_image_pil = Image.fromarray(rgb_image)\n",
    "    tiff_np = onnx_inference(onnx_model_dir=onnx_model_dir, input_image=org_rgb_image)\n",
    "    vmin = np.percentile(tiff_np, 2)\n",
    "    vmax = np.percentile(tiff_np, 85)\n",
    "    # print(vmin, vmax)\n",
    "\n",
    "    tiff_np -= vmin\n",
    "    tiff_np /= vmax - vmin\n",
    "    tiff_np = 1.0 - tiff_np\n",
    "    tiff_np_uint8 = (tiff_np * 255.0).clip(0, 255).astype(np.uint8)\n",
    "    tiff_pil_uint8 = Image.fromarray(tiff_np_uint8)\n",
    "    # print(np.unique(tiff_np_uint8))\n",
    "    # print(type(tiff_np_uint8))\n",
    "\n",
    "    out_control_diff_pil = controlnet_diff(corrosion_with_bg_pil=rgb_image_pil, mask_depth_pil=tiff_pil_uint8, \n",
    "                                           prompt=prompt,scheduler_select=scheduler_select, \n",
    "                                           inference_steps=inference_steps, guidance_scale=guidance_scale,\n",
    "                                           seed=seed,)\n",
    "    \n",
    "    return [tiff_pil_uint8,out_control_diff_pil]\n",
    "    # return tiff_pil_uint8,out_control_diff_pil\n",
    "\n",
    "# 绘图\n",
    "def create_canvas(w, h):\n",
    "    return np.zeros(shape=(h, w, 3), dtype=np.uint8)\n",
    "\n",
    "\n",
    "\n",
    "brush_colors = gr.Brush()\n",
    "brush_colors.colors=[\n",
    "                \"rgb(128, 0, 0)\",\n",
    "                \"rgb(0, 128, 0)\",\n",
    "                \"rgb(128, 128, 0)\"]\n",
    "# print()\n",
    "print(brush_colors.colors\n",
    "                # format=\"png\",\n",
    "                )\n",
    "def colored_mask(mask, save_path=None):\n",
    "    lbl_pil = Image.fromarray(mask.astype(np.uint8), mode=\"P\")\n",
    "    colormap = imgviz.label_colormap()\n",
    "    # print(colormap)\n",
    "    lbl_pil.putpalette(colormap.flatten())\n",
    "    if save_path is not None:\n",
    "        lbl_pil.save(save_path)\n",
    "\n",
    "    return lbl_pil\n",
    "\n",
    "\n",
    "\n",
    "def sleep(im):\n",
    "    # time.sleep(5)\n",
    "    # return [im[\"background\"], im[\"layers\"][0], im[\"layers\"][1],im[\"composite\"]]\n",
    "    return [im[\"background\"], im[\"layers\"][0],im[\"composite\"]]\n",
    " \n",
    "def upim(im_in,im):\n",
    "    print(type(im_in), im_in.shape,im)#, type(im), im)\n",
    "    height, width, channels = im_in.shape[:3]\n",
    "    im[\"background\"]=im_in\n",
    "    im[\"layers\"][0]=np.zeros((height, width, 4), np.uint8)\n",
    "    #im[\"layers\"][1]=None\n",
    "\n",
    "    print(f\" shape:{im_in.shape}, ndim:{im_in.ndim}, size:{im_in.size}\")\n",
    "    return gr.ImageMask(value=im,height=600)\n",
    "\n",
    "\n",
    "def saveimg(im):\n",
    "    #print(im[\"layers\"][0])\n",
    "    background=im[\"background\"]\n",
    "    composite=im[\"composite\"]\n",
    "    background=cv2.cvtColor(background, cv2.COLOR_BGR2RGB )\n",
    "    composite=cv2.cvtColor(composite, cv2.COLOR_BGR2RGB )\n",
    "    img_layer = cv2.cvtColor(im[\"layers\"][0], cv2.COLOR_BGR2RGB)\n",
    "    cv2.imwrite(\"layers0.png\",img_layer)\n",
    "    cv2.imwrite(\"background.png\",background)\n",
    "    cv2.imwrite(\"composite.png\",composite)\n",
    "    return gr.ImageMask(value=im,height=600), background\n",
    "\n",
    "# inputs_np=gr.Image(type=\"numpy\")\n",
    "\n",
    "CACHE_EXAMPLES = torch.cuda.is_available() and os.getenv(\"CACHE_EXAMPLES\", \"0\") == \"1\"\n",
    "org_img_dir = '/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/corrosion_with_background_for_test/images/00003.jpg'\n",
    "masked_corrosion_with_bg_dir = '/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/corrosion_with_background_for_test/conditioning_images/00003.png'\n",
    "examples = [ [load_image(org_img_dir)]]\n",
    "examples_mask = [[load_image(masked_corrosion_with_bg_dir)]]\n",
    "\n",
    "MCD = gr.Blocks(title=\"Multi-Control corrosion generator\").queue()\n",
    "with MCD:\n",
    "    gr.Markdown('Multi-Control corrosion generator')\n",
    "    ###################################################  Tab 1  ###################################################\n",
    "    with gr.Tab('Use test images'):\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                org_image_pil = gr.Image(type=\"numpy\", label='Original image')\n",
    "                masked_corrosion_with_bg_pil = gr.Image(type=\"numpy\", label='Masked corrosion with background')\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                scheduler_select = gr.Radio(['PSNR', 'DDPM', 'DDIM'], label='Scheduler', info='Which scheduler to use for diffusion')\n",
    "                print(type(scheduler_select), scheduler_select)\n",
    "                with gr.Accordion(\"Advanced options\", open=False):\n",
    "                    prompt = gr.Textbox(label=\"Prompt\", value='')\n",
    "                    # num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n",
    "                    image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n",
    "                    strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n",
    "                    # guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n",
    "                    # detect_resolution = gr.Slider(label=\"Depth Resolution\", minimum=128, maximum=1024, value=384, step=1)\n",
    "                    inference_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n",
    "                    scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=7.5, step=0.1)\n",
    "                    seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=7000, step=1, randomize=False, value=2024)\n",
    "                    # eta = gr.Number(label=\"eta (DDIM)\", value=0.0)\n",
    "                    # a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality, extremely detailed')\n",
    "                    # n_prompt = gr.Textbox(label=\"Negative Prompt\",\n",
    "                    #                     value='longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality')\n",
    "                run_button = gr.Button()\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                result_gallery_depth = gr.Gallery(label='Output depth & Generated image', show_label=True, elem_id=\"gallery\", height='auto')\n",
    "                # gr.Interface(\n",
    "                # fn=show_image, \n",
    "                # inputs=[org_image_pil, ],  # 原图， \n",
    "                # outputs=[gr.Image(type=\"pil\", label='Depth inference'),\n",
    "                #         gr.Image(type=\"pil\", label='Generated image'),\n",
    "                #         ],\n",
    "                #         title=\"For test images\",)\n",
    "                # result_gallery_final = gr.Gallery(label='Generated image', show_label=False, elem_id=\"gallery\", height='auto')\n",
    "        inputs = [org_image_pil, masked_corrosion_with_bg_pil, prompt, image_resolution, scheduler_select, inference_steps,scale, seed]\n",
    "        run_button.click(fn=show_image, inputs=inputs, outputs=[result_gallery_depth])\n",
    "\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                \n",
    "                gr.Examples(\n",
    "                label='Example: Original image',\n",
    "                examples=examples,\n",
    "                inputs=inputs[0],\n",
    "                outputs=[result_gallery_depth],\n",
    "                fn=show_image,\n",
    "                cache_examples=CACHE_EXAMPLES,\n",
    "                )\n",
    "                \n",
    "                gr.Examples(\n",
    "                label='Example: Masked corrosion with background',\n",
    "                examples=examples_mask,\n",
    "                inputs=inputs[1],\n",
    "                outputs=[result_gallery_depth],\n",
    "                fn=show_image,\n",
    "                cache_examples=CACHE_EXAMPLES,\n",
    "                )\n",
    "    ###################################################  Tab 2  ###################################################           \n",
    "    with gr.Tab('Draw corrosion images'):\n",
    "        # with gr.Blocks():\n",
    "        with gr.Group():\n",
    "            with gr.Row():\n",
    "                org_img = gr.Image(type=\"numpy\", label=\"Original Image\")\n",
    "                \n",
    "                im = gr.ImageEditor(\n",
    "                    type=\"numpy\",\n",
    "                    crop_size=\"1:1\",\n",
    "                    height=512,\n",
    "                    width=512,\n",
    "                    brush=brush_colors,\n",
    "                    label='Draw',\n",
    "                    canvas_size=(512, 512),\n",
    "                    sources='upload',\n",
    "                )\n",
    "        \n",
    "        with gr.Row():\n",
    "            \n",
    "            uploadbtn = gr.Button(\"image upload\")\n",
    "            savebtn = gr.Button(\"save\")\n",
    "            \n",
    "        uploadbtn.click(upim,[org_img,im],[im])\n",
    "        \n",
    "        \n",
    "        # im =gr.Sketchpad(type=\"pil\",\n",
    "        #     crop_size=\"1:1\",)\n",
    "    \n",
    "        with gr.Group():\n",
    "            with gr.Row():\n",
    "                im_out_1 = gr.Image(type=\"numpy\", label='background')\n",
    "                im_out_2 = gr.Image(type=\"numpy\", label='mask')\n",
    "                im_out_3 = gr.Image(type=\"numpy\", label='mask with background')\n",
    "                # im_out_4 = gr.Image(type=\"numpy\")\n",
    "                im.change(sleep, outputs=[im_out_1, im_out_2, im_out_3], inputs=im)\n",
    "        savebtn.click(saveimg, outputs=[im, im_out_2], inputs=im)\n",
    "        # btn = gr.Button()\n",
    "\n",
    "        # with gr.Column():\n",
    "        #     with gr.Row():\n",
    "        #         org_image_pil = gr.Image(type=\"numpy\", label='Original image')\n",
    "        #         ##\n",
    "        #         draw = gr.ImageEditor(label='Draw corrosion', type=\"numpy\")#,height=image_resolution, width=image_resolution)\n",
    "        #         print(draw, type(draw))\n",
    "\n",
    "                \n",
    "                # masked_corrosion_with_bg_pil = gr.Image(type=\"numpy\", label='Masked corrosion with background')\n",
    "\n",
    "    # with gr.Tab('Use test images'):\n",
    "        # with gr.Column():\n",
    "        #     with gr.Row():\n",
    "                # org_image_pil = gr.Image(type=\"numpy\", label='Original image')\n",
    "                # masked_corrosion_with_bg_pil = gr.Image(type=\"numpy\", label='Masked corrosion with background')\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                scheduler_select = gr.Radio(['PSNR', 'DDPM', 'DDIM'], label='Scheduler', info='Which scheduler to use for diffusion')\n",
    "                print(type(scheduler_select), scheduler_select)\n",
    "                with gr.Accordion(\"Advanced options\", open=False):\n",
    "                    prompt = gr.Textbox(label=\"Prompt\", value='')\n",
    "                    # num_samples = gr.Slider(label=\"Images\", minimum=1, maximum=12, value=1, step=1)\n",
    "                    image_resolution = gr.Slider(label=\"Image Resolution\", minimum=256, maximum=768, value=512, step=64)\n",
    "                    strength = gr.Slider(label=\"Control Strength\", minimum=0.0, maximum=2.0, value=1.0, step=0.01)\n",
    "                    # guess_mode = gr.Checkbox(label='Guess Mode', value=False)\n",
    "                    # detect_resolution = gr.Slider(label=\"Depth Resolution\", minimum=128, maximum=1024, value=384, step=1)\n",
    "                    inference_steps = gr.Slider(label=\"Steps\", minimum=1, maximum=100, value=20, step=1)\n",
    "                    scale = gr.Slider(label=\"Guidance Scale\", minimum=0.1, maximum=30.0, value=7.5, step=0.1)\n",
    "                    seed = gr.Slider(label=\"Seed\", minimum=-1, maximum=7000, step=1, randomize=False, value=2024)\n",
    "                    # eta = gr.Number(label=\"eta (DDIM)\", value=0.0)\n",
    "                    # a_prompt = gr.Textbox(label=\"Added Prompt\", value='best quality, extremely detailed')\n",
    "                    # n_prompt = gr.Textbox(label=\"Negative Prompt\",\n",
    "                    #                     value='longbody, lowres, bad anatomy, bad hands, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality')\n",
    "                run_button = gr.Button()\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                result_gallery_depth = gr.Gallery(label='Output depth & Generated image', show_label=True, elem_id=\"gallery\", height='auto')\n",
    "                # gr.Interface(\n",
    "                # fn=show_image, \n",
    "                # inputs=[org_image_pil, ],  # 原图， \n",
    "                # outputs=[gr.Image(type=\"pil\", label='Depth inference'),\n",
    "                #         gr.Image(type=\"pil\", label='Generated image'),\n",
    "                #         ],\n",
    "                #         title=\"For test images\",)\n",
    "                # result_gallery_final = gr.Gallery(label='Generated image', show_label=False, elem_id=\"gallery\", height='auto')\n",
    "        inputs = [org_img, im, prompt, image_resolution, scheduler_select, inference_steps,scale, seed]\n",
    "        run_button.click(fn=show_image_draw, inputs=inputs, outputs=[result_gallery_depth])\n",
    "\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                \n",
    "                gr.Examples(\n",
    "                label='Example: Original image',\n",
    "                examples=examples,\n",
    "                inputs=org_img,\n",
    "                outputs=[result_gallery_depth],\n",
    "                fn=show_image_draw,\n",
    "                cache_examples=CACHE_EXAMPLES,\n",
    "                )\n",
    "                \n",
    "                # gr.Examples(\n",
    "                # label='Example: Masked corrosion with background',\n",
    "                # examples=examples_mask,\n",
    "                # inputs=inputs[1],\n",
    "                # outputs=[result_gallery_depth],\n",
    "                # fn=show_image,\n",
    "                # cache_examples=CACHE_EXAMPLES,\n",
    "                # )\n",
    "        ###################################################  Tab 2  ###################################################           \n",
    "    with gr.Tab('Find members then draw corrosion images'):\n",
    "         # with gr.Blocks():\n",
    "        with gr.Group():\n",
    "            with gr.Row():\n",
    "                org_img = gr.Image(type=\"numpy\", label=\"Original Image\")\n",
    "# MCD.launch(share=True)\n",
    "MCD.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['rgb(128, 0, 0)', 'rgb(0, 128, 0)', 'rgb(128, 128, 0)']\n",
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> {'background': array([[[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]]], dtype=uint8), 'layers': [array([[[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]]], dtype=uint8)], 'composite': array([[[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]],\n",
      "\n",
      "       [[0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0],\n",
      "        [0, 0, 0, 0]]], dtype=uint8)}\n",
      " shape:(512, 512, 3), ndim:3, size:786432\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "# from gradio import Brush\n",
    "import time\n",
    "import imgviz\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "brush_colors = gr.Brush()\n",
    "brush_colors.colors=[\n",
    "                \"rgb(128, 0, 0)\",\n",
    "                \"rgb(0, 128, 0)\",\n",
    "                \"rgb(128, 128, 0)\"]\n",
    "# print()\n",
    "print(brush_colors.colors\n",
    "                # format=\"png\",\n",
    "                )\n",
    "def colored_mask(mask, save_path=None):\n",
    "    lbl_pil = Image.fromarray(mask.astype(np.uint8), mode=\"P\")\n",
    "    colormap = imgviz.label_colormap()\n",
    "    # print(colormap)\n",
    "    lbl_pil.putpalette(colormap.flatten())\n",
    "    if save_path is not None:\n",
    "        lbl_pil.save(save_path)\n",
    "\n",
    "    return lbl_pil\n",
    "\n",
    "\n",
    "\n",
    "def sleep(im):\n",
    "    # time.sleep(5)\n",
    "    # return [im[\"background\"], im[\"layers\"][0], im[\"layers\"][1],im[\"composite\"]]\n",
    "    return [im[\"background\"], im[\"layers\"][0],im[\"composite\"]]\n",
    " \n",
    "def upim(im_in,im):\n",
    "    print(type(im_in), im)\n",
    "    height, width, channels = im_in.shape[:3]\n",
    "    im[\"background\"]=im_in\n",
    "    im[\"layers\"][0]=np.zeros((height, width, 4), np.uint8)\n",
    "    #im[\"layers\"][1]=None\n",
    "\n",
    "    print(f\" shape:{im_in.shape}, ndim:{im_in.ndim}, size:{im_in.size}\")\n",
    "    return gr.ImageMask(value=im,height=600)\n",
    "\n",
    "\n",
    "def saveimg(im):\n",
    "    #print(im[\"layers\"][0])\n",
    "    background=im[\"background\"]\n",
    "    composite=im[\"composite\"]\n",
    "    background=cv2.cvtColor(background, cv2.COLOR_BGR2RGB )\n",
    "    composite=cv2.cvtColor(composite, cv2.COLOR_BGR2RGB )\n",
    "    img_layer = cv2.cvtColor(im[\"layers\"][0], cv2.COLOR_BGR2RGB)\n",
    "    cv2.imwrite(\"layers0.png\",img_layer)\n",
    "    cv2.imwrite(\"background.png\",background)\n",
    "    cv2.imwrite(\"composite.png\",composite)\n",
    "    return gr.ImageMask(value=im,height=600), background\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Column():\n",
    "        with gr.Row():\n",
    "            org_img = gr.Image(type=\"numpy\", label=\"Original Image\")\n",
    "            \n",
    "            im = gr.ImageEditor(\n",
    "                type=\"numpy\",\n",
    "                crop_size=\"1:1\",\n",
    "                height=512,\n",
    "                width=512,\n",
    "                brush=brush_colors,\n",
    "            )\n",
    "    \n",
    "    with gr.Row():\n",
    "        \n",
    "        uploadbtn = gr.Button(\"image upload\")\n",
    "        savebtn = gr.Button(\"save\")\n",
    "        uploadbtn.click(upim,[org_img,im],[im])\n",
    "    \n",
    "    # im =gr.Sketchpad(type=\"pil\",\n",
    "    #     crop_size=\"1:1\",)\n",
    " \n",
    "    with gr.Column():\n",
    "        with gr.Row():\n",
    "            im_out_1 = gr.Image(type=\"numpy\")\n",
    "            im_out_2 = gr.Image(type=\"numpy\")\n",
    "            im_out_3 = gr.Image(type=\"numpy\")\n",
    "            # im_out_4 = gr.Image(type=\"numpy\")\n",
    " \n",
    "    btn = gr.Button()\n",
    "    im.change(sleep, outputs=[im_out_1, im_out_2, im_out_3], inputs=im)\n",
    "    savebtn.click(saveimg, outputs=[im, im_out_2], inputs=im)\n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubunto/software/miniconda3/envs/dif/lib/python3.9/site-packages/gradio/utils.py:1002: UserWarning: Expected 1 arguments for function <function <lambda> at 0x7f65941664c0>, received 0.\n",
      "  warnings.warn(\n",
      "/home/ubunto/software/miniconda3/envs/dif/lib/python3.9/site-packages/gradio/utils.py:1006: UserWarning: Expected at least 1 arguments for function <function <lambda> at 0x7f65941664c0>, received 0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"1024\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubunto/software/miniconda3/envs/dif/lib/python3.9/site-packages/gradio/helpers.py:978: UserWarning: Unexpected argument. Filling with None.\n",
      "  warnings.warn(\"Unexpected argument. Filling with None.\")\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "\n",
    "brush_colors = gr.Brush()\n",
    "brush_colors.colors=[\n",
    "                \"rgb(128, 0, 0)\",\n",
    "                \"rgb(0, 128, 0)\",\n",
    "                \"rgb(128, 128, 0)\"]\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    img = gr.ImageEditor(\n",
    "        brush=gr.Brush(\n",
    "            # default_color=\"rgb(200, 200, 200)\",\n",
    "            colors=brush_colors.colors,\n",
    "            color_mode=\"fixed\",\n",
    "\n",
    "        ),\n",
    "        interactive=True,\n",
    "        height=512,\n",
    "        width=512,\n",
    "    )\n",
    "\n",
    "    change_color = gr.Button(\n",
    "        value=\"Change color\",\n",
    "    )\n",
    "\n",
    "    change_color.click(\n",
    "        fn=lambda brush_colors:  gr.update(\n",
    "            brush=brush_colors\n",
    "        ),\n",
    "        inputs=None,\n",
    "        outputs=img,\n",
    "    )\n",
    "    \n",
    "    # change_color.click(\n",
    "    #     fn=lambda: gr.update(\n",
    "    #         brush=gr.Brush(colors=[\"rgb(128, 0, 0)\", \"rgb(200, 200, 255)\"], color_mode=\"fixed\")\n",
    "    #     ),\n",
    "    #     inputs=None,\n",
    "    #     outputs=img,\n",
    "    # )\n",
    "\n",
    "demo.launch(height=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0  50 204]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "layer_dir = '/home/ubunto/Project/konglx/generate/diffusers/UI/layers0.png'\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the layer image\n",
    "layer_img = Image.open(layer_dir)\n",
    "\n",
    "# Convert the image to a numpy array\n",
    "layer_arr = np.array(layer_img)\n",
    "print(np.unique(layer_arr))\n",
    "# Plot the image\n",
    "plt.imshow(layer_arr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:8044\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:8044/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      " shape:(512, 512, 3), ndim:3, size:786432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/ubunto/software/miniconda3/envs/dif/lib/python3.9/site-packages/gradio/queueing.py\", line 536, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/home/ubunto/software/miniconda3/envs/dif/lib/python3.9/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/home/ubunto/software/miniconda3/envs/dif/lib/python3.9/site-packages/gradio/blocks.py\", line 1935, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/home/ubunto/software/miniconda3/envs/dif/lib/python3.9/site-packages/gradio/blocks.py\", line 1520, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/home/ubunto/software/miniconda3/envs/dif/lib/python3.9/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/home/ubunto/software/miniconda3/envs/dif/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 2177, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/home/ubunto/software/miniconda3/envs/dif/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 859, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/home/ubunto/software/miniconda3/envs/dif/lib/python3.9/site-packages/gradio/utils.py\", line 826, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_3253328/2631065541.py\", line 22, in loadimg\n",
      "    background=cv2.cvtColor(background, cv2.COLOR_RGB2BGR )\n",
      "cv2.error: OpenCV(4.5.2) /tmp/pip-req-build-dccdjyga/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def saveimg(im):\n",
    "    #print(im[\"layers\"][0])\n",
    "    background=im[\"background\"]\n",
    "    composite=im[\"composite\"]\n",
    "    background=cv2.cvtColor(background, cv2.COLOR_BGR2RGB )\n",
    "    composite=cv2.cvtColor(composite, cv2.COLOR_BGR2RGB )\n",
    "    cv2.imwrite(\"layers0.png\",im[\"layers\"][0])\n",
    "    cv2.imwrite(\"background.png\",background)\n",
    "    cv2.imwrite(\"composite.png\",composite)\n",
    "    return gr.ImageMask(value=im,height=600), background\n",
    "\n",
    "def loadimg(im):\n",
    "    layers0=cv2.imread(\"layers0.png\",-1)\n",
    "    background=cv2.imread(\"background.png\")\n",
    "    composite=cv2.imread(\"composite.png\")\n",
    "    #layers0=cv2.cvtColor(layers0, cv2.COLOR_RGB2BGR, cv2.IMREAD_UNCHANGED )\n",
    "    background=cv2.cvtColor(background, cv2.COLOR_RGB2BGR )\n",
    "    composite=cv2.cvtColor(composite, cv2.COLOR_RGB2BGR )\n",
    "    \n",
    "    im[\"layers\"][0]=layers0\n",
    "    im[\"background\"]=background\n",
    "    im[\"composite\"]=composite\n",
    "    return gr.ImageMask(value=im,height=600), background\n",
    "\n",
    "def upim(im_in,im):\n",
    "    print(type(im_in))\n",
    "    height, width, channels = im_in.shape[:3]\n",
    "    im[\"background\"]=im_in\n",
    "    im[\"layers\"][0]=np.zeros((height, width, 4), np.uint8)\n",
    "    #im[\"layers\"][1]=None\n",
    "\n",
    "    print(f\" shape:{im_in.shape}, ndim:{im_in.ndim}, size:{im_in.size}\")\n",
    "    return gr.ImageMask(value=im,height=600)\n",
    "def getmask(im):\n",
    "    im.save(\"i.png\")\n",
    "    #dst = Image.new('RGBA', (im.width + o_img.width, im.height))\n",
    "    \n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Tab(\"カウント\"):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                im_in = gr.Image(sources=\"upload\")\n",
    "                uploadbtn = gr.Button(\"image upload\")\n",
    "            with gr.Column():    \n",
    "                with gr.Row():\n",
    "                    im = gr.ImageMask(height=600)\n",
    "                with gr.Row():\n",
    "                    savebtn = gr.Button(\"save\")\n",
    "                    loadbtn = gr.Button(\"load\")\n",
    "                with gr.Accordion():\n",
    "                    with gr.Row():\n",
    "                        im2 = gr.Image(height=600)\n",
    "            uploadbtn.click(upim,[im_in,im],[im])\n",
    "            savebtn.click(saveimg, outputs=[im, im2], inputs=im)\n",
    "            \n",
    "            loadbtn.click(loadimg, outputs=[im, im2], inputs=im)\n",
    "    with gr.Tab(\"症例一覧\"):\n",
    "        with gr.Row():\n",
    "            im3 = gr.Image(height=600)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.queue()\n",
    "    demo.launch(server_name=\"0.0.0.0\",server_port=8044, root_path=\"/8044\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
