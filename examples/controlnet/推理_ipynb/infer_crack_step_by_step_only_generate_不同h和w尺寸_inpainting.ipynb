{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***##step by step denoise##***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler, ControlNetModel\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from diffusers import StableDiffusionControlNetPipeline, StableDiffusionControlNetInpaintPipeline, UniPCMultistepScheduler, DDIMScheduler\n",
    "from diffusers.utils import load_image\n",
    "from torchvision import transforms\n",
    "import imgviz\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"crack\"]\n",
    "device = torch_device= 'cuda'\n",
    "seed = 1\n",
    "height = 512  # default height of Stable Diffusion\n",
    "width = 512  # default width of Stable Diffusion\n",
    "num_inference_steps = 25  # Number of denoising steps\n",
    "guidance_scale = 7.5  # Scale for classifier-free guidance\n",
    "generator = torch.Generator(device=device).manual_seed(seed)  # Seed generator to create the initial latent noise\n",
    "batch_size = len(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.图像准备，原图，image_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.检测和定位损伤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img 想增加损伤的图片\n",
    "img_dir = '/home/ubunto/Project/konglx/generate/ControlNet-v1-1-nightly/training/origin_dataset/open_datasets_element_cls_open/JPEGImages/cls_a1206.jpg'\n",
    "image_org = load_image(img_dir)\n",
    "print(image_org.mode)\n",
    "image_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img 检测的segmentation 选择添加损伤的构件\n",
    "img_seg_dir = '/home/ubunto/Project/konglx/generate/ControlNet-v1-1-nightly/training/origin_dataset/open_datasets_element_cls_open/SegmentationClass/cls_a1206.png'\n",
    "image_seg = Image.open(img_seg_dir)\n",
    "print(image_seg.mode)\n",
    "image_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colored_mask(mask, save_path=None):\n",
    "    lbl_pil = Image.fromarray(mask.astype(np.uint8), mode=\"P\")\n",
    "    colormap = imgviz.label_colormap()\n",
    "    # print(colormap)\n",
    "    lbl_pil.putpalette(colormap.flatten())\n",
    "    if save_path is not None:\n",
    "        lbl_pil.save(save_path)\n",
    "\n",
    "    return lbl_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取对应部位的mask单体\n",
    "image_seg_np = np.array(image_seg)\n",
    "image_seg_labels = np.unique(image_seg_np)\n",
    "\n",
    "list_labels = []\n",
    "for label in image_seg_labels:\n",
    "    if label == 0:\n",
    "        continue\n",
    "    label_zero = np.zeros_like(image_seg_np)\n",
    "    label_zero[image_seg_np == label] = label\n",
    "    list_labels.append(label_zero)\n",
    "print(len(list_labels))\n",
    "\n",
    "labels_unique_list = []\n",
    "for i in list_labels:\n",
    "    labels_unique_list.append(int(np.unique(i)[1]))\n",
    "print(labels_unique_list)\n",
    "\n",
    "print(labels_unique_list.index(6))\n",
    "label_np = list_labels[labels_unique_list.index(6)]\n",
    "label_type = int(np.unique(label_np)[1])\n",
    "print(label_type,type(label_type))\n",
    "print(type(label_np), label_np.shape, np.unique(label_np))\n",
    "label_pil = colored_mask(label_np)\n",
    "print(label_np[200][200])\n",
    "label_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximum_internal_rectangle_input_np(mask_np):\n",
    "    # img = cv2.imread(mask_path)\n",
    "    # img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "    # ret, img_bin = cv2.threshold(img_gray, 0, 255, cv2.THRESH_BINARY)\n",
    "    mask_type = int(np.unique(mask_np)[1])\n",
    " \n",
    "    contours, _ = cv2.findContours(mask_np, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # print(len(contours))\n",
    " \n",
    "    contour = contours[0].reshape(len(contours[0]), 2)\n",
    "    # print(contour,len(contour))\n",
    " \n",
    "    rect = []\n",
    " \n",
    "    for i in range(len(contour)):\n",
    "        x1, y1 = contour[i]\n",
    "        for j in range(len(contour)):\n",
    "            x2, y2 = contour[j]\n",
    "            area = abs(y2 - y1) * abs(x2 - x1)\n",
    "            rect.append(((x1, y1), (x2, y2), area))\n",
    " \n",
    "    all_rect = sorted(rect, key=lambda x: x[2], reverse=True)\n",
    " \n",
    "    if all_rect:\n",
    "        best_rect_found = False\n",
    "        index_rect = 0\n",
    "        nb_rect = len(all_rect)\n",
    " \n",
    "        while not best_rect_found and index_rect < nb_rect:\n",
    " \n",
    "            rect = all_rect[index_rect]\n",
    "            (x1, y1) = rect[0]\n",
    "            (x2, y2) = rect[1]\n",
    " \n",
    "            valid_rect = True\n",
    " \n",
    "            x = min(x1, x2)\n",
    "            while x < max(x1, x2) + 1 and valid_rect:\n",
    "                if mask_np[y1][x] == 0 or mask_np[y2][x] == 0:\n",
    "                    valid_rect = False\n",
    "                x += 1\n",
    " \n",
    "            y = min(y1, y2)\n",
    "            while y < max(y1, y2) + 1 and valid_rect:\n",
    "                if mask_np[y][x1] == 0 or mask_np[y][x2] == 0:\n",
    "                    valid_rect = False\n",
    "                y += 1\n",
    " \n",
    "            if valid_rect:\n",
    "                best_rect_found = True\n",
    " \n",
    "            index_rect += 1\n",
    " \n",
    "        if best_rect_found:\n",
    "            # 如果要在灰度图img_gray上画矩形，请用黑色画（0,0,0）\n",
    "            cv2.rectangle(mask_np, (x1, y1), (x2, y2), (0, 0, 0), 2)\n",
    "            # cv2.imshow(\"rec\", mask_np)\n",
    "            # cv2.waitKey(0)\n",
    "            plt.imshow(mask_np)\n",
    "            list_xy = list([(x1, y1), (x2, y2)])\n",
    "            return mask_np, list_xy\n",
    " \n",
    "        else:\n",
    "            print(\"No rectangle fitting into the area\")\n",
    " \n",
    "    else:\n",
    "        print(\"No rectangle found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_np_max, xy_list = maximum_internal_rectangle_input_np(label_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 座标点和尺寸面积, h_mask, w_mask\n",
    "w_mask_org = abs(xy_list[0][0]-xy_list[1][0])\n",
    "w_mask = w_mask_org if w_mask_org % 8 ==0 else 8*(w_mask_org//8)\n",
    "\n",
    "h_mask_org = abs(xy_list[0][1]-xy_list[1][1])\n",
    "h_mask = h_mask_org if h_mask_org % 8 ==0 else 8*(h_mask_org//8)\n",
    "\n",
    "xy_list, abs((xy_list[0][0]-xy_list[1][0])*(xy_list[0][1]-xy_list[1][1])), h_mask, w_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize裂缝区域为1\n",
    "mask_one = np.ones((h_mask, w_mask), dtype=np.uint8)*255\n",
    "# resize_crack_mask_np = np.array(mask_copy_cvt_l.resize((abs(xy_list[0][0]-xy_list[1][0]), abs(xy_list[0][1]-xy_list[1][1])), Image.NEAREST))\n",
    "print(np.unique(mask_one))\n",
    "mask_one_pil = Image.fromarray(mask_one, mode='L')\n",
    "mask_one_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 先创建zero的mask与原裂缝的mask尺寸一致，再将裂缝的mask对应到xy_list的座标点位置，\n",
    "\n",
    "new_crack_mask_np = np.zeros_like(label_np)\n",
    "\n",
    "mask_one_pil_zero = Image.fromarray(new_crack_mask_np)\n",
    "mask_one_pil_zero.paste(mask_one_pil, (xy_list[0][0], xy_list[1][1]))\n",
    "mask_one_pt_l = transforms.ToTensor()(mask_one_pil_zero)\n",
    "mask_one_pt_l_4dim = mask_one_pt_l.unsqueeze(0)\n",
    "print(mask_one_pt_l_4dim.shape, mask_one_pt_l_4dim.min(), mask_one_pt_l_4dim.max())  ###################################### mask_one_pt_l_4dim  #########################\n",
    "\n",
    "mask_one_pil_zero_rgb = mask_one_pil_zero.convert('RGB')\n",
    "mask_one_zero_rgb_pt = transforms.ToTensor()(mask_one_pil_zero_rgb)\n",
    "print(mask_one_zero_rgb_pt.shape) \n",
    "mask_only_mask_pt_4dim = mask_one_zero_rgb_pt.unsqueeze(0) \n",
    "print(mask_only_mask_pt_4dim.shape)   ###################################### mask_only_mask_pt_4dim  #########################（c=3）先不用\n",
    "mask_one_pil_np_paste = np.array(mask_one_pil_zero)\n",
    "print(mask_one_pil_np_paste.shape, np.unique(mask_one_pil_np_paste))\n",
    "mask_one_pil_rgb_np_paste = np.array(mask_one_pil_zero_rgb)\n",
    "print(mask_one_pil_rgb_np_paste.shape, np.unique(mask_one_pil_rgb_np_paste))\n",
    "print(mask_one_pil_zero.mode)\n",
    "mask_one_pil_zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "mask_dir = '/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/crack（复件）/SegmentationClass/DeepCrack_11240-6.png'\n",
    "validation_image = Image.open(mask_dir).convert(\"RGB\")\n",
    "validation_image.size\n",
    "\n",
    "conditioning_image_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize([height, width], interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop([height, width]),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "conditioning_image_pil = validation_image.resize([w_mask, h_mask])\n",
    "conditioning_pixel_values = torch.stack([conditioning_image_transforms(conditioning_image_pil)])\n",
    "conditioning_pixel_values = conditioning_pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "print(conditioning_pixel_values.shape)\n",
    "controlnet_image = conditioning_pixel_values.to(torch_device)\n",
    "print(controlnet_image.shape)\n",
    "print(conditioning_image_pil.mode)\n",
    "conditioning_image_pil_l = conditioning_image_pil.convert('L')\n",
    "print(conditioning_image_pil.mode)\n",
    "print(conditioning_image_pil_l.mode)\n",
    "conditioning_image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## crack_mask和原图组合\n",
    "# new_crack_mask_np = np.zeros_like(label_np)\n",
    "\n",
    "mask_crack_pil_zero = Image.fromarray(new_crack_mask_np)\n",
    "mask_crack_pil_zero.paste(conditioning_image_pil_l, (xy_list[0][0], xy_list[1][1]))\n",
    "mask_crack_pil_zero_rgb = mask_crack_pil_zero.convert('RGB')\n",
    "mask_crack_rgb_pt = transforms.ToTensor()(mask_crack_pil_zero_rgb)\n",
    "print(mask_crack_rgb_pt.shape)\n",
    "mask_crack_pil_zero_rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 组合init_image 和 选中的mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_image_pt = transforms.ToTensor()(image_org)\n",
    "print(init_image_pt.shape)\n",
    "init_image_pt_4dim = init_image_pt.unsqueeze(0)   #####################  init_image_pt_4dim  #########################\n",
    "print(init_image_pt_4dim.shape)\n",
    "mask_pt = transforms.ToTensor()(mask_one_pil_zero_rgb)\n",
    "print(mask_pt.shape)\n",
    "masked_image = init_image_pt * (mask_pt < 0.5)\n",
    "print(masked_image.shape)\n",
    "transforms.ToPILImage()(masked_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 组合init_image和crack_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_crack_image = init_image_pt * (mask_crack_rgb_pt<0.01)\n",
    "print(masked_crack_image.shape)\n",
    "transforms.ToPILImage()(masked_crack_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 组合init_image和crack_mask_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_image = image_org.copy()\n",
    "control_image.mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "control_image.paste(conditioning_image_pil, (xy_list[0][0], xy_list[1][1]))\n",
    "control_image_pt = transforms.ToTensor()(control_image)\n",
    "print(control_image_pt.shape)           \n",
    "control_image_pt_4_dim = control_image_pt.unsqueeze(0) ###############  control_image_pt_4_dim  #######################\n",
    "control_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.创建各个部分的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# control_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_crack_only_generate/checkpoint-2000/controlnet'\n",
    "control_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_inpainting_h-256_w-256_2024-07-05_09:14:29_seeds-2024/checkpoint-2000/controlnet'\n",
    "## 使用原来最初的config:\n",
    "# config_dir = '/home/ubunto/Project/konglx/generate/ControlNet/models/stable-diffusion-v1-5'\n",
    "## 使用inpainting的config:\n",
    "config_dir = '/home/ubunto/Project/konglx/generate/diffusers/pretrained/stable-diffusion-inpainting'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from diffusers import AutoPipelineForInpainting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(config_dir, subfolder='vae', use_safetensors=None)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(config_dir, subfolder='tokenizer')\n",
    "text_encoder = CLIPTextModel.from_pretrained(config_dir, subfolder='text_encoder', use_safetensors=None)\n",
    "# unet = UNet2DConditionModel.from_pretrained(config_dir, subfolder='unet', use_safetensors=None)\n",
    "unet = UNet2DConditionModel.from_pretrained(config_dir, subfolder='unet', use_safetensors=None, \n",
    "                                            ignore_mismatched_sizes=True, low_cpu_mem_usage=False)\n",
    "## ignore_mismatched_sizes=True, low_cpu_mem_usage=False ：这两个在in_channels与默认的4不同时，需要加上\n",
    "controlnet = ControlNetModel.from_pretrained(control_dir)\n",
    "controlnet.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UniPCMultistepScheduler\n",
    "\n",
    "scheduler_multistep = UniPCMultistepScheduler.from_pretrained(config_dir, subfolder=\"scheduler\")\n",
    "# scheduler_multistep.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "scheduler_ddpm = DDPMScheduler.from_pretrained(config_dir, subfolder='scheduler')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = \"cuda\"\n",
    "vae.to(torch_device)\n",
    "text_encoder.to(torch_device)\n",
    "unet.to(torch_device)\n",
    "controlnet.to(torch_device)\n",
    "print('')\n",
    "# unet.config\n",
    "unet.config.in_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Create embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tokenizer(\n",
    "    prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "prompt_embeds = text_embeddings\n",
    "encoder_hidden_states_control = text_embeddings\n",
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You’ll also need to generate the unconditional text embeddings which are the embeddings for the padding token.\n",
    "These need to have the same shape (batch_size and seq_length) as the conditional text_embeddings:\n",
    "'''\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "uncond_embeddings.shape, text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s concatenate the conditional and unconditional embeddings into a batch to avoid doing two forward passes:\n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "# text_embeddings\n",
    "text_embeddings.shape, text_embeddings.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create image embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 control_image的embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import PIL\n",
    "\n",
    "def pil_to_numpy(images: Union[List[PIL.Image.Image], PIL.Image.Image]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a PIL image or a list of PIL images to NumPy arrays.\n",
    "    \"\"\"\n",
    "    if not isinstance(images, list):\n",
    "        images = [images]\n",
    "    images = [np.array(image).astype(np.float32) / 255.0 for image in images]\n",
    "    images = np.stack(images, axis=0)\n",
    "\n",
    "    return images\n",
    "\n",
    "def numpy_to_pt(images: np.ndarray) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert a NumPy image to a PyTorch tensor.\n",
    "    \"\"\"\n",
    "    if images.ndim == 3:\n",
    "        images = images[..., None]\n",
    "\n",
    "    images = torch.from_numpy(images.transpose(0, 3, 1, 2))\n",
    "    return images\n",
    "\n",
    "def pt_to_numpy(images: torch.Tensor) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a PyTorch tensor to a NumPy image.\n",
    "    \"\"\"\n",
    "    images = images.cpu().permute(0, 2, 3, 1).float().numpy()\n",
    "    return images\n",
    "\n",
    "def img_normalize(images: torch.Tensor) -> torch.Tensor: # [-1, 1]\n",
    "    return 2*images - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_dir = '/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/corrosion_and_crack_dataset/target/11247-12.jpg'\n",
    "# org_image = Image.open(image_dir)\n",
    "# org_image_pt = transforms.ToTensor()(org_image)\n",
    "# print(org_image_pt.shape, org_image_pt.min(), org_image_pt.max())\n",
    "\n",
    "# org_image_norm_pt = img_normalize(org_image_pt)\n",
    "# print(org_image_norm_pt.shape, org_image_norm_pt.min(), org_image_norm_pt.max())\n",
    "# org_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conditioning_image_pil_l.mode)\n",
    "conditioning_image_pil_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 如文献:https://onlinelibrary.wiley.com/doi/epdf/10.1111/mice.13171 的相加\n",
    "# x0 = org_image_norm_pt # [3, 512, 512]\n",
    "# mask_crack = transforms.ToTensor()(conditioning_image_pil_l)\n",
    "# mask_crack_norm = img_normalize(mask_crack)\n",
    "# # add_x0_\n",
    "# # mask_crack.min(), mask_crack.max()\n",
    "# # add_x0_mask = x0 + mask_crack\n",
    "# print(x0.min(), x0.max(), x0.mean(), x0.std())\n",
    "# print(mask_crack.min(), mask_crack.max(), mask_crack.mean(), mask_crack.std())\n",
    "# print(mask_crack_norm.min(), mask_crack_norm.max(), mask_crack_norm.mean(), mask_crack_norm.std())\n",
    "\n",
    "# add_x0_mask = x0 + mask_crack\n",
    "# # add_x0_mask = x0 + mask_crack_norm\n",
    "# add_norm = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(add_x0_mask)\n",
    "# print(add_x0_mask.shape, add_norm.min(), add_norm.max(), add_norm.mean(), add_norm.std())\n",
    "# transforms.ToPILImage()(add_x0_mask)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. masked_image的embeddings(暂不考虑)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_image_pt = transforms.ToTensor()(image_org)\n",
    "print(init_image_pt.shape)\n",
    "mask_pt = transforms.ToTensor()(mask_one_pil_zero_rgb)\n",
    "print(mask_pt.shape)\n",
    "masked_image_pt = init_image_pt * (mask_pt < 0.5)\n",
    "print(masked_image_pt.shape)\n",
    "transforms.ToPILImage()(masked_image_pt)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare control image\n",
    "def make_inpaint_condition(init_image, mask_image):\n",
    "    init_image = np.array(init_image.convert(\"RGB\")).astype(np.float32) / 255.0\n",
    "    mask_image = np.array(mask_image.convert(\"L\")).astype(np.float32) / 255.0\n",
    "\n",
    "    assert init_image.shape[0:1] == mask_image.shape[0:1], \"image and image_mask must have the same image size\"\n",
    "    init_image[mask_image > 0.5] = -1.0  # set as masked pixel\n",
    "    init_image = np.expand_dims(init_image, 0).transpose(0, 3, 1, 2)\n",
    "    init_image = torch.from_numpy(init_image)\n",
    "    return init_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_inpaint_pt = make_inpaint_condition(init_image=image_org, mask_image=mask_one_pil_zero_rgb)\n",
    "print(mask_inpaint_pt.shape)\n",
    "transforms.ToPILImage()(mask_inpaint_pt.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_img2img.retrieve_latents\n",
    "def retrieve_latents(\n",
    "    encoder_output: torch.Tensor, generator: Optional[torch.Generator] = None, sample_mode: str = \"sample\"\n",
    "):\n",
    "    if hasattr(encoder_output, \"latent_dist\") and sample_mode == \"sample\":\n",
    "        return encoder_output.latent_dist.sample(generator)\n",
    "    elif hasattr(encoder_output, \"latent_dist\") and sample_mode == \"argmax\":\n",
    "        return encoder_output.latent_dist.mode()\n",
    "    elif hasattr(encoder_output, \"latents\"):\n",
    "        return encoder_output.latents\n",
    "    else:\n",
    "        raise AttributeError(\"Could not access latents of provided encoder_output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_image_latents = retrieve_latents(vae.encode(masked_image_pt.to(device=device)), generator=generator)\n",
    "# masked_image_latents.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Create random noise (如果是*无inpainting*，dim=1的shape为unet.config.in_channels=4;如果是*有inpainting*，dim=1的shape=vae.config._in_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 *有inpainting*，dim=1的shape=vae.config._in_channels=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.config.scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_inpaint.StableDiffusionInpaintPipeline._encode_vae_image\n",
    "def _encode_vae_image(image: torch.Tensor, generator: torch.Generator, vae=vae):\n",
    "    if isinstance(generator, list):\n",
    "        image_latents = [\n",
    "            retrieve_latents(vae.encode(image[i : i + 1]), generator=generator[i])\n",
    "            for i in range(image.shape[0])\n",
    "        ]\n",
    "        image_latents = torch.cat(image_latents, dim=0)\n",
    "    else:\n",
    "        image_latents = retrieve_latents(vae.encode(image), generator=generator)\n",
    "\n",
    "    image_latents = vae.config.scaling_factor * image_latents\n",
    "\n",
    "    return image_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_inpaint.StableDiffusionInpaintPipeline.prepare_mask_latents\n",
    "def prepare_mask_latents(\n",
    "    mask, masked_image, height, width, dtype, device, generator, encode_vae_image, batch_size=1, do_classifier_free_guidance=False\n",
    "):\n",
    "    # resize the mask to latents shape as we concatenate the mask to the latents\n",
    "    # we do that before converting to dtype to avoid breaking in case we're using cpu_offload\n",
    "    # and half precision\n",
    "    mask = torch.nn.functional.interpolate(\n",
    "        mask, size=(height // 8, width // 8)\n",
    "    )\n",
    "    mask = mask.to(device=device, dtype=dtype)\n",
    "\n",
    "    masked_image = masked_image.to(device=device, dtype=dtype)\n",
    "\n",
    "    if masked_image.shape[1] == 4:\n",
    "        masked_image_latents = masked_image\n",
    "    else:\n",
    "        masked_image_latents = encode_vae_image(masked_image, generator=generator)\n",
    "\n",
    "    # duplicate mask and masked_image_latents for each generation per prompt, using mps friendly method\n",
    "    if mask.shape[0] < batch_size:\n",
    "        if not batch_size % mask.shape[0] == 0:\n",
    "            raise ValueError(\n",
    "                \"The passed mask and the required batch size don't match. Masks are supposed to be duplicated to\"\n",
    "                f\" a total batch size of {batch_size}, but {mask.shape[0]} masks were passed. Make sure the number\"\n",
    "                \" of masks that you pass is divisible by the total requested batch size.\"\n",
    "            )\n",
    "        mask = mask.repeat(batch_size // mask.shape[0], 1, 1, 1)\n",
    "    if masked_image_latents.shape[0] < batch_size:\n",
    "        if not batch_size % masked_image_latents.shape[0] == 0:\n",
    "            raise ValueError(\n",
    "                \"The passed images and the required batch size don't match. Images are supposed to be duplicated\"\n",
    "                f\" to a total batch size of {batch_size}, but {masked_image_latents.shape[0]} images were passed.\"\n",
    "                \" Make sure the number of images that you pass is divisible by the total requested batch size.\"\n",
    "            )\n",
    "        masked_image_latents = masked_image_latents.repeat(batch_size // masked_image_latents.shape[0], 1, 1, 1)\n",
    "\n",
    "    mask = torch.cat([mask] * 2) if do_classifier_free_guidance else mask\n",
    "    masked_image_latents = (\n",
    "        torch.cat([masked_image_latents] * 2) if do_classifier_free_guidance else masked_image_latents\n",
    "    )\n",
    "\n",
    "    # aligning device to prevent device errors when concating it with the latent model input\n",
    "    masked_image_latents = masked_image_latents.to(device=device, dtype=dtype)\n",
    "    return mask, masked_image_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_latent, masked_image_latent = prepare_mask_latents(mask=mask_one_pt_l_4dim, masked_image=control_image_pt_4_dim, \n",
    "                                                        height=mask_only_mask_pt_4dim.shape[2], width=mask_only_mask_pt_4dim.shape[3], \n",
    "                                                        dtype=text_embeddings.dtype, device=device, generator=generator, encode_vae_image=_encode_vae_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_latent.shape, masked_image_latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latents_vae = torch.randn(\n",
    "    (batch_size, vae.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    "    device=torch_device,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "latents_vae.shape, vae.config.in_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. *无inpainting*，dim=1的shape为unet.config.in_channels=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Next, generate some initial random noise as a starting point for the diffusion process. \n",
    "This is the latent representation of the image, and it’ll be gradually denoised. \n",
    "At this point, the latent image is smaller than the final image size but that’s okay though \n",
    "because the model will transform it into the final 512x512 image dimensions later.\n",
    "'''\n",
    "\n",
    "# The height and width are divided by 8 because the vae model has 3 down-sampling layers.\n",
    "# You can check by running the following:   2 ** (len(vae.config.block_out_channels) - 1) == 8\n",
    "\n",
    "# do_classifier_free_guidance = False\n",
    "# guess_mode = False\n",
    "\n",
    "latents = torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    "    device=torch_device,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "#                     latents,\n",
    "#                     2,\n",
    "#                     encoder_hidden_states=encoder_hidden_states_control,\n",
    "#                     controlnet_cond=controlnet_image,\n",
    "#                     return_dict=False,\n",
    "#                 )\n",
    "# len(down_block_res_samples), down_block_res_samples[-1].shape, len(mid_block_res_sample), mid_block_res_sample[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Denoise the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Start by scaling the input with the initial noise distribution, sigma, the noise scale value, \n",
    "which is required for improved schedulers like UniPCMultistepScheduler: \n",
    "'''\n",
    "print(scheduler_multistep.init_noise_sigma)\n",
    "latents = latents * scheduler_multistep.init_noise_sigma\n",
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "def display_sample(sample, i):\n",
    "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
    "    image_processed = (image_processed + 1.0) * 127.5\n",
    "    image_processed = image_processed.numpy().astype(np.uint8)\n",
    "\n",
    "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
    "    display(f\"Image at step {i}\")\n",
    "    display(image_pil)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_latents(latents, vae):\n",
    "    # scale and decode the image latents with vae\n",
    "    latents_scaled = 1 / vae.config.scaling_factor * latents\n",
    "    # pil_latents_scaled = transforms.ToPILImage()(latents.squeeze(0))\n",
    "    # pil_latents_scaled = transforms.ToPILImage()(latents_scaled.squeeze(0))\n",
    "    with torch.no_grad():\n",
    "        # image = vae.decode(latents).sample\n",
    "        image = vae.decode(latents_scaled).sample\n",
    "    image = (image / 2 + 0.5).clamp(0, 1).squeeze()\n",
    "    image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n",
    "    image = Image.fromarray(image)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_hidden_states_control.shape, text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  ***controlnet***\n",
    "The last step is to create the denoising loop that’ll progressively \n",
    "transform the pure noise in latents to an image described by your prompt.\n",
    "Remember, the denoising loop needs to do three things:\n",
    "\n",
    "1.Set the scheduler’s timesteps to use during denoising.\n",
    "2.Iterate over the timesteps.\n",
    "3.At each timestep, call the UNet model to predict the noise residual and \n",
    "pass it to the scheduler to compute the previous noisy sample.\n",
    "\n",
    "'''\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def denoise_control_sd_v15(latents, num_inference_steps=25, scheduler = scheduler_multistep, controlnet = controlnet,\n",
    "                           unet = unet, text_embeddings=text_embeddings, guidance_scale=5,\n",
    "                           controlnet_image=controlnet_image, encoder_hidden_states=encoder_hidden_states_control,\n",
    "                           weight_dtype = torch.float32):\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "  \n",
    "\n",
    "    for i, t in enumerate(tqdm(scheduler.timesteps)):\n",
    "        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "        # if \n",
    "        latent_model_input = torch.cat([latents] * 2)\n",
    "        # latent_model_input = latents\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "        # controlnet\n",
    "        down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "                        latents,\n",
    "                        t,\n",
    "                        encoder_hidden_states=encoder_hidden_states,  # encoder_hidden_states_control [1,77,768]\n",
    "                        controlnet_cond=controlnet_image,\n",
    "                        return_dict=False,\n",
    "                    )\n",
    "        \n",
    "        \n",
    "        # predict the noise residual\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings, # text_embedding [2,77,768]\n",
    "                                down_block_additional_residuals=[sample.to(dtype=weight_dtype) for sample in down_block_res_samples],\n",
    "                                mid_block_additional_residual=mid_block_res_sample.to(dtype=weight_dtype),\n",
    "                                return_dict=False,\n",
    "                                )[0]\n",
    "\n",
    "        # perform guidance\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "\n",
    "\n",
    "        # compute the previous noisy sample x_t -> x_t-1\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            display_sample(latents, i + 1)\n",
    "            decoded_image_pil = show_latents(latents=latents, vae=vae)\n",
    "            display(decoded_image_pil)\n",
    "        \n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  ***controlnet_inpainting***\n",
    "The last step is to create the denoising loop that’ll progressively \n",
    "transform the pure noise in latents to an image described by your prompt.\n",
    "Remember, the denoising loop needs to do three things:\n",
    "\n",
    "1.Set the scheduler’s timesteps to use during denoising.\n",
    "2.Iterate over the timesteps.\n",
    "3.At each timestep, call the UNet model to predict the noise residual and \n",
    "pass it to the scheduler to compute the previous noisy sample.\n",
    "\n",
    "'''\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def denoise_control_sd_v15_inpainting(latents, mask, masked_image_latents, num_inference_steps=25, scheduler = scheduler_multistep, controlnet = controlnet,\n",
    "                           unet = unet, text_embeddings=text_embeddings, guidance_scale=5,\n",
    "                           controlnet_image=controlnet_image, encoder_hidden_states=encoder_hidden_states_control,\n",
    "                           weight_dtype = torch.float32, do_classifier_free_guidance=False):\n",
    "    scheduler.set_timesteps(num_inference_steps)\n",
    "\n",
    "  \n",
    "\n",
    "    for i, t in enumerate(tqdm(scheduler.timesteps)):\n",
    "        # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "        # if \n",
    "        latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "        # latent_model_input = latents\n",
    "        latent_model_input = scheduler.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "        # controlnet\n",
    "        down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "                        latents,\n",
    "                        t,\n",
    "                        encoder_hidden_states=encoder_hidden_states,  # encoder_hidden_states_control [1,77,768]\n",
    "                        controlnet_cond=controlnet_image,\n",
    "                        return_dict=False,\n",
    "                    )\n",
    "            # predict the noise residual\n",
    "    # if num_channels_unet == 9:\n",
    "        latent_model_input = torch.cat([latent_model_input, mask, masked_image_latents], dim=1)\n",
    "        \n",
    "        # predict the noise residual\n",
    "        with torch.no_grad():\n",
    "            noise_pred = unet(latent_model_input, t, encoder_hidden_states=encoder_hidden_states, # text_embedding [2,77,768]\n",
    "                                down_block_additional_residuals=[sample.to(dtype=weight_dtype) for sample in down_block_res_samples],\n",
    "                                mid_block_additional_residual=mid_block_res_sample.to(dtype=weight_dtype),\n",
    "                                return_dict=False,\n",
    "                                )[0]\n",
    "\n",
    "        # # perform guidance\n",
    "        # noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        # noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "\n",
    "\n",
    "        # compute the previous noisy sample x_t -> x_t-1\n",
    "        latents = scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        \n",
    "        if (i + 1) % 5 == 0:\n",
    "            display_sample(latents, i + 1)\n",
    "            decoded_image_pil = show_latents(latents=latents, vae=vae)\n",
    "            display(decoded_image_pil)\n",
    "        \n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "device = 'cuda'\n",
    "generator = torch.Generator(device=device).manual_seed(seed) \n",
    "guidance_scale = 10\n",
    "\n",
    "\n",
    "noise_randn =  torch.randn(\n",
    "    (batch_size, 4, height // 8, width // 8),\n",
    "    generator=generator,\n",
    "    device=torch_device,\n",
    ")\n",
    "\n",
    "noise_randn = noise_randn * scheduler_multistep.init_noise_sigma\n",
    "print(noise_randn.shape)\n",
    "\n",
    "latents_controled = denoise_control_sd_v15_inpainting(scheduler=scheduler_multistep, \n",
    "                                                      mask=mask_latent, masked_image_latents=masked_image_latent,\n",
    "                                           latents=noise_randn,controlnet=controlnet, \n",
    "                                           controlnet_image=controlnet_image, \n",
    "                                           guidance_scale=guidance_scale,encoder_hidden_states=encoder_hidden_states_control,\n",
    "                                           text_embeddings=text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "device = 'cuda'\n",
    "generator = torch.Generator(device=device).manual_seed(seed) \n",
    "guidance_scale = 10\n",
    "\n",
    "\n",
    "noise_randn =  torch.randn(\n",
    "    (batch_size, 4, height // 8, width // 8),\n",
    "    generator=generator,\n",
    "    device=torch_device,\n",
    ")\n",
    "\n",
    "noise_randn = noise_randn * scheduler_multistep.init_noise_sigma\n",
    "print(noise_randn.shape)\n",
    "\n",
    "latents_controled = denoise_control_sd_v15(scheduler=scheduler_multistep, \n",
    "                                           latents=noise_randn,controlnet=controlnet, \n",
    "                                           controlnet_image=controlnet_image, \n",
    "                                           guidance_scale=guidance_scale,encoder_hidden_states=encoder_hidden_states_control,\n",
    "                                           text_embeddings=text_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "device = 'cuda'\n",
    "generator = torch.Generator(device=device).manual_seed(seed) \n",
    "guidance_scale = 10\n",
    "\n",
    "\n",
    "noise_randn =  torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    "    device=torch_device,\n",
    ")\n",
    "\n",
    "noise_randn = noise_randn * scheduler_multistep.init_noise_sigma\n",
    "print(noise_randn.shape)\n",
    "\n",
    "latents_controled = denoise_control_sd_v15(scheduler=scheduler_multistep, \n",
    "                                           latents=noise_randn,controlnet=controlnet, \n",
    "                                           controlnet_image=controlnet_image, \n",
    "                                           guidance_scale=guidance_scale,encoder_hidden_states=encoder_hidden_states_control,\n",
    "                                           text_embeddings=text_embeddings)\n",
    "\n",
    "\n",
    "# show_latents(latents=latents_controled, vae=vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "device = 'cuda'\n",
    "generator = torch.Generator(device=device).manual_seed(seed) \n",
    "guidance_scale = 10\n",
    "\n",
    "\n",
    "noise_randn =  torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    "    device=torch_device,\n",
    ")\n",
    "\n",
    "noise_randn = noise_randn * scheduler_multistep.init_noise_sigma\n",
    "print(noise_randn.shape)\n",
    "\n",
    "latents_controled = denoise_control_sd_v15(scheduler=scheduler_multistep, \n",
    "                                           latents=noise_randn,controlnet=controlnet, \n",
    "                                           controlnet_image=controlnet_image, \n",
    "                                           guidance_scale=guidance_scale,encoder_hidden_states=encoder_hidden_states_control,\n",
    "                                           text_embeddings=text_embeddings)\n",
    "\n",
    "\n",
    "# show_latents(latents=latents_controled, vae=vae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
