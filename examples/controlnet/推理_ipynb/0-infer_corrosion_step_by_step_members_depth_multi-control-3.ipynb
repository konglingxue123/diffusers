{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline, DDIMPipeline, DDIMScheduler\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import PIL\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ''\n",
    "\n",
    "device = 'cuda'\n",
    "seed = 2024\n",
    "noisy_sample = torch.randn(\n",
    "    1, 4, 64, 64\n",
    ").to(device)\n",
    "controlnet_conditioning_scale=[1.0,0.6,1.0]\n",
    "# noisy_sample = torch.randn(\n",
    "#     1, 4, 512, 512\n",
    "# ).to('cuda:0')\n",
    "# noisy_sample = torch.randn(\n",
    "#     1, 4, 56, 56\n",
    "# ).to('cuda:0')\n",
    "# prompt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample(sample, i):\n",
    "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
    "    image_processed = (image_processed + 1.0) * 127.5\n",
    "    image_processed = image_processed.numpy().astype(np.uint8)\n",
    "\n",
    "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
    "    display(f\"Image at step {i}\")\n",
    "    display(image_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è½½å…¥å›¾ç‰‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512)\n",
      "145005\n",
      "170\n",
      "252\n",
      "85\n",
      "255\n",
      "55\n",
      "328\n",
      "136\n",
      "183\n",
      "182\n",
      "147\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=P size=512x512>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "mask_members_np_demo = np.array(Image.open(mask_members_dir))\n",
    "print(mask_members_np_demo.shape)\n",
    "\n",
    "random.seed(4)\n",
    "\n",
    "########### å®šä¹‰é¢œè‰²è½¬æ¢æ–¹æ³• ##############\n",
    "import imgviz\n",
    "\n",
    "def colored_mask(mask, save_path=None):\n",
    "    lbl_pil = Image.fromarray(mask.astype(np.uint8), mode=\"P\")\n",
    "    colormap = imgviz.label_colormap()\n",
    "    # print(colormap)\n",
    "    lbl_pil.putpalette(colormap.flatten())\n",
    "    if save_path is not None:\n",
    "        lbl_pil.save(save_path)\n",
    "\n",
    "    return lbl_pil\n",
    "\n",
    "\n",
    "\n",
    "def generate_ellipses(arr, fill_value=1, values=[5], num_ellipses=10, a_min=20, a_max=50, b_min=10, b_max=30):\n",
    "    \"\"\"\n",
    "    ðŸ•ç”¨å°‘æ•°é‡çš„å¤§æ¤­åœ†ï¼Œæ¨¡æ‹Ÿå¤§ç‰‡çš„è…èš€ï¼›\n",
    "    ðŸ•‘ç”¨å¤šæ•°é‡çš„å°æ¤­åœ†ï¼Œæ¨¡æ‹Ÿpitting corrosion\n",
    "    Generate random quadrilaterals within the specified regions of a NumPy array\n",
    "    and fill them with a specified value.\n",
    "\n",
    "    Args:\n",
    "        arr (numpy.ndarray): The input array.\n",
    "        num_polygons (int): The number of quadrilaterals to generate.\n",
    "        fill_value (int, optional): The value to use for filling the quadrilaterals. Default is 1.\n",
    "        values (list or tuple, optional): A list or tuple of values to consider as the selected region.\n",
    "                                          Default is [1].\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A new array with the same shape as the input array,\n",
    "                       containing the generated and filled quadrilaterals.\n",
    "    \"\"\"\n",
    "    \n",
    "    selected_region = np.isin(arr, values)\n",
    "    # print(selected_region)\n",
    "    selected_coords = np.argwhere(selected_region)\n",
    "    print(len(selected_coords))\n",
    "    # # æ¤­åœ†æ•°é‡å’ŒèŒƒå›´\n",
    "    # num_ellipses = 5\n",
    "    # a_min, a_max = 20, 50\n",
    "    # b_min, b_max = 10, 30\n",
    "    \n",
    "    result = arr.copy()\n",
    "    image = Image.fromarray(result)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # ç©ºç™½mask\n",
    "    result_blank = np.zeros_like(result)\n",
    "    # print(result_blank)\n",
    "    image_blank = Image.fromarray(result_blank)\n",
    "    draw_blank = ImageDraw.Draw(image_blank)\n",
    "    # # ç”Ÿæˆå›¾ç‰‡\n",
    "    # im = Image.new('RGB', (100,100), color='white')  \n",
    "    # draw = ImageDraw.Draw(im)\n",
    "\n",
    "    for i in range(num_ellipses):\n",
    "        xy_arr = random.sample(list(selected_coords), k=1)\n",
    "        # print(random.sample(list(selected_coords), k=2), random.sample(list(selected_coords), k=3))\n",
    "        print(xy_arr[0][0])\n",
    "        y, x = xy_arr[0][0], xy_arr[0][1]\n",
    "        # éšæœºç”Ÿæˆé•¿çŸ­åŠè½´\n",
    "        a = random.randint(a_min, a_max)\n",
    "        b = random.randint(b_min, b_max)\n",
    "        \n",
    "        # éšæœºä¸­å¿ƒç‚¹\n",
    "        # x = random.randint(0, 100-a)\n",
    "        # y = random.randint(0, 100-b)\n",
    "        \n",
    "        # ç»˜åˆ¶æ¤­åœ†\n",
    "        draw.ellipse((x, y, x+a, y+b), fill=fill_value)\n",
    "        draw_blank.ellipse((x, y, x+a, y+b), fill=fill_value)\n",
    "    \n",
    "    result = np.array(image)\n",
    "    result_mask = np.array(image_blank)\n",
    "    \n",
    "    return result, result_mask\n",
    "    \n",
    "result_e, result_mask_e = generate_ellipses(arr=mask_members_np_demo)\n",
    "colored_mask(result_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_dir = '/home/ubunto/å›¾ç‰‡/overture-creations-5sI6fQgYIuo_mask.png'\n",
    "# img_name = '2__1__1848___924'\n",
    "# img_name = '120'\n",
    "img_name = 'IMG_20230715_152221'\n",
    "org_img_dir = f'/home/ubunto/Project/konglx/generate/ControlNet-v1-1-nightly/training/corrosion_and_crack/corrosion/JPEGImages/{img_name}.jpg'\n",
    "org_img_pil = Image.open(org_img_dir)\n",
    "print(org_img_pil.mode)\n",
    "mask_corrosion_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/corrosion/conditioning_images/{img_name}.png'\n",
    "mask_members_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/members/conditioning_images/{img_name}.png'\n",
    "# mask_members_dir = None\n",
    "mask_depth_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/depth/conditioning_images/{img_name}.png'\n",
    "# mask_depth_dir = None\n",
    "\n",
    "if mask_depth_dir is None:\n",
    "    # c = 3\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.title('Corrosion mask')\n",
    "    plt.imshow(Image.open(mask_corrosion_dir))\n",
    "    plt.subplot(133)\n",
    "    plt.title('Members mask')\n",
    "    plt.imshow(Image.open(mask_members_dir))\n",
    "elif mask_members_dir is None:\n",
    "    # c = 3\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.title('Corrosion mask')\n",
    "    plt.imshow(Image.open(mask_corrosion_dir))\n",
    "    plt.subplot(133)\n",
    "    plt.title('Depth mask')\n",
    "    plt.imshow(Image.open(mask_depth_dir))\n",
    "else:\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(141)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(142)\n",
    "    plt.title('Corrosion mask')\n",
    "    plt.imshow(Image.open(mask_corrosion_dir))\n",
    "    plt.subplot(143)\n",
    "    plt.title('Members mask')\n",
    "    plt.imshow(Image.open(mask_members_dir))\n",
    "    plt.subplot(144)\n",
    "    plt.title('Depth mask')\n",
    "    plt.imshow(Image.open(mask_depth_dir), cmap='gray')\n",
    "# plt.imshow(Image.open(mask_depth_dir))\n",
    "# org_img_pil\n",
    "# mask_corrosion_dir = None\n",
    "# mask_members_dir = None\n",
    "\n",
    "# mask_corrosion_pil = load_image(mask_corrosion_dir)\n",
    "# mask_members_pil = load_image(mask_members_dir)\n",
    "# print(mask_corrosion_pil.mode, mask_members_pil.mode)\n",
    "# mask_members_pil\n",
    "# print(np.array(mask).shape, np.unique(np.array(mask)))\n",
    "# mask_copy = mask.copy()\n",
    "# mask_copy_cvt_l = mask_copy.convert(\"L\")\n",
    "# mask_copy_cvt_l_np = np.array(mask_copy_cvt_l)\n",
    "# print(mask_copy_cvt_l_np.shape, np.unique(mask_copy_cvt_l_np))\n",
    "# mask_copy_cvt_l_resized = mask_copy_cvt_l.resize((512, 512))\n",
    "# print(mask_copy_cvt_l_resized.size)\n",
    "# mask_copy_cvt_l_t = transforms.ToTensor()(mask_copy_cvt_l_resized).to(device)\n",
    "# print(mask_copy_cvt_l_t.shape, mask_copy_cvt_l_t.unique())\n",
    "# mask_copy_cvt_l_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = '/home/ubunto/Project/konglx/generate/ControlNet/models/stable-diffusion-v1-5'\n",
    "control_corrosion_trained_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_corrosion_inpainting_h-256_w-256_2024-07-12_09:10:44_seeds-2024/checkpoint-5700/controlnet'\n",
    "control_members_trained_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_members_inpainting_h-256_w-256_2024-07-12_14:52:34_seeds-2024/checkpoint-3800/controlnet'\n",
    "control_depth_trained_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_depth_h-512_w-512_2024-07-16_11:25:38_seeds-2023/checkpoint-5700/controlnet'\n",
    "# control_depth_trained_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_h-512_w-512_2024-07-16_17:42:27_seeds-2024/checkpoint-2000/controlnet'\n",
    "\n",
    "# æœ‰membersï¼Œæ— corrosionï¼Œæ— depth\n",
    "if mask_members_dir is not None and mask_corrosion_dir is None and mask_depth_dir is None:\n",
    "    mask_members_pil = load_image(mask_members_dir)\n",
    "    validation_image = mask_members_pil\n",
    "    control_members_dir = control_members_trained_dir\n",
    "    print('mask_members_dir is not None and mask_corrosion_dir is None and mask_depth_dir is None')\n",
    "    controlnet_members = ControlNetModel.from_pretrained(control_members_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=controlnet_members).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    print(mask_members_pil.mode)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    # Image.show(image)\n",
    "# æ— membersï¼Œæœ‰corrosionï¼Œæ— depth\n",
    "elif mask_members_dir is  None and mask_corrosion_dir is not None and mask_depth_dir is None:\n",
    "    control_corrosion_dir = control_corrosion_trained_dir\n",
    "    mask_corrosion_pil = load_image(mask_corrosion_dir)\n",
    "    validation_image = mask_corrosion_pil\n",
    "    print('mask_members_dir is  None and mask_corrosion_dir is not None and mask_depth_dir is None')\n",
    "    controlnet_corrosion = ControlNetModel.from_pretrained(control_corrosion_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=controlnet_corrosion).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "# æ— membersï¼Œæ— corrosionï¼Œæœ‰depth  \n",
    "elif mask_members_dir is  None and mask_corrosion_dir is None and mask_depth_dir is not None:\n",
    "    # control_depth_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_depth_h-512_w-512_2024-07-16_11:25:38_seeds-2023/checkpoint-5700/controlnet'\n",
    "    control_depth_dir = control_depth_trained_dir\n",
    "    \n",
    "    mask_depth_pil = load_image(mask_depth_dir)\n",
    "    validation_image = mask_depth_pil\n",
    "    print('mask_members_dir is  None and mask_corrosion_dir is None and mask_depth_dir is not None')\n",
    "    controlnet_depth = ControlNetModel.from_pretrained(control_depth_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=controlnet_depth).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mask_depth_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "# æœ‰membersï¼Œæœ‰corrosionï¼Œæ— depth\n",
    "elif mask_members_dir is not None and mask_corrosion_dir is not None and mask_depth_dir is None:\n",
    "    control_corrosion_dir = control_corrosion_trained_dir\n",
    "    control_members_dir = control_members_trained_dir\n",
    "    mask_members_pil = load_image(mask_members_dir)\n",
    "    mask_corrosion_pil = load_image(mask_corrosion_dir)\n",
    "    validation_image = [mask_corrosion_pil, mask_members_pil]\n",
    "    print('mask_members_dir is not None and mask_corrosion_dir is not None and mask_depth_dir is None')\n",
    "    controlnet_corrosion = ControlNetModel.from_pretrained(control_corrosion_dir)\n",
    "    controlnet_members = ControlNetModel.from_pretrained(control_members_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=[controlnet_corrosion, controlnet_members]).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    print(mask_corrosion_pil.mode)\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    print(mask_members_pil.mode)\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(144)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "# æ— membersï¼Œæœ‰corrosionï¼Œæœ‰depth \n",
    "elif mask_members_dir is  None and mask_corrosion_dir is not None and mask_depth_dir is not None:\n",
    "    control_corrosion_dir = control_corrosion_trained_dir\n",
    "    # control_depth_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_depth_h-512_w-512_2024-07-16_11:25:38_seeds-2023/checkpoint-5700/controlnet'\n",
    "    control_depth_dir = control_depth_trained_dir\n",
    "    \n",
    "    mask_depth_pil = load_image(mask_depth_dir)\n",
    "    mask_corrosion_pil = load_image(mask_corrosion_dir)\n",
    "    validation_image = [mask_corrosion_pil, mask_depth_pil]\n",
    "    print('mask_members_dir is  None and mask_corrosion_dir is not None and mask_depth_dir is not None')\n",
    "    controlnet_corrosion = ControlNetModel.from_pretrained(control_corrosion_dir)\n",
    "    controlnet_depth = ControlNetModel.from_pretrained(control_depth_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=[controlnet_corrosion, controlnet_depth]).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    print(mask_corrosion_pil.mode)\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(mask_depth_pil)\n",
    "    print(mask_depth_pil.mode)\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(144)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "# æœ‰membersï¼Œæ— corrosionï¼Œæœ‰depth\n",
    "elif mask_members_dir is not None and mask_corrosion_dir is None and mask_depth_dir is not None:\n",
    "    control_members_dir = control_members_trained_dir\n",
    "    # control_depth_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_depth_h-512_w-512_2024-07-16_11:25:38_seeds-2023/checkpoint-5700/controlnet'\n",
    "    control_depth_dir = control_depth_trained_dir\n",
    "    \n",
    "    mask_depth_pil = load_image(mask_depth_dir)\n",
    "    mask_members_pil = load_image(mask_members_dir)\n",
    "    validation_image = [mask_members_pil, mask_depth_pil]\n",
    "    print('mask_members_dir is not None and mask_corrosion_dir is None and mask_depth_dir is not None')\n",
    "    controlnet_members = ControlNetModel.from_pretrained(control_members_dir)\n",
    "    controlnet_depth = ControlNetModel.from_pretrained(control_depth_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=[controlnet_members, controlnet_depth]).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    print(mask_members_pil.mode)\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(mask_depth_pil)\n",
    "    print(mask_depth_pil.mode)\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(144)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "# æœ‰membersï¼Œæœ‰corrosionï¼Œæœ‰depth\n",
    "elif mask_members_dir is not None and mask_corrosion_dir is not None and mask_depth_dir is not None:\n",
    "    control_corrosion_dir = control_corrosion_trained_dir\n",
    "    control_members_dir = control_members_trained_dir\n",
    "    # control_depth_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_depth_h-512_w-512_2024-07-16_11:25:38_seeds-2023/checkpoint-5700/controlnet'\n",
    "    control_depth_dir = control_depth_trained_dir\n",
    "    \n",
    "    mask_corrosion_pil = load_image(mask_corrosion_dir)\n",
    "    mask_members_pil = load_image(mask_members_dir)\n",
    "    mask_depth_pil = load_image(mask_depth_dir)\n",
    "    validation_image = [mask_corrosion_pil, mask_members_pil, mask_depth_pil]\n",
    "    print('mask_members_dir is not None and mask_corrosion_dir is not None and mask_depth_dir is not None')\n",
    "    controlnet_corrosion = ControlNetModel.from_pretrained(control_corrosion_dir)\n",
    "    controlnet_members = ControlNetModel.from_pretrained(control_members_dir)\n",
    "    controlnet_depth = ControlNetModel.from_pretrained(control_depth_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=[controlnet_corrosion,controlnet_members, controlnet_depth], controlnet_conditioning_scale=controlnet_conditioning_scale).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator, iter_times=5\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(151)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    plt.subplot(152)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    print(mask_members_pil.mode)\n",
    "    plt.subplot(153)\n",
    "    plt.imshow(mask_depth_pil)\n",
    "    print(mask_depth_pil.mode)\n",
    "    plt.subplot(154)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(155)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "# æ— membersï¼Œæ— corrosionï¼Œæ— depth\n",
    "else:\n",
    "    print('Input nothing')\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_corrosion_np = np.array(mask_corrosion_pil)\n",
    "mask_corrosion_np.min(), mask_corrosion_np.max(), mask_corrosion_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mask_members_dir is not None and mask_corrosion_dir is None:\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    # Image.show(image)\n",
    "    \n",
    "elif mask_members_dir is  None and mask_corrosion_dir is not None:\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "else:\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PNDMScheduler\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                ).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPMScheduler\n",
    "from diffusers import DDPMScheduler\n",
    "import os\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "scheduler_ddpm = DDPMScheduler.from_pretrained(os.path.join(config_dir, 'scheduler'))\n",
    "\n",
    "# print(pipeline.scheduler)\n",
    "pipeline.scheduler = scheduler_ddpm\n",
    "# print(pipeline.scheduler)\n",
    "image = pipeline(\n",
    "                   prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                ).images[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mask_members_dir is not None and mask_corrosion_dir is None:\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    # Image.show(image)\n",
    "    \n",
    "elif mask_members_dir is  None and mask_corrosion_dir is not None:\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "else:\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(144)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDIMScheduler\n",
    "from diffusers import DDIMScheduler\n",
    "import os\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "scheduler_ddim = DDIMScheduler.from_pretrained(os.path.join(config_dir, 'scheduler'))\n",
    "\n",
    "print(pipeline.scheduler)\n",
    "pipeline.scheduler = scheduler_ddim\n",
    "print(pipeline.scheduler)\n",
    "image = pipeline(\n",
    "                   prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                ).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PNDMScheduler\n",
    "from diffusers import PNDMScheduler\n",
    "import os\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "scheduler_pndm = PNDMScheduler.from_pretrained(os.path.join(config_dir, 'scheduler'))\n",
    "\n",
    "print(pipeline.scheduler)\n",
    "pipeline.scheduler = scheduler_pndm\n",
    "print(pipeline.scheduler)\n",
    "image = pipeline(\n",
    "                   prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                ).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***##step by step denoise##***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "control_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_crack_only_generate/checkpoint-2000/controlnet'\n",
    "config_dir = '/home/ubunto/Project/konglx/generate/ControlNet/models/stable-diffusion-v1-5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.åˆ›å»ºå„ä¸ªéƒ¨åˆ†çš„æ¨¡åž‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler, ControlNetModel\n",
    "import os\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(config_dir, subfolder='vae', use_safetensors=None)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(config_dir, subfolder='tokenizer')\n",
    "text_encoder = CLIPTextModel.from_pretrained(config_dir, subfolder='text_encoder', use_safetensors=None)\n",
    "unet = UNet2DConditionModel.from_pretrained(config_dir, subfolder='unet', use_safetensors=None)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(control_dir)\n",
    "controlnet.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.config.scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UniPCMultistepScheduler\n",
    "\n",
    "scheduler_multistep = UniPCMultistepScheduler.from_pretrained(config_dir, subfolder=\"scheduler\")\n",
    "scheduler_multistep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = \"cuda\"\n",
    "vae.to(torch_device)\n",
    "text_encoder.to(torch_device)\n",
    "unet.to(torch_device)\n",
    "controlnet.to(torch_device)\n",
    "unet.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Create embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"crack\"]\n",
    "device = 'cuda'\n",
    "seed = 0\n",
    "height = 512  # default height of Stable Diffusion\n",
    "width = 512  # default width of Stable Diffusion\n",
    "num_inference_steps = 25  # Number of denoising steps\n",
    "guidance_scale = 7.5  # Scale for classifier-free guidance\n",
    "generator = torch.Generator(device=device).manual_seed(seed)  # Seed generator to create the initial latent noise\n",
    "batch_size = len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tokenizer(\n",
    "    prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "prompt_embeds = text_embeddings\n",
    "encoder_hidden_states_control = text_embeddings\n",
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "mask_dir = '/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/crackï¼ˆå¤ä»¶ï¼‰/SegmentationClass/DeepCrack_11240-6.png'\n",
    "validation_image = Image.open(mask_dir).convert(\"RGB\")\n",
    "validation_image.size\n",
    "\n",
    "conditioning_image_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(512),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "conditioning_image_pil = validation_image.resize([512, 512])\n",
    "conditioning_pixel_values = torch.stack([conditioning_image_transforms(conditioning_image_pil)])\n",
    "conditioning_pixel_values = conditioning_pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "print(conditioning_pixel_values.shape)\n",
    "controlnet_image = conditioning_pixel_values.to(torch_device)\n",
    "# print(controlnet_image)\n",
    "conditioning_image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Youâ€™ll also need to generate the unconditional text embeddings which are the embeddings for the padding token.\n",
    "These need to have the same shape (batch_size and seq_length) as the conditional text_embeddings:\n",
    "'''\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "uncond_embeddings.shape, text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Letâ€™s concatenate the conditional and unconditional embeddings into a batch to avoid doing two forward passes:\n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "# text_embeddings\n",
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Create random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Next, generate some initial random noise as a starting point for the diffusion process. \n",
    "This is the latent representation of the image, and itâ€™ll be gradually denoised. \n",
    "At this point, the latent image is smaller than the final image size but thatâ€™s okay though \n",
    "because the model will transform it into the final 512x512 image dimensions later.\n",
    "'''\n",
    "\n",
    "# The height and width are divided by 8 because the vae model has 3 down-sampling layers.\n",
    "# You can check by running the following:   2 ** (len(vae.config.block_out_channels) - 1) == 8\n",
    "\n",
    "do_classifier_free_guidance = False\n",
    "guess_mode = False\n",
    "\n",
    "latents = torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    "    device=torch_device,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "                    latents,\n",
    "                    2,\n",
    "                    encoder_hidden_states=encoder_hidden_states_control,\n",
    "                    controlnet_cond=controlnet_image,\n",
    "                    return_dict=False,\n",
    "                )\n",
    "len(down_block_res_samples), down_block_res_samples[-1].shape, len(mid_block_res_sample), mid_block_res_sample[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Denoise the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Start by scaling the input with the initial noise distribution, sigma, the noise scale value, \n",
    "which is required for improved schedulers like UniPCMultistepScheduler: \n",
    "'''\n",
    "print(scheduler_multistep.init_noise_sigma)\n",
    "latents = latents * scheduler_multistep.init_noise_sigma\n",
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "The last step is to create the denoising loop thatâ€™ll progressively \n",
    "transform the pure noise in latents to an image described by your prompt.\n",
    "Remember, the denoising loop needs to do three things:\n",
    "\n",
    "1.Set the schedulerâ€™s timesteps to use during denoising.\n",
    "2.Iterate over the timesteps.\n",
    "3.At each timestep, call the UNet model to predict the noise residual and \n",
    "pass it to the scheduler to compute the previous noisy sample.\n",
    "\n",
    "'''\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "scheduler_multistep.set_timesteps(num_inference_steps)\n",
    "\n",
    "for t in tqdm(scheduler_multistep.timesteps):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "    latent_model_input = scheduler_multistep.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler_multistep.step(noise_pred, t, latents).prev_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "pil_latents = transforms.ToPILImage()(latents.squeeze(0))\n",
    "pil_latents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Decode the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and decode the image latents with vae\n",
    "latents_scaled = 1 / vae.config.scaling_factor * latents\n",
    "# pil_latents_scaled = transforms.ToPILImage()(latents.squeeze(0))\n",
    "# pil_latents_scaled = transforms.ToPILImage()(latents_scaled.squeeze(0))\n",
    "with torch.no_grad():\n",
    "    # image = vae.decode(latents).sample\n",
    "    image = vae.decode(latents_scaled).sample\n",
    "print(image.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (image / 2 + 0.5).clamp(0, 1).squeeze()\n",
    "image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n",
    "image = Image.fromarray(image)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "The last step is to create the denoising loop thatâ€™ll progressively \n",
    "transform the pure noise in latents to an image described by your prompt.\n",
    "Remember, the denoising loop needs to do three things:\n",
    "\n",
    "1.Set the schedulerâ€™s timesteps to use during denoising.\n",
    "2.Iterate over the timesteps.\n",
    "3.At each timestep, call the UNet model to predict the noise residual and \n",
    "pass it to the scheduler to compute the previous noisy sample.\n",
    "\n",
    "'''\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "scheduler_multistep.set_timesteps(num_inference_steps)\n",
    "\n",
    "for t in tqdm(scheduler.timesteps):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    # expand the latents if we are doing classifier free guidance\n",
    "    latent_model_input_control = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "    # latent_model_input_control = torch.cat([latents_control] * 2)\n",
    "\n",
    "    latent_model_input_control = scheduler.scale_model_input(latent_model_input_control, timestep=t)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        noise_pred_control = unet(latent_model_input_control, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond_control, noise_pred_text_control = noise_pred_control.chunk(2)\n",
    "    noise_pred_control = noise_pred_uncond_control+ guidance_scale * (noise_pred_text_control - noise_pred_uncond_control)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler.step(noise_pred_control, t, latents).prev_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = any\n",
    "a in 'abc'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
