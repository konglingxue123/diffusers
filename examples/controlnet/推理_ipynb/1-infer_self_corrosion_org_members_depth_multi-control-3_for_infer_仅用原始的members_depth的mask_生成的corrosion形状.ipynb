{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubunto/software/miniconda3/envs/dif/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ubunto/software/miniconda3/envs/dif/lib/python3.9/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DDPMPipeline, DDIMPipeline, DDIMScheduler\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import PIL\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ''\n",
    "\n",
    "device = 'cuda'\n",
    "seed = 2024\n",
    "noisy_sample = torch.randn(\n",
    "    1, 4, 64, 64\n",
    ").to(device)\n",
    "# noisy_sample = torch.randn(\n",
    "#     1, 4, 512, 512\n",
    "# ).to('cuda:0')\n",
    "# noisy_sample = torch.randn(\n",
    "#     1, 4, 56, 56\n",
    "# ).to('cuda:0')\n",
    "# prompt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample(sample, i):\n",
    "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
    "    image_processed = (image_processed + 1.0) * 127.5\n",
    "    image_processed = image_processed.numpy().astype(np.uint8)\n",
    "\n",
    "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
    "    display(f\"Image at step {i}\")\n",
    "    display(image_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 填充腐蚀区域（前提要有members的mask）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.基于members的mask，生成腐蚀（腐蚀形状：多边形；腐蚀数量：自定义，一定范围自动确定；腐蚀尺度：一定范围随机生成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def times(x, y):\n",
    "    return x * y\n",
    "type(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "img_name = '120'\n",
    "mask_members_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/members/conditioning_images/{img_name}.png'\n",
    "mask_members_np_demo = np.array(Image.open(mask_members_dir))\n",
    "print(mask_members_np_demo.shape)\n",
    "\n",
    "random.seed(4)\n",
    "\n",
    "########### 定义颜色转换方法 ##############\n",
    "import imgviz\n",
    "\n",
    "def colored_mask(mask, save_path=None):\n",
    "    lbl_pil = Image.fromarray(mask.astype(np.uint8), mode=\"P\")\n",
    "    colormap = imgviz.label_colormap()\n",
    "    # print(colormap)\n",
    "    lbl_pil.putpalette(colormap.flatten())\n",
    "    if save_path is not None:\n",
    "        lbl_pil.save(save_path)\n",
    "\n",
    "    return lbl_pil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ！！形状和形状的数量可控，损伤的类型可控（以腐蚀为例）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105082\n",
      "117\n",
      "258\n",
      "86\n",
      "11\n",
      "133\n",
      "112\n",
      "153\n",
      "63\n",
      "15\n",
      "124\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=P size=512x512>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def generate_ellipses(arr, fill_value=1, values=[5], num_ellipses=10, a_min=20, a_max=50, b_min=10, b_max=30):\n",
    "    \"\"\"\n",
    "    🕐用少数量的大椭圆，模拟大片的腐蚀；\n",
    "    🕑用多数量的小椭圆，模拟pitting corrosion\n",
    "    Generate random quadrilaterals within the specified regions of a NumPy array\n",
    "    and fill them with a specified value.\n",
    "\n",
    "    Args:\n",
    "        arr (numpy.ndarray): The input array.\n",
    "        num_polygons (int): The number of quadrilaterals to generate.\n",
    "        fill_value (int, optional): The value to use for filling the quadrilaterals. Default is 1.\n",
    "        values (list or tuple, optional): A list or tuple of values to consider as the selected region.\n",
    "                                          Default is [1].\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A new array with the same shape as the input array,\n",
    "                       containing the generated and filled quadrilaterals.\n",
    "    \"\"\"\n",
    "    \n",
    "    selected_region = np.isin(arr, values)\n",
    "    # print(selected_region)\n",
    "    selected_coords = np.argwhere(selected_region)\n",
    "    print(len(selected_coords))\n",
    "    # # 椭圆数量和范围\n",
    "    # num_ellipses = 5\n",
    "    # a_min, a_max = 20, 50\n",
    "    # b_min, b_max = 10, 30\n",
    "    \n",
    "    result = arr.copy()\n",
    "    image = Image.fromarray(result)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # 空白mask\n",
    "    result_blank = np.zeros_like(result)\n",
    "    # print(result_blank)\n",
    "    image_blank = Image.fromarray(result_blank)\n",
    "    draw_blank = ImageDraw.Draw(image_blank)\n",
    "    # # 生成图片\n",
    "    # im = Image.new('RGB', (100,100), color='white')  \n",
    "    # draw = ImageDraw.Draw(im)\n",
    "\n",
    "    for i in range(num_ellipses):\n",
    "        xy_arr = random.sample(list(selected_coords), k=1)\n",
    "        # print(random.sample(list(selected_coords), k=2), random.sample(list(selected_coords), k=3))\n",
    "        print(xy_arr[0][0])\n",
    "        y, x = xy_arr[0][0], xy_arr[0][1]\n",
    "        # 随机生成长短半轴\n",
    "        a = random.randint(a_min, a_max)\n",
    "        b = random.randint(b_min, b_max)\n",
    "        \n",
    "        # 随机中心点\n",
    "        # x = random.randint(0, 100-a)\n",
    "        # y = random.randint(0, 100-b)\n",
    "        \n",
    "        # 绘制椭圆\n",
    "        draw.ellipse((x, y, x+a, y+b), fill=fill_value)\n",
    "        draw_blank.ellipse((x, y, x+a, y+b), fill=fill_value)\n",
    "    \n",
    "    result = np.array(image)\n",
    "    result_mask = np.array(image_blank)\n",
    "    \n",
    "    return result, result_mask\n",
    "    \n",
    "result_e, result_mask_e = generate_ellipses(arr=mask_members_np_demo)\n",
    "colored_mask(result_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "# img_name = 'IMG_20230715_152221'\n",
    "select_area_list = [1]\n",
    "img_name = '120'\n",
    "org_img_dir = f'/home/ubunto/Project/konglx/generate/ControlNet-v1-1-nightly/training/corrosion_and_crack/corrosion/JPEGImages/{img_name}.jpg'\n",
    "org_img_pil = load_image(org_img_dir)\n",
    "# print(org_img_pil.mode)\n",
    "mask_corrosion_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/corrosion/conditioning_images/{img_name}.png'\n",
    "mask_members_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/members/conditioning_images/{img_name}.png'\n",
    "\n",
    "# mask_members_dir = None\n",
    "mask_depth_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/depth/conditioning_images/{img_name}.png'\n",
    "###################################################################################\n",
    "\n",
    "########### 定义颜色转换方法 ##############\n",
    "import imgviz\n",
    "\n",
    "def colored_mask(mask, save_path=None):\n",
    "    lbl_pil = Image.fromarray(mask.astype(np.uint8), mode=\"P\")\n",
    "    colormap = imgviz.label_colormap()\n",
    "    # print(colormap)\n",
    "    lbl_pil.putpalette(colormap.flatten())\n",
    "    if save_path is not None:\n",
    "        lbl_pil.save(save_path)\n",
    "\n",
    "    return lbl_pil\n",
    "\n",
    "\n",
    "########### 定义基于numpy选择构件的函数#####\n",
    "\n",
    "def filter_array(arr, values):\n",
    "    \"\"\"\n",
    "    Filter a NumPy array to keep only the specified values.\n",
    "\n",
    "    Args:\n",
    "        arr (numpy.ndarray): The input array to be filtered.\n",
    "        values (list or tuple): A list or tuple of values to keep in the filtered array.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A new array with the same shape as the input array,\n",
    "                       containing only the specified values.\n",
    "    \"\"\"\n",
    "    conditions = [arr == value for value in values]\n",
    "    condition = np.logical_or.reduce(conditions)\n",
    "    filtered_arr = np.where(condition, arr, 0)\n",
    "    return filtered_arr\n",
    "#########################################\n",
    "\n",
    "########## 定义生成任意多边形 ##############\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def generate_polygon(arr, num_polygons, fill_value=1, values=[1], polygon_shape=4):\n",
    "    \"\"\"\n",
    "    Generate random quadrilaterals within the specified regions of a NumPy array\n",
    "    and fill them with a specified value.\n",
    "\n",
    "    Args:\n",
    "        arr (numpy.ndarray): The input array.\n",
    "        num_polygons (int): The number of quadrilaterals to generate.\n",
    "        fill_value (int, optional): The value to use for filling the quadrilaterals. Default is 1.\n",
    "        values (list or tuple, optional): A list or tuple of values to consider as the selected region.\n",
    "                                          Default is [1].\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A new array with the same shape as the input array,\n",
    "                       containing the generated and filled quadrilaterals.\n",
    "    \"\"\"\n",
    "    selected_region = np.isin(arr, values)\n",
    "    selected_coords = np.argwhere(selected_region)\n",
    "\n",
    "    result = arr.copy()\n",
    "    image = Image.fromarray(result)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # 空白mask\n",
    "    result_blank = np.zeros_like(result)\n",
    "    # print(result_blank)\n",
    "    image_blank = Image.fromarray(result_blank)\n",
    "    draw_blank = ImageDraw.Draw(image_blank)\n",
    "\n",
    "    for _ in range(num_polygons):\n",
    "        coords = random.sample(list(selected_coords), polygon_shape)\n",
    "        polygon_coords = [(coord[1], coord[0]) for coord in coords]\n",
    "        draw.polygon(polygon_coords, fill=fill_value)\n",
    "        \n",
    "        draw_blank.polygon(polygon_coords, fill=fill_value)\n",
    "\n",
    "    result = np.array(image)\n",
    "    result_mask = np.array(image_blank)\n",
    "    # plt.imshow(result_mask)\n",
    "    return result, result_mask\n",
    "#########################################\n",
    "\n",
    "###### 定义选定区域内生成若干数量的腐蚀#######\n",
    "def generate_polygon_to_want_area(org_img_np, generate_polygon, fill_value, want_area = [1,4]):\n",
    "    \"\"\"\n",
    "    generate_polygon_to_want_area based on function 'generate_polygon'.\n",
    "\n",
    "    Args:\n",
    "        org_img_np (numpy.ndarray): The input array.\n",
    "        generate_polygon (function): generate_polygon\n",
    "        want_area_list (list or tuple, optional): A list or tuple of values to consider as the selected region.\n",
    "                                          Default is [1,4].\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A new array with the same shape as the input array,\n",
    "                       containing the generated and filled polygon.\n",
    "    \"\"\"\n",
    "    list_area_corrosion = []\n",
    "    list_area_only_corrosion = []\n",
    "    for area in want_area:\n",
    "        gen_polygon, gen_polygon_blank = generate_polygon(arr=org_img_np, num_polygons=1, values=[area], fill_value=fill_value, polygon_shape=5)\n",
    "        list_area_corrosion.append(gen_polygon)\n",
    "        list_area_only_corrosion.append(gen_polygon_blank)\n",
    "    sum_area = np.sum(list_area_corrosion, axis=0)\n",
    "    sum_area_only_corrosion = np.sum(list_area_only_corrosion, axis=0)\n",
    "    return sum_area, list_area_corrosion, sum_area_only_corrosion, list_area_only_corrosion\n",
    "\n",
    "mask_corrosion_pil_org = load_image(mask_corrosion_dir)\n",
    "mask_corrosion_pil_org.mode, np.array(mask_corrosion_pil_org).min(), np.array(mask_corrosion_pil_org).max(), np.array(org_img_pil).min(), np.array(org_img_pil).max(), transforms.ToTensor()(org_img_pil).min(),transforms.ToTensor()(org_img_pil).max(), transforms.ToTensor()(mask_corrosion_pil_org).min(),transforms.ToTensor()(mask_corrosion_pil_org).max()\n",
    "\n",
    "seed_setting = 7 # 展示用的是seed_setting=16\n",
    "fill_value_setting = 12 # 以12 为填充数据，与memberstype分开，而后再通过corrosionType_to_fill_value返回对应corrosionType\n",
    "\n",
    "corrosionType = [3] # 腐蚀类型： 1-fair，2-poor， 3-severe\n",
    "def corrosionType_to_fill_value(corrosionType):\n",
    "    fill_value = []\n",
    "    for i in corrosionType:\n",
    "        if i==1:\n",
    "            fill_value.append(12)\n",
    "        elif i==2:\n",
    "            fill_value.append(6)\n",
    "        elif i==3:\n",
    "            fill_value.append(4)\n",
    "    return fill_value\n",
    "\n",
    "fill_value_list = corrosionType_to_fill_value(corrosionType=corrosionType) # 以12为总数量，fill_value与corrosionType对应：腐蚀类型： 1-fair对应12/12，2-poor对应12/6， 3-severe对应12/4\n",
    "\n",
    "\n",
    "random.seed(seed_setting)  # 7\n",
    "# 1.members的mask\n",
    "mask_members_pil_org = Image.open(mask_members_dir)\n",
    "plt.figure(figsize=(16,7))\n",
    "plt.subplot(241)\n",
    "plt.imshow(mask_members_pil_org)\n",
    "# mask_members_pt_mode_R = transforms.ToTensor()(mask_members_pil_org)  # mode为P的PIL，经过ToTensor转为了[0,1]区间内\n",
    "# print(transforms.ToTensor()(mask_members_pil_org).unique())\n",
    "mask_members_np_mode_R = np.array(mask_members_pil_org) # 将PIL转为np，不改变数值\n",
    "mask_members_pt_mode_R_from_np = torch.from_numpy(mask_members_np_mode_R)\n",
    "# print(mask_members_pt_mode_R_from_np.min(), mask_members_pt_mode_R_from_np.max(), mask_members_pt_mode_R_from_np.shape)\n",
    "\n",
    "mask_members_np_mode_R_filter = filter_array(mask_members_np_mode_R, [1,4])\n",
    "# 2.选择members区域\n",
    "plt.subplot(242)\n",
    "plt.imshow(mask_members_np_mode_R_filter)\n",
    "print(mask_members_np_mode_R_filter.shape, mask_members_np_mode_R_filter.min(), mask_members_np_mode_R_filter.max(), np.unique(mask_members_np_mode_R_filter))\n",
    "# print(mask_members_np_mode_R[mask_members_np_mode_R==1].reshape(mask_members_np_mode_R.shape[0], mask_members_np_mode_R.shape[1]))\n",
    "# mask_members_pt_mode_R.min(), mask_members_pt_mode_R.max(), mask_members_np_mode_R.min(), mask_members_np_mode_R.max(), mask_members_np_mode_R.shape\n",
    "transforms.ToTensor()(mask_members_np_mode_R_filter).max()\n",
    "\n",
    "# 3.在members区域生成任意数量，任意数值的corrosion的mask （展示用）\n",
    "\n",
    "mask_members_with_gen_corrosion, mask_members_with_gen_corrosion_blank = generate_polygon(arr=mask_members_np_mode_R_filter, num_polygons=1, values=[4], fill_value=fill_value_setting, polygon_shape=5)\n",
    "plt.subplot(243)\n",
    "plt.imshow(mask_members_with_gen_corrosion)\n",
    "\n",
    "# 4.在members指定区域生成任意数量的腐蚀，返回仅仅为腐蚀区域 （生成用）\n",
    "mask_corrosion_corr_to_members, mask_corrosion_corr_to_members_list, mask_only_corrosion_corr_to_members, mask_only_corrosion_corr_to_members_list = generate_polygon_to_want_area(org_img_np=mask_members_np_mode_R, generate_polygon=generate_polygon, fill_value=fill_value_setting)\n",
    "plt.subplot(244)\n",
    "plt.imshow(mask_corrosion_corr_to_members)\n",
    "plt.title('Generate corrosion mask with members')\n",
    "\n",
    "# plt.subplot(245)\n",
    "# plt.imshow(mask_only_corrosion_corr_to_members)\n",
    "\n",
    "# print(np.unique(mask_only_corrosion_corr_to_members))\n",
    "# plt.subplot(246)\n",
    "# plt.imshow(mask_only_corrosion_corr_to_members/12)\n",
    "\n",
    "mask_only_corrosion_corr_to_members_devide_by_fill_value = (mask_only_corrosion_corr_to_members/fill_value_list[0]).astype((np.uint64))\n",
    "plt.subplot(247)\n",
    "plt.imshow(mask_only_corrosion_corr_to_members_devide_by_fill_value)\n",
    "print(mask_only_corrosion_corr_to_members_devide_by_fill_value.min(), mask_only_corrosion_corr_to_members_devide_by_fill_value.max(), np.unique(mask_only_corrosion_corr_to_members_devide_by_fill_value), mask_only_corrosion_corr_to_members_devide_by_fill_value.dtype)\n",
    "# print(np.unique(mask_only_corrosion_corr_to_members/12), mask_only_corrosion_corr_to_members.dtype, ((mask_only_corrosion_corr_to_members/12).astype(np.uint64)).dtype)\n",
    "\n",
    "mask_only_corrosion_final_pil = colored_mask(mask_only_corrosion_corr_to_members_devide_by_fill_value)\n",
    "plt.subplot(248)\n",
    "plt.imshow(mask_only_corrosion_final_pil)\n",
    "print(mask_only_corrosion_final_pil.mode)\n",
    "plt.title('Final generate mask')\n",
    "\n",
    "# 存储\n",
    "save_mask_dir = f'/home/ubunto/Project/konglx/generate/diffusers/output_imgs/钢构件生成的腐蚀mask/{img_name}__seed_{seed_setting}__corrosionType_{corrosionType[0]}.png'\n",
    "mask_only_corrosion_final_pil.save(save_mask_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paste_pil_a_to_b(a_pil, b_pil):\n",
    "    a_pil_rgba = a_pil.convert('RGBA')\n",
    "    b_pil_rgba = b_pil.convert('RGBA')\n",
    "    a_b_pil_rgba = Image.blend(a_pil_rgba, b_pil_rgba, 0.5)\n",
    "    a_b_pil_rgb = a_b_pil_rgba.convert('RGB')\n",
    "    return a_b_pil_rgb\n",
    "\n",
    "gen_corrosion_with_org_img = paste_pil_a_to_b(org_img_pil, mask_only_corrosion_final_pil)\n",
    "gen_corrosion_with_org_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_dir = '/home/ubunto/图片/overture-creations-5sI6fQgYIuo_mask.png'\n",
    "# img_name = '2__1__1848___924'\n",
    "img_name = '120'\n",
    "corrosiont_type = corrosionType[0]\n",
    "print('corrosionType[0]:', corrosionType[0])\n",
    "# img_name = 'IMG_20230715_152221'\n",
    "org_img_dir = f'/home/ubunto/Project/konglx/generate/ControlNet-v1-1-nightly/training/corrosion_and_crack/corrosion/JPEGImages/{img_name}.jpg'\n",
    "org_img_pil = Image.open(org_img_dir)\n",
    "print(org_img_pil.mode)\n",
    "# mask_corrosion_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/corrosion/conditioning_images/{img_name}.png'\n",
    "# mask_corrosion_dir = f'/home/ubunto/Project/konglx/generate/diffusers/output_imgs/钢构件生成的腐蚀mask/{img_name}__seed_16__corrosionType_{corrosiont_type}.png'\n",
    "# mask_corrosion_dir = f'/home/ubunto/Project/konglx/generate/diffusers/output_imgs/钢构件生成的腐蚀mask/{img_name}__seed_16__corrosionType_1.png'\n",
    "# mask_corrosion_dir = f'/home/ubunto/Project/konglx/generate/diffusers/output_imgs/钢构件生成的腐蚀mask/{img_name}__seed_16__corrosionType_2.png'\n",
    "mask_corrosion_dir = save_mask_dir\n",
    "\n",
    "mask_members_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/members/conditioning_images/{img_name}.png'\n",
    "# mask_members_dir = None\n",
    "mask_depth_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/depth/conditioning_images/{img_name}.png'\n",
    "# mask_depth_dir = None\n",
    "\n",
    "if mask_depth_dir is None:\n",
    "    # c = 3\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.title('Corrosion mask')\n",
    "    plt.imshow(Image.open(mask_corrosion_dir))\n",
    "    plt.subplot(133)\n",
    "    plt.title('Members mask')\n",
    "    plt.imshow(Image.open(mask_members_dir))\n",
    "elif mask_members_dir is None:\n",
    "    # c = 3\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.title('Corrosion mask')\n",
    "    plt.imshow(Image.open(mask_corrosion_dir))\n",
    "    plt.subplot(133)\n",
    "    plt.title('Depth mask')\n",
    "    plt.imshow(Image.open(mask_depth_dir))\n",
    "else:\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(141)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(142)\n",
    "    plt.title('Corrosion mask')\n",
    "    plt.imshow(Image.open(mask_corrosion_dir))\n",
    "    plt.subplot(143)\n",
    "    plt.title('Members mask')\n",
    "    plt.imshow(Image.open(mask_members_dir))\n",
    "    plt.subplot(144)\n",
    "    plt.title('Depth mask')\n",
    "    plt.imshow(Image.open(mask_depth_dir), cmap='gray')\n",
    "# plt.imshow(Image.open(mask_depth_dir))\n",
    "# org_img_pil\n",
    "# mask_corrosion_dir = None\n",
    "# mask_members_dir = None\n",
    "\n",
    "# mask_corrosion_pil = load_image(mask_corrosion_dir)\n",
    "# mask_members_pil = load_image(mask_members_dir)\n",
    "# print(mask_corrosion_pil.mode, mask_members_pil.mode)\n",
    "# mask_members_pil\n",
    "# print(np.array(mask).shape, np.unique(np.array(mask)))\n",
    "# mask_copy = mask.copy()\n",
    "# mask_copy_cvt_l = mask_copy.convert(\"L\")\n",
    "# mask_copy_cvt_l_np = np.array(mask_copy_cvt_l)\n",
    "# print(mask_copy_cvt_l_np.shape, np.unique(mask_copy_cvt_l_np))\n",
    "# mask_copy_cvt_l_resized = mask_copy_cvt_l.resize((512, 512))\n",
    "# print(mask_copy_cvt_l_resized.size)\n",
    "# mask_copy_cvt_l_t = transforms.ToTensor()(mask_copy_cvt_l_resized).to(device)\n",
    "# print(mask_copy_cvt_l_t.shape, mask_copy_cvt_l_t.unique())\n",
    "# mask_copy_cvt_l_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = '/home/ubunto/Project/konglx/generate/ControlNet/models/stable-diffusion-v1-5'\n",
    "control_corrosion_trained_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_corrosion_inpainting_h-256_w-256_2024-07-12_09:10:44_seeds-2024/checkpoint-5700/controlnet'\n",
    "control_members_trained_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_members_inpainting_h-256_w-256_2024-07-12_14:52:34_seeds-2024/checkpoint-3800/controlnet'\n",
    "# control_depth_trained_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_depth_h-512_w-512_2024-07-16_11:25:38_seeds-2023/checkpoint-5700/controlnet'\n",
    "control_depth_trained_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_h-512_w-512_2024-07-16_17:42:27_seeds-2024/checkpoint-2000/controlnet'\n",
    "\n",
    "# controlnet=[controlnet_corrosion,controlnet_members, controlnet_depth]\n",
    "controlnet_conditioning_scale_list=[1.0,1.0,1.0]\n",
    "\n",
    "# 有members，无corrosion，无depth\n",
    "if mask_members_dir is not None and mask_corrosion_dir is None and mask_depth_dir is None:\n",
    "    mask_members_pil = load_image(mask_members_dir)\n",
    "    validation_image = mask_members_pil\n",
    "    control_members_dir = control_members_trained_dir\n",
    "    print('mask_members_dir is not None and mask_corrosion_dir is None and mask_depth_dir is None')\n",
    "    controlnet_members = ControlNetModel.from_pretrained(control_members_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=controlnet_members).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    print(mask_members_pil.mode)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    # Image.show(image)\n",
    "# 无members，有corrosion，无depth\n",
    "elif mask_members_dir is  None and mask_corrosion_dir is not None and mask_depth_dir is None:\n",
    "    control_corrosion_dir = control_corrosion_trained_dir\n",
    "    mask_corrosion_pil = load_image(mask_corrosion_dir)\n",
    "    validation_image = mask_corrosion_pil\n",
    "    print('mask_members_dir is  None and mask_corrosion_dir is not None and mask_depth_dir is None')\n",
    "    controlnet_corrosion = ControlNetModel.from_pretrained(control_corrosion_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=controlnet_corrosion).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "# 无members，无corrosion，有depth  \n",
    "elif mask_members_dir is  None and mask_corrosion_dir is None and mask_depth_dir is not None:\n",
    "    # control_depth_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_depth_h-512_w-512_2024-07-16_11:25:38_seeds-2023/checkpoint-5700/controlnet'\n",
    "    control_depth_dir = control_depth_trained_dir\n",
    "    \n",
    "    mask_depth_pil = load_image(mask_depth_dir)\n",
    "    validation_image = mask_depth_pil\n",
    "    print('mask_members_dir is  None and mask_corrosion_dir is None and mask_depth_dir is not None')\n",
    "    controlnet_depth = ControlNetModel.from_pretrained(control_depth_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=controlnet_depth).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mask_depth_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "# 有members，有corrosion，无depth\n",
    "elif mask_members_dir is not None and mask_corrosion_dir is not None and mask_depth_dir is None:\n",
    "    control_corrosion_dir = control_corrosion_trained_dir\n",
    "    control_members_dir = control_members_trained_dir\n",
    "    mask_members_pil = load_image(mask_members_dir)\n",
    "    mask_corrosion_pil = load_image(mask_corrosion_dir)\n",
    "    validation_image = [mask_corrosion_pil, mask_members_pil]\n",
    "    print('mask_members_dir is not None and mask_corrosion_dir is not None and mask_depth_dir is None')\n",
    "    controlnet_corrosion = ControlNetModel.from_pretrained(control_corrosion_dir)\n",
    "    controlnet_members = ControlNetModel.from_pretrained(control_members_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=[controlnet_corrosion, controlnet_members]).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    print(mask_corrosion_pil.mode)\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    print(mask_members_pil.mode)\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(144)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "# 无members，有corrosion，有depth \n",
    "elif mask_members_dir is  None and mask_corrosion_dir is not None and mask_depth_dir is not None:\n",
    "    control_corrosion_dir = control_corrosion_trained_dir\n",
    "    # control_depth_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_depth_h-512_w-512_2024-07-16_11:25:38_seeds-2023/checkpoint-5700/controlnet'\n",
    "    control_depth_dir = control_depth_trained_dir\n",
    "    \n",
    "    mask_depth_pil = load_image(mask_depth_dir)\n",
    "    mask_corrosion_pil = load_image(mask_corrosion_dir)\n",
    "    validation_image = [mask_corrosion_pil, mask_depth_pil]\n",
    "    print('mask_members_dir is  None and mask_corrosion_dir is not None and mask_depth_dir is not None')\n",
    "    controlnet_corrosion = ControlNetModel.from_pretrained(control_corrosion_dir)\n",
    "    controlnet_depth = ControlNetModel.from_pretrained(control_depth_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=[controlnet_corrosion, controlnet_depth]).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    print(mask_corrosion_pil.mode)\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(mask_depth_pil)\n",
    "    print(mask_depth_pil.mode)\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(144)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "# 有members，无corrosion，有depth\n",
    "elif mask_members_dir is not None and mask_corrosion_dir is None and mask_depth_dir is not None:\n",
    "    control_members_dir = control_members_trained_dir\n",
    "    # control_depth_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_depth_h-512_w-512_2024-07-16_11:25:38_seeds-2023/checkpoint-5700/controlnet'\n",
    "    control_depth_dir = control_depth_trained_dir\n",
    "    \n",
    "    mask_depth_pil = load_image(mask_depth_dir)\n",
    "    mask_members_pil = load_image(mask_members_dir)\n",
    "    validation_image = [mask_members_pil, mask_depth_pil]\n",
    "    print('mask_members_dir is not None and mask_corrosion_dir is None and mask_depth_dir is not None')\n",
    "    controlnet_members = ControlNetModel.from_pretrained(control_members_dir)\n",
    "    controlnet_depth = ControlNetModel.from_pretrained(control_depth_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=[controlnet_members, controlnet_depth]).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    print(mask_members_pil.mode)\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(mask_depth_pil)\n",
    "    print(mask_depth_pil.mode)\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(144)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "# 有members，有corrosion，有depth\n",
    "elif mask_members_dir is not None and mask_corrosion_dir is not None and mask_depth_dir is not None:\n",
    "    control_corrosion_dir = control_corrosion_trained_dir\n",
    "    control_members_dir = control_members_trained_dir\n",
    "    # control_depth_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_depth_h-512_w-512_2024-07-16_11:25:38_seeds-2023/checkpoint-5700/controlnet'\n",
    "    control_depth_dir = control_depth_trained_dir\n",
    "    \n",
    "    mask_corrosion_pil = load_image(mask_corrosion_dir)\n",
    "    mask_members_pil = load_image(mask_members_dir)\n",
    "    mask_depth_pil = load_image(mask_depth_dir)\n",
    "    validation_image = [mask_corrosion_pil, mask_members_pil, mask_depth_pil]\n",
    "    print('mask_members_dir is not None and mask_corrosion_dir is not None and mask_depth_dir is not None')\n",
    "    controlnet_corrosion = ControlNetModel.from_pretrained(control_corrosion_dir)\n",
    "    controlnet_members = ControlNetModel.from_pretrained(control_members_dir)\n",
    "    controlnet_depth = ControlNetModel.from_pretrained(control_depth_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=[controlnet_corrosion,controlnet_members, controlnet_depth], controlnet_conditioning_scale=controlnet_conditioning_scale_list).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(151)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    plt.subplot(152)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    print(mask_members_pil.mode)\n",
    "    plt.subplot(153)\n",
    "    plt.imshow(mask_depth_pil)\n",
    "    print(mask_depth_pil.mode)\n",
    "    plt.subplot(154)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(155)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "# 无members，无corrosion，无depth\n",
    "else:\n",
    "    print('Input nothing')\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_corrosion_np = np.array(mask_corrosion_pil)\n",
    "mask_corrosion_np.min(), mask_corrosion_np.max(), mask_corrosion_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if mask_members_dir is not None and mask_corrosion_dir is None:\n",
    "\n",
    "#     plt.figure(figsize=(16, 8))\n",
    "#     plt.subplot(121)\n",
    "#     plt.imshow(mask_members_pil)\n",
    "#     plt.subplot(122)\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()\n",
    "#     # Image.show(image)\n",
    "    \n",
    "# elif mask_members_dir is  None and mask_corrosion_dir is not None:\n",
    "\n",
    "#     plt.figure(figsize=(16, 8))\n",
    "#     plt.subplot(121)\n",
    "#     plt.imshow(mask_corrosion_pil)\n",
    "#     plt.subplot(122)\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()\n",
    "# else:\n",
    "\n",
    "#     plt.figure(figsize=(16, 8))\n",
    "#     plt.subplot(131)\n",
    "#     plt.imshow(mask_corrosion_pil)\n",
    "#     plt.subplot(132)\n",
    "#     plt.imshow(mask_members_pil)\n",
    "#     plt.subplot(133)\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PNDMScheduler\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                ).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPMScheduler\n",
    "from diffusers import DDPMScheduler\n",
    "import os\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "scheduler_ddpm = DDPMScheduler.from_pretrained(os.path.join(config_dir, 'scheduler'))\n",
    "\n",
    "# print(pipeline.scheduler)\n",
    "pipeline.scheduler = scheduler_ddpm\n",
    "# print(pipeline.scheduler)\n",
    "image = pipeline(\n",
    "                   prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                ).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mask_members_dir is not None and mask_corrosion_dir is None:\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    # Image.show(image)\n",
    "    \n",
    "elif mask_members_dir is  None and mask_corrosion_dir is not None:\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "else:\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(144)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDIMScheduler\n",
    "from diffusers import DDIMScheduler\n",
    "import os\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "scheduler_ddim = DDIMScheduler.from_pretrained(os.path.join(config_dir, 'scheduler'))\n",
    "\n",
    "print(pipeline.scheduler)\n",
    "pipeline.scheduler = scheduler_ddim\n",
    "print(pipeline.scheduler)\n",
    "image = pipeline(\n",
    "                   prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                ).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PNDMScheduler\n",
    "from diffusers import PNDMScheduler\n",
    "import os\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "scheduler_pndm = PNDMScheduler.from_pretrained(os.path.join(config_dir, 'scheduler'))\n",
    "\n",
    "print(pipeline.scheduler)\n",
    "pipeline.scheduler = scheduler_pndm\n",
    "print(pipeline.scheduler)\n",
    "image = pipeline(\n",
    "                   prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                ).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***##step by step denoise##***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "control_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_crack_only_generate/checkpoint-2000/controlnet'\n",
    "config_dir = '/home/ubunto/Project/konglx/generate/ControlNet/models/stable-diffusion-v1-5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.创建各个部分的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler, ControlNetModel\n",
    "import os\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(config_dir, subfolder='vae', use_safetensors=None)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(config_dir, subfolder='tokenizer')\n",
    "text_encoder = CLIPTextModel.from_pretrained(config_dir, subfolder='text_encoder', use_safetensors=None)\n",
    "unet = UNet2DConditionModel.from_pretrained(config_dir, subfolder='unet', use_safetensors=None)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(control_dir)\n",
    "controlnet.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.config.scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UniPCMultistepScheduler\n",
    "\n",
    "scheduler_multistep = UniPCMultistepScheduler.from_pretrained(config_dir, subfolder=\"scheduler\")\n",
    "scheduler_multistep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = \"cuda\"\n",
    "vae.to(torch_device)\n",
    "text_encoder.to(torch_device)\n",
    "unet.to(torch_device)\n",
    "controlnet.to(torch_device)\n",
    "unet.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Create embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"crack\"]\n",
    "device = 'cuda'\n",
    "seed = 0\n",
    "height = 512  # default height of Stable Diffusion\n",
    "width = 512  # default width of Stable Diffusion\n",
    "num_inference_steps = 25  # Number of denoising steps\n",
    "guidance_scale = 7.5  # Scale for classifier-free guidance\n",
    "generator = torch.Generator(device=device).manual_seed(seed)  # Seed generator to create the initial latent noise\n",
    "batch_size = len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tokenizer(\n",
    "    prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "prompt_embeds = text_embeddings\n",
    "encoder_hidden_states_control = text_embeddings\n",
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "mask_dir = '/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/crack（复件）/SegmentationClass/DeepCrack_11240-6.png'\n",
    "validation_image = Image.open(mask_dir).convert(\"RGB\")\n",
    "validation_image.size\n",
    "\n",
    "conditioning_image_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(512),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "conditioning_image_pil = validation_image.resize([512, 512])\n",
    "conditioning_pixel_values = torch.stack([conditioning_image_transforms(conditioning_image_pil)])\n",
    "conditioning_pixel_values = conditioning_pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "print(conditioning_pixel_values.shape)\n",
    "controlnet_image = conditioning_pixel_values.to(torch_device)\n",
    "# print(controlnet_image)\n",
    "conditioning_image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You’ll also need to generate the unconditional text embeddings which are the embeddings for the padding token.\n",
    "These need to have the same shape (batch_size and seq_length) as the conditional text_embeddings:\n",
    "'''\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "uncond_embeddings.shape, text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s concatenate the conditional and unconditional embeddings into a batch to avoid doing two forward passes:\n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "# text_embeddings\n",
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Create random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Next, generate some initial random noise as a starting point for the diffusion process. \n",
    "This is the latent representation of the image, and it’ll be gradually denoised. \n",
    "At this point, the latent image is smaller than the final image size but that’s okay though \n",
    "because the model will transform it into the final 512x512 image dimensions later.\n",
    "'''\n",
    "\n",
    "# The height and width are divided by 8 because the vae model has 3 down-sampling layers.\n",
    "# You can check by running the following:   2 ** (len(vae.config.block_out_channels) - 1) == 8\n",
    "\n",
    "do_classifier_free_guidance = False\n",
    "guess_mode = False\n",
    "\n",
    "latents = torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    "    device=torch_device,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "                    latents,\n",
    "                    2,\n",
    "                    encoder_hidden_states=encoder_hidden_states_control,\n",
    "                    controlnet_cond=controlnet_image,\n",
    "                    return_dict=False,\n",
    "                )\n",
    "len(down_block_res_samples), down_block_res_samples[-1].shape, len(mid_block_res_sample), mid_block_res_sample[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Denoise the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Start by scaling the input with the initial noise distribution, sigma, the noise scale value, \n",
    "which is required for improved schedulers like UniPCMultistepScheduler: \n",
    "'''\n",
    "print(scheduler_multistep.init_noise_sigma)\n",
    "latents = latents * scheduler_multistep.init_noise_sigma\n",
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "The last step is to create the denoising loop that’ll progressively \n",
    "transform the pure noise in latents to an image described by your prompt.\n",
    "Remember, the denoising loop needs to do three things:\n",
    "\n",
    "1.Set the scheduler’s timesteps to use during denoising.\n",
    "2.Iterate over the timesteps.\n",
    "3.At each timestep, call the UNet model to predict the noise residual and \n",
    "pass it to the scheduler to compute the previous noisy sample.\n",
    "\n",
    "'''\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "scheduler_multistep.set_timesteps(num_inference_steps)\n",
    "\n",
    "for t in tqdm(scheduler_multistep.timesteps):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "    latent_model_input = scheduler_multistep.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler_multistep.step(noise_pred, t, latents).prev_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "pil_latents = transforms.ToPILImage()(latents.squeeze(0))\n",
    "pil_latents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Decode the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and decode the image latents with vae\n",
    "latents_scaled = 1 / vae.config.scaling_factor * latents\n",
    "# pil_latents_scaled = transforms.ToPILImage()(latents.squeeze(0))\n",
    "# pil_latents_scaled = transforms.ToPILImage()(latents_scaled.squeeze(0))\n",
    "with torch.no_grad():\n",
    "    # image = vae.decode(latents).sample\n",
    "    image = vae.decode(latents_scaled).sample\n",
    "print(image.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (image / 2 + 0.5).clamp(0, 1).squeeze()\n",
    "image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n",
    "image = Image.fromarray(image)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "The last step is to create the denoising loop that’ll progressively \n",
    "transform the pure noise in latents to an image described by your prompt.\n",
    "Remember, the denoising loop needs to do three things:\n",
    "\n",
    "1.Set the scheduler’s timesteps to use during denoising.\n",
    "2.Iterate over the timesteps.\n",
    "3.At each timestep, call the UNet model to predict the noise residual and \n",
    "pass it to the scheduler to compute the previous noisy sample.\n",
    "\n",
    "'''\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "scheduler_multistep.set_timesteps(num_inference_steps)\n",
    "\n",
    "for t in tqdm(scheduler.timesteps):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    # expand the latents if we are doing classifier free guidance\n",
    "    latent_model_input_control = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "    # latent_model_input_control = torch.cat([latents_control] * 2)\n",
    "\n",
    "    latent_model_input_control = scheduler.scale_model_input(latent_model_input_control, timestep=t)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        noise_pred_control = unet(latent_model_input_control, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond_control, noise_pred_text_control = noise_pred_control.chunk(2)\n",
    "    noise_pred_control = noise_pred_uncond_control+ guidance_scale * (noise_pred_text_control - noise_pred_uncond_control)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler.step(noise_pred_control, t, latents).prev_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = any\n",
    "a in 'abc'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
