{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline, DDIMPipeline, DDIMScheduler\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import PIL\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ''\n",
    "\n",
    "device = 'cuda'\n",
    "seed = 2024\n",
    "noisy_sample = torch.randn(\n",
    "    1, 4, 64, 64\n",
    ").to(device)\n",
    "# noisy_sample = torch.randn(\n",
    "#     1, 4, 512, 512\n",
    "# ).to('cuda:0')\n",
    "# noisy_sample = torch.randn(\n",
    "#     1, 4, 56, 56\n",
    "# ).to('cuda:0')\n",
    "# prompt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample(sample, i):\n",
    "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
    "    image_processed = (image_processed + 1.0) * 127.5\n",
    "    image_processed = image_processed.numpy().astype(np.uint8)\n",
    "\n",
    "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
    "    display(f\"Image at step {i}\")\n",
    "    display(image_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Â°´ÂÖÖËÖêËöÄÂå∫ÂüüÔºàÂâçÊèêË¶ÅÊúâmembersÁöÑmaskÔºâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Âü∫‰∫émembersÁöÑmaskÔºåÁîüÊàêËÖêËöÄÔºàËÖêËöÄÂΩ¢Áä∂ÔºöÂ§öËæπÂΩ¢ÔºõËÖêËöÄÊï∞ÈáèÔºöËá™ÂÆö‰πâÔºå‰∏ÄÂÆöËåÉÂõ¥Ëá™Âä®Á°ÆÂÆöÔºõËÖêËöÄÂ∞∫Â∫¶Ôºö‰∏ÄÂÆöËåÉÂõ¥ÈöèÊú∫ÁîüÊàêÔºâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def times(x, y):\n",
    "    return x * y\n",
    "type(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ÔºÅÔºÅÂΩ¢Áä∂ÂíåÂΩ¢Áä∂ÁöÑÊï∞ÈáèÂèØÊéßÔºåÊçü‰º§ÁöÑÁ±ªÂûãÂèØÊéßÔºà‰ª•ËÖêËöÄ‰∏∫‰æãÔºâ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512)\n",
      "145005\n",
      "170\n",
      "252\n",
      "85\n",
      "255\n",
      "55\n",
      "328\n",
      "136\n",
      "183\n",
      "182\n",
      "147\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=P size=512x512>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "mask_members_np_demo = np.array(Image.open(mask_members_dir))\n",
    "print(mask_members_np_demo.shape)\n",
    "\n",
    "random.seed(4)\n",
    "\n",
    "########### ÂÆö‰πâÈ¢úËâ≤ËΩ¨Êç¢ÊñπÊ≥ï ##############\n",
    "import imgviz\n",
    "\n",
    "def colored_mask(mask, save_path=None):\n",
    "    lbl_pil = Image.fromarray(mask.astype(np.uint8), mode=\"P\")\n",
    "    colormap = imgviz.label_colormap()\n",
    "    # print(colormap)\n",
    "    lbl_pil.putpalette(colormap.flatten())\n",
    "    if save_path is not None:\n",
    "        lbl_pil.save(save_path)\n",
    "\n",
    "    return lbl_pil\n",
    "\n",
    "\n",
    "\n",
    "def generate_ellipses(arr, fill_value=1, values=[5], num_ellipses=10, a_min=20, a_max=50, b_min=10, b_max=30):\n",
    "    \"\"\"\n",
    "    üïêÁî®Â∞ëÊï∞ÈáèÁöÑÂ§ßÊ§≠ÂúÜÔºåÊ®°ÊãüÂ§ßÁâáÁöÑËÖêËöÄÔºõ\n",
    "    üïëÁî®Â§öÊï∞ÈáèÁöÑÂ∞èÊ§≠ÂúÜÔºåÊ®°Êãüpitting corrosion\n",
    "    Generate random quadrilaterals within the specified regions of a NumPy array\n",
    "    and fill them with a specified value.\n",
    "\n",
    "    Args:\n",
    "        arr (numpy.ndarray): The input array.\n",
    "        num_polygons (int): The number of quadrilaterals to generate.\n",
    "        fill_value (int, optional): The value to use for filling the quadrilaterals. Default is 1.\n",
    "        values (list or tuple, optional): A list or tuple of values to consider as the selected region.\n",
    "                                          Default is [1].\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A new array with the same shape as the input array,\n",
    "                       containing the generated and filled quadrilaterals.\n",
    "    \"\"\"\n",
    "    \n",
    "    selected_region = np.isin(arr, values)\n",
    "    # print(selected_region)\n",
    "    selected_coords = np.argwhere(selected_region)\n",
    "    print(len(selected_coords))\n",
    "    # # Ê§≠ÂúÜÊï∞ÈáèÂíåËåÉÂõ¥\n",
    "    # num_ellipses = 5\n",
    "    # a_min, a_max = 20, 50\n",
    "    # b_min, b_max = 10, 30\n",
    "    \n",
    "    result = arr.copy()\n",
    "    image = Image.fromarray(result)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Á©∫ÁôΩmask\n",
    "    result_blank = np.zeros_like(result)\n",
    "    # print(result_blank)\n",
    "    image_blank = Image.fromarray(result_blank)\n",
    "    draw_blank = ImageDraw.Draw(image_blank)\n",
    "    # # ÁîüÊàêÂõæÁâá\n",
    "    # im = Image.new('RGB', (100,100), color='white')  \n",
    "    # draw = ImageDraw.Draw(im)\n",
    "\n",
    "    for i in range(num_ellipses):\n",
    "        xy_arr = random.sample(list(selected_coords), k=1)\n",
    "        # print(random.sample(list(selected_coords), k=2), random.sample(list(selected_coords), k=3))\n",
    "        print(xy_arr[0][0])\n",
    "        y, x = xy_arr[0][0], xy_arr[0][1]\n",
    "        # ÈöèÊú∫ÁîüÊàêÈïøÁü≠ÂçäËΩ¥\n",
    "        a = random.randint(a_min, a_max)\n",
    "        b = random.randint(b_min, b_max)\n",
    "        \n",
    "        # ÈöèÊú∫‰∏≠ÂøÉÁÇπ\n",
    "        # x = random.randint(0, 100-a)\n",
    "        # y = random.randint(0, 100-b)\n",
    "        \n",
    "        # ÁªòÂà∂Ê§≠ÂúÜ\n",
    "        draw.ellipse((x, y, x+a, y+b), fill=fill_value)\n",
    "        draw_blank.ellipse((x, y, x+a, y+b), fill=fill_value)\n",
    "    \n",
    "    result = np.array(image)\n",
    "    result_mask = np.array(image_blank)\n",
    "    \n",
    "    return result, result_mask\n",
    "    \n",
    "result_e, result_mask_e = generate_ellipses(arr=mask_members_np_demo)\n",
    "colored_mask(result_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "# img_name = 'IMG_20230715_152221'\n",
    "select_area_list = [1]\n",
    "img_name = '120'\n",
    "org_img_dir = f'/home/ubunto/Project/konglx/generate/ControlNet-v1-1-nightly/training/corrosion_and_crack/corrosion/JPEGImages/{img_name}.jpg'\n",
    "org_img_pil = load_image(org_img_dir)\n",
    "# print(org_img_pil.mode)\n",
    "mask_corrosion_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/corrosion/conditioning_images/{img_name}.png'\n",
    "mask_members_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/members/conditioning_images/{img_name}.png'\n",
    "\n",
    "# mask_members_dir = None\n",
    "mask_depth_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/depth/conditioning_images/{img_name}.png'\n",
    "###################################################################################\n",
    "\n",
    "########### ÂÆö‰πâÈ¢úËâ≤ËΩ¨Êç¢ÊñπÊ≥ï ##############\n",
    "import imgviz\n",
    "\n",
    "def colored_mask(mask, save_path=None):\n",
    "    lbl_pil = Image.fromarray(mask.astype(np.uint8), mode=\"P\")\n",
    "    colormap = imgviz.label_colormap()\n",
    "    # print(colormap)\n",
    "    lbl_pil.putpalette(colormap.flatten())\n",
    "    if save_path is not None:\n",
    "        lbl_pil.save(save_path)\n",
    "\n",
    "    return lbl_pil\n",
    "\n",
    "\n",
    "########### ÂÆö‰πâÂü∫‰∫énumpyÈÄâÊã©ÊûÑ‰ª∂ÁöÑÂáΩÊï∞#####\n",
    "\n",
    "def filter_array(arr, values):\n",
    "    \"\"\"\n",
    "    Filter a NumPy array to keep only the specified values.\n",
    "\n",
    "    Args:\n",
    "        arr (numpy.ndarray): The input array to be filtered.\n",
    "        values (list or tuple): A list or tuple of values to keep in the filtered array.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A new array with the same shape as the input array,\n",
    "                       containing only the specified values.\n",
    "    \"\"\"\n",
    "    conditions = [arr == value for value in values]\n",
    "    condition = np.logical_or.reduce(conditions)\n",
    "    filtered_arr = np.where(condition, arr, 0)\n",
    "    return filtered_arr\n",
    "#########################################\n",
    "\n",
    "########## ÂÆö‰πâÁîüÊàê‰ªªÊÑèÂ§öËæπÂΩ¢ ##############\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "def generate_polygon(arr, num_polygons, fill_value=1, values=[1], polygon_shape=4):\n",
    "    \"\"\"\n",
    "    Generate random quadrilaterals within the specified regions of a NumPy array\n",
    "    and fill them with a specified value.\n",
    "\n",
    "    Args:\n",
    "        arr (numpy.ndarray): The input array.\n",
    "        num_polygons (int): The number of quadrilaterals to generate.\n",
    "        fill_value (int, optional): The value to use for filling the quadrilaterals. Default is 1.\n",
    "        values (list or tuple, optional): A list or tuple of values to consider as the selected region.\n",
    "                                          Default is [1].\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A new array with the same shape as the input array,\n",
    "                       containing the generated and filled quadrilaterals.\n",
    "    \"\"\"\n",
    "    selected_region = np.isin(arr, values)\n",
    "    selected_coords = np.argwhere(selected_region)\n",
    "\n",
    "    result = arr.copy()\n",
    "    image = Image.fromarray(result)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Á©∫ÁôΩmask\n",
    "    result_blank = np.zeros_like(result)\n",
    "    # print(result_blank)\n",
    "    image_blank = Image.fromarray(result_blank)\n",
    "    draw_blank = ImageDraw.Draw(image_blank)\n",
    "\n",
    "    for _ in range(num_polygons):\n",
    "        coords = random.sample(list(selected_coords), polygon_shape)\n",
    "        polygon_coords = [(coord[1], coord[0]) for coord in coords]\n",
    "        draw.polygon(polygon_coords, fill=fill_value)\n",
    "        \n",
    "        draw_blank.polygon(polygon_coords, fill=fill_value)\n",
    "\n",
    "    result = np.array(image)\n",
    "    result_mask = np.array(image_blank)\n",
    "    # plt.imshow(result_mask)\n",
    "    return result, result_mask\n",
    "#########################################\n",
    "\n",
    "###### ÂÆö‰πâÈÄâÂÆöÂå∫ÂüüÂÜÖÁîüÊàêËã•Âπ≤Êï∞ÈáèÁöÑËÖêËöÄ#######\n",
    "def generate_polygon_to_want_area(org_img_np, generate_polygon, fill_value, want_area = [1,4]):\n",
    "    \"\"\"\n",
    "    generate_polygon_to_want_area based on function 'generate_polygon'.\n",
    "\n",
    "    Args:\n",
    "        org_img_np (numpy.ndarray): The input array.\n",
    "        generate_polygon (function): generate_polygon\n",
    "        want_area_list (list or tuple, optional): A list or tuple of values to consider as the selected region.\n",
    "                                          Default is [1,4].\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A new array with the same shape as the input array,\n",
    "                       containing the generated and filled polygon.\n",
    "    \"\"\"\n",
    "    list_area_corrosion = []\n",
    "    list_area_only_corrosion = []\n",
    "    for area in want_area:\n",
    "        gen_polygon, gen_polygon_blank = generate_polygon(arr=org_img_np, num_polygons=1, values=[area], fill_value=fill_value, polygon_shape=5)\n",
    "        list_area_corrosion.append(gen_polygon)\n",
    "        list_area_only_corrosion.append(gen_polygon_blank)\n",
    "    sum_area = np.sum(list_area_corrosion, axis=0)\n",
    "    sum_area_only_corrosion = np.sum(list_area_only_corrosion, axis=0)\n",
    "    return sum_area, list_area_corrosion, sum_area_only_corrosion, list_area_only_corrosion\n",
    "\n",
    "mask_corrosion_pil_org = load_image(mask_corrosion_dir)\n",
    "mask_corrosion_pil_org.mode, np.array(mask_corrosion_pil_org).min(), np.array(mask_corrosion_pil_org).max(), np.array(org_img_pil).min(), np.array(org_img_pil).max(), transforms.ToTensor()(org_img_pil).min(),transforms.ToTensor()(org_img_pil).max(), transforms.ToTensor()(mask_corrosion_pil_org).min(),transforms.ToTensor()(mask_corrosion_pil_org).max()\n",
    "\n",
    "seed_setting = 7 # Â±ïÁ§∫Áî®ÁöÑÊòØseed_setting=16\n",
    "fill_value_setting = 12 # ‰ª•12 ‰∏∫Â°´ÂÖÖÊï∞ÊçÆÔºå‰∏émemberstypeÂàÜÂºÄÔºåËÄåÂêéÂÜçÈÄöËøácorrosionType_to_fill_valueËøîÂõûÂØπÂ∫îcorrosionType\n",
    "\n",
    "corrosionType = [3] # ËÖêËöÄÁ±ªÂûãÔºö 1-fairÔºå2-poorÔºå 3-severe\n",
    "def corrosionType_to_fill_value(corrosionType):\n",
    "    fill_value = []\n",
    "    for i in corrosionType:\n",
    "        if i==1:\n",
    "            fill_value.append(12)\n",
    "        elif i==2:\n",
    "            fill_value.append(6)\n",
    "        elif i==3:\n",
    "            fill_value.append(4)\n",
    "    return fill_value\n",
    "\n",
    "fill_value_list = corrosionType_to_fill_value(corrosionType=corrosionType) # ‰ª•12‰∏∫ÊÄªÊï∞ÈáèÔºåfill_value‰∏écorrosionTypeÂØπÂ∫îÔºöËÖêËöÄÁ±ªÂûãÔºö 1-fairÂØπÂ∫î12/12Ôºå2-poorÂØπÂ∫î12/6Ôºå 3-severeÂØπÂ∫î12/4\n",
    "\n",
    "\n",
    "random.seed(seed_setting)  # 7\n",
    "# 1.membersÁöÑmask\n",
    "mask_members_pil_org = Image.open(mask_members_dir)\n",
    "plt.figure(figsize=(16,7))\n",
    "plt.subplot(241)\n",
    "plt.imshow(mask_members_pil_org)\n",
    "# mask_members_pt_mode_R = transforms.ToTensor()(mask_members_pil_org)  # mode‰∏∫PÁöÑPILÔºåÁªèËøáToTensorËΩ¨‰∏∫‰∫Ü[0,1]Âå∫Èó¥ÂÜÖ\n",
    "# print(transforms.ToTensor()(mask_members_pil_org).unique())\n",
    "mask_members_np_mode_R = np.array(mask_members_pil_org) # Â∞ÜPILËΩ¨‰∏∫npÔºå‰∏çÊîπÂèòÊï∞ÂÄº\n",
    "mask_members_pt_mode_R_from_np = torch.from_numpy(mask_members_np_mode_R)\n",
    "# print(mask_members_pt_mode_R_from_np.min(), mask_members_pt_mode_R_from_np.max(), mask_members_pt_mode_R_from_np.shape)\n",
    "\n",
    "mask_members_np_mode_R_filter = filter_array(mask_members_np_mode_R, [1,4])\n",
    "# 2.ÈÄâÊã©membersÂå∫Âüü\n",
    "plt.subplot(242)\n",
    "plt.imshow(mask_members_np_mode_R_filter)\n",
    "print(mask_members_np_mode_R_filter.shape, mask_members_np_mode_R_filter.min(), mask_members_np_mode_R_filter.max(), np.unique(mask_members_np_mode_R_filter))\n",
    "# print(mask_members_np_mode_R[mask_members_np_mode_R==1].reshape(mask_members_np_mode_R.shape[0], mask_members_np_mode_R.shape[1]))\n",
    "# mask_members_pt_mode_R.min(), mask_members_pt_mode_R.max(), mask_members_np_mode_R.min(), mask_members_np_mode_R.max(), mask_members_np_mode_R.shape\n",
    "transforms.ToTensor()(mask_members_np_mode_R_filter).max()\n",
    "\n",
    "# 3.Âú®membersÂå∫ÂüüÁîüÊàê‰ªªÊÑèÊï∞ÈáèÔºå‰ªªÊÑèÊï∞ÂÄºÁöÑcorrosionÁöÑmask ÔºàÂ±ïÁ§∫Áî®Ôºâ\n",
    "\n",
    "mask_members_with_gen_corrosion, mask_members_with_gen_corrosion_blank = generate_polygon(arr=mask_members_np_mode_R_filter, num_polygons=1, values=[4], fill_value=fill_value_setting, polygon_shape=5)\n",
    "plt.subplot(243)\n",
    "plt.imshow(mask_members_with_gen_corrosion)\n",
    "\n",
    "# 4.Âú®membersÊåáÂÆöÂå∫ÂüüÁîüÊàê‰ªªÊÑèÊï∞ÈáèÁöÑËÖêËöÄÔºåËøîÂõû‰ªÖ‰ªÖ‰∏∫ËÖêËöÄÂå∫Âüü ÔºàÁîüÊàêÁî®Ôºâ\n",
    "mask_corrosion_corr_to_members, mask_corrosion_corr_to_members_list, mask_only_corrosion_corr_to_members, mask_only_corrosion_corr_to_members_list = generate_polygon_to_want_area(org_img_np=mask_members_np_mode_R, generate_polygon=generate_polygon, fill_value=fill_value_setting)\n",
    "plt.subplot(244)\n",
    "plt.imshow(mask_corrosion_corr_to_members)\n",
    "plt.title('Generate corrosion mask with members')\n",
    "\n",
    "# plt.subplot(245)\n",
    "# plt.imshow(mask_only_corrosion_corr_to_members)\n",
    "\n",
    "# print(np.unique(mask_only_corrosion_corr_to_members))\n",
    "# plt.subplot(246)\n",
    "# plt.imshow(mask_only_corrosion_corr_to_members/12)\n",
    "\n",
    "mask_only_corrosion_corr_to_members_devide_by_fill_value = (mask_only_corrosion_corr_to_members/fill_value_list[0]).astype((np.uint64))\n",
    "plt.subplot(247)\n",
    "plt.imshow(mask_only_corrosion_corr_to_members_devide_by_fill_value)\n",
    "print(mask_only_corrosion_corr_to_members_devide_by_fill_value.min(), mask_only_corrosion_corr_to_members_devide_by_fill_value.max(), np.unique(mask_only_corrosion_corr_to_members_devide_by_fill_value), mask_only_corrosion_corr_to_members_devide_by_fill_value.dtype)\n",
    "# print(np.unique(mask_only_corrosion_corr_to_members/12), mask_only_corrosion_corr_to_members.dtype, ((mask_only_corrosion_corr_to_members/12).astype(np.uint64)).dtype)\n",
    "\n",
    "mask_only_corrosion_final_pil = colored_mask(mask_only_corrosion_corr_to_members_devide_by_fill_value)\n",
    "plt.subplot(248)\n",
    "plt.imshow(mask_only_corrosion_final_pil)\n",
    "print(mask_only_corrosion_final_pil.mode)\n",
    "plt.title('Final generate mask')\n",
    "\n",
    "# Â≠òÂÇ®\n",
    "save_mask_dir = f'/home/ubunto/Project/konglx/generate/diffusers/output_imgs/Èí¢ÊûÑ‰ª∂ÁîüÊàêÁöÑËÖêËöÄmask/{img_name}__seed_{seed_setting}__corrosionType_{corrosionType[0]}.png'\n",
    "mask_only_corrosion_final_pil.save(save_mask_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paste_pil_a_to_b(a_pil, b_pil):\n",
    "    a_pil_rgba = a_pil.convert('RGBA')\n",
    "    b_pil_rgba = b_pil.convert('RGBA')\n",
    "    a_b_pil_rgba = Image.blend(a_pil_rgba, b_pil_rgba, 0.5)\n",
    "    a_b_pil_rgb = a_b_pil_rgba.convert('RGB')\n",
    "    return a_b_pil_rgb\n",
    "\n",
    "gen_corrosion_with_org_img = paste_pil_a_to_b(org_img_pil, mask_only_corrosion_final_pil)\n",
    "gen_corrosion_with_org_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ËΩΩÂÖ•ÂõæÁâá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask_dir = '/home/ubunto/ÂõæÁâá/overture-creations-5sI6fQgYIuo_mask.png'\n",
    "# img_name = '2__1__1848___924'\n",
    "img_name = '120'\n",
    "corrosiont_type = corrosionType[0]\n",
    "print('corrosionType[0]:', corrosionType[0])\n",
    "# img_name = 'IMG_20230715_152221'\n",
    "org_img_dir = f'/home/ubunto/Project/konglx/generate/ControlNet-v1-1-nightly/training/corrosion_and_crack/corrosion/JPEGImages/{img_name}.jpg'\n",
    "org_img_pil = Image.open(org_img_dir)\n",
    "print(org_img_pil.mode)\n",
    "# mask_corrosion_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/corrosion/conditioning_images/{img_name}.png'\n",
    "# mask_corrosion_dir = f'/home/ubunto/Project/konglx/generate/diffusers/output_imgs/Èí¢ÊûÑ‰ª∂ÁîüÊàêÁöÑËÖêËöÄmask/{img_name}__seed_16__corrosionType_{corrosiont_type}.png'\n",
    "# mask_corrosion_dir = f'/home/ubunto/Project/konglx/generate/diffusers/output_imgs/Èí¢ÊûÑ‰ª∂ÁîüÊàêÁöÑËÖêËöÄmask/{img_name}__seed_16__corrosionType_1.png'\n",
    "# mask_corrosion_dir = f'/home/ubunto/Project/konglx/generate/diffusers/output_imgs/Èí¢ÊûÑ‰ª∂ÁîüÊàêÁöÑËÖêËöÄmask/{img_name}__seed_16__corrosionType_2.png'\n",
    "mask_corrosion_dir = save_mask_dir\n",
    "\n",
    "mask_members_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/members/conditioning_images/{img_name}.png'\n",
    "# mask_members_dir = None\n",
    "mask_depth_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/depth/conditioning_images/{img_name}.png'\n",
    "# mask_depth_dir = None\n",
    "\n",
    "if mask_depth_dir is None:\n",
    "    # c = 3\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.title('Corrosion mask')\n",
    "    plt.imshow(Image.open(mask_corrosion_dir))\n",
    "    plt.subplot(133)\n",
    "    plt.title('Members mask')\n",
    "    plt.imshow(Image.open(mask_members_dir))\n",
    "elif mask_members_dir is None:\n",
    "    # c = 3\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.title('Corrosion mask')\n",
    "    plt.imshow(Image.open(mask_corrosion_dir))\n",
    "    plt.subplot(133)\n",
    "    plt.title('Depth mask')\n",
    "    plt.imshow(Image.open(mask_depth_dir))\n",
    "else:\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.subplot(141)\n",
    "    plt.title('Original image')\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(142)\n",
    "    plt.title('Corrosion mask')\n",
    "    plt.imshow(Image.open(mask_corrosion_dir))\n",
    "    plt.subplot(143)\n",
    "    plt.title('Members mask')\n",
    "    plt.imshow(Image.open(mask_members_dir))\n",
    "    plt.subplot(144)\n",
    "    plt.title('Depth mask')\n",
    "    plt.imshow(Image.open(mask_depth_dir), cmap='gray')\n",
    "# plt.imshow(Image.open(mask_depth_dir))\n",
    "# org_img_pil\n",
    "# mask_corrosion_dir = None\n",
    "# mask_members_dir = None\n",
    "\n",
    "# mask_corrosion_pil = load_image(mask_corrosion_dir)\n",
    "# mask_members_pil = load_image(mask_members_dir)\n",
    "# print(mask_corrosion_pil.mode, mask_members_pil.mode)\n",
    "# mask_members_pil\n",
    "# print(np.array(mask).shape, np.unique(np.array(mask)))\n",
    "# mask_copy = mask.copy()\n",
    "# mask_copy_cvt_l = mask_copy.convert(\"L\")\n",
    "# mask_copy_cvt_l_np = np.array(mask_copy_cvt_l)\n",
    "# print(mask_copy_cvt_l_np.shape, np.unique(mask_copy_cvt_l_np))\n",
    "# mask_copy_cvt_l_resized = mask_copy_cvt_l.resize((512, 512))\n",
    "# print(mask_copy_cvt_l_resized.size)\n",
    "# mask_copy_cvt_l_t = transforms.ToTensor()(mask_copy_cvt_l_resized).to(device)\n",
    "# print(mask_copy_cvt_l_t.shape, mask_copy_cvt_l_t.unique())\n",
    "# mask_copy_cvt_l_resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = '/home/ubunto/Project/konglx/generate/ControlNet/models/stable-diffusion-v1-5'\n",
    "control_corrosion_trained_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_corrosion_inpainting_h-256_w-256_2024-07-12_09:10:44_seeds-2024/checkpoint-5700/controlnet'\n",
    "control_members_trained_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_members_inpainting_h-256_w-256_2024-07-12_14:52:34_seeds-2024/checkpoint-3800/controlnet'\n",
    "# control_depth_trained_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_depth_h-512_w-512_2024-07-16_11:25:38_seeds-2023/checkpoint-5700/controlnet'\n",
    "control_depth_trained_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_h-512_w-512_2024-07-16_17:42:27_seeds-2024/checkpoint-2000/controlnet'\n",
    "\n",
    "# controlnet=[controlnet_corrosion,controlnet_members, controlnet_depth]\n",
    "controlnet_conditioning_scale_list=[1.0,1.0,1.0]\n",
    "\n",
    "# ÊúâmembersÔºåÊó†corrosionÔºåÊó†depth\n",
    "if mask_members_dir is not None and mask_corrosion_dir is None and mask_depth_dir is None:\n",
    "    mask_members_pil = load_image(mask_members_dir)\n",
    "    validation_image = mask_members_pil\n",
    "    control_members_dir = control_members_trained_dir\n",
    "    print('mask_members_dir is not None and mask_corrosion_dir is None and mask_depth_dir is None')\n",
    "    controlnet_members = ControlNetModel.from_pretrained(control_members_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=controlnet_members).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    print(mask_members_pil.mode)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    # Image.show(image)\n",
    "# Êó†membersÔºåÊúâcorrosionÔºåÊó†depth\n",
    "elif mask_members_dir is  None and mask_corrosion_dir is not None and mask_depth_dir is None:\n",
    "    control_corrosion_dir = control_corrosion_trained_dir\n",
    "    mask_corrosion_pil = load_image(mask_corrosion_dir)\n",
    "    validation_image = mask_corrosion_pil\n",
    "    print('mask_members_dir is  None and mask_corrosion_dir is not None and mask_depth_dir is None')\n",
    "    controlnet_corrosion = ControlNetModel.from_pretrained(control_corrosion_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=controlnet_corrosion).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "# Êó†membersÔºåÊó†corrosionÔºåÊúâdepth  \n",
    "elif mask_members_dir is  None and mask_corrosion_dir is None and mask_depth_dir is not None:\n",
    "    # control_depth_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_depth_h-512_w-512_2024-07-16_11:25:38_seeds-2023/checkpoint-5700/controlnet'\n",
    "    control_depth_dir = control_depth_trained_dir\n",
    "    \n",
    "    mask_depth_pil = load_image(mask_depth_dir)\n",
    "    validation_image = mask_depth_pil\n",
    "    print('mask_members_dir is  None and mask_corrosion_dir is None and mask_depth_dir is not None')\n",
    "    controlnet_depth = ControlNetModel.from_pretrained(control_depth_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=controlnet_depth).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mask_depth_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "# ÊúâmembersÔºåÊúâcorrosionÔºåÊó†depth\n",
    "elif mask_members_dir is not None and mask_corrosion_dir is not None and mask_depth_dir is None:\n",
    "    control_corrosion_dir = control_corrosion_trained_dir\n",
    "    control_members_dir = control_members_trained_dir\n",
    "    mask_members_pil = load_image(mask_members_dir)\n",
    "    mask_corrosion_pil = load_image(mask_corrosion_dir)\n",
    "    validation_image = [mask_corrosion_pil, mask_members_pil]\n",
    "    print('mask_members_dir is not None and mask_corrosion_dir is not None and mask_depth_dir is None')\n",
    "    controlnet_corrosion = ControlNetModel.from_pretrained(control_corrosion_dir)\n",
    "    controlnet_members = ControlNetModel.from_pretrained(control_members_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=[controlnet_corrosion, controlnet_members]).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    print(mask_corrosion_pil.mode)\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    print(mask_members_pil.mode)\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(144)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "# Êó†membersÔºåÊúâcorrosionÔºåÊúâdepth \n",
    "elif mask_members_dir is  None and mask_corrosion_dir is not None and mask_depth_dir is not None:\n",
    "    control_corrosion_dir = control_corrosion_trained_dir\n",
    "    # control_depth_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_depth_h-512_w-512_2024-07-16_11:25:38_seeds-2023/checkpoint-5700/controlnet'\n",
    "    control_depth_dir = control_depth_trained_dir\n",
    "    \n",
    "    mask_depth_pil = load_image(mask_depth_dir)\n",
    "    mask_corrosion_pil = load_image(mask_corrosion_dir)\n",
    "    validation_image = [mask_corrosion_pil, mask_depth_pil]\n",
    "    print('mask_members_dir is  None and mask_corrosion_dir is not None and mask_depth_dir is not None')\n",
    "    controlnet_corrosion = ControlNetModel.from_pretrained(control_corrosion_dir)\n",
    "    controlnet_depth = ControlNetModel.from_pretrained(control_depth_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=[controlnet_corrosion, controlnet_depth]).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    print(mask_corrosion_pil.mode)\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(mask_depth_pil)\n",
    "    print(mask_depth_pil.mode)\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(144)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "# ÊúâmembersÔºåÊó†corrosionÔºåÊúâdepth\n",
    "elif mask_members_dir is not None and mask_corrosion_dir is None and mask_depth_dir is not None:\n",
    "    control_members_dir = control_members_trained_dir\n",
    "    # control_depth_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_depth_h-512_w-512_2024-07-16_11:25:38_seeds-2023/checkpoint-5700/controlnet'\n",
    "    control_depth_dir = control_depth_trained_dir\n",
    "    \n",
    "    mask_depth_pil = load_image(mask_depth_dir)\n",
    "    mask_members_pil = load_image(mask_members_dir)\n",
    "    validation_image = [mask_members_pil, mask_depth_pil]\n",
    "    print('mask_members_dir is not None and mask_corrosion_dir is None and mask_depth_dir is not None')\n",
    "    controlnet_members = ControlNetModel.from_pretrained(control_members_dir)\n",
    "    controlnet_depth = ControlNetModel.from_pretrained(control_depth_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=[controlnet_members, controlnet_depth]).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    print(mask_members_pil.mode)\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(mask_depth_pil)\n",
    "    print(mask_depth_pil.mode)\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(144)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "# ÊúâmembersÔºåÊúâcorrosionÔºåÊúâdepth\n",
    "elif mask_members_dir is not None and mask_corrosion_dir is not None and mask_depth_dir is not None:\n",
    "    control_corrosion_dir = control_corrosion_trained_dir\n",
    "    control_members_dir = control_members_trained_dir\n",
    "    # control_depth_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_depth_h-512_w-512_2024-07-16_11:25:38_seeds-2023/checkpoint-5700/controlnet'\n",
    "    control_depth_dir = control_depth_trained_dir\n",
    "    \n",
    "    mask_corrosion_pil = load_image(mask_corrosion_dir)\n",
    "    mask_members_pil = load_image(mask_members_dir)\n",
    "    mask_depth_pil = load_image(mask_depth_dir)\n",
    "    validation_image = [mask_corrosion_pil, mask_members_pil, mask_depth_pil]\n",
    "    print('mask_members_dir is not None and mask_corrosion_dir is not None and mask_depth_dir is not None')\n",
    "    controlnet_corrosion = ControlNetModel.from_pretrained(control_corrosion_dir)\n",
    "    controlnet_members = ControlNetModel.from_pretrained(control_members_dir)\n",
    "    controlnet_depth = ControlNetModel.from_pretrained(control_depth_dir)\n",
    "    pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=[controlnet_corrosion,controlnet_members, controlnet_depth], controlnet_conditioning_scale=controlnet_conditioning_scale_list).to(device)\n",
    "    generator = torch.Generator(device=device).manual_seed(seed)\n",
    "    image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                    ).images[0]\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(151)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    plt.subplot(152)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    print(mask_members_pil.mode)\n",
    "    plt.subplot(153)\n",
    "    plt.imshow(mask_depth_pil)\n",
    "    print(mask_depth_pil.mode)\n",
    "    plt.subplot(154)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(155)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "# Êó†membersÔºåÊó†corrosionÔºåÊó†depth\n",
    "else:\n",
    "    print('Input nothing')\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_corrosion_np = np.array(mask_corrosion_pil)\n",
    "mask_corrosion_np.min(), mask_corrosion_np.max(), mask_corrosion_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if mask_members_dir is not None and mask_corrosion_dir is None:\n",
    "\n",
    "#     plt.figure(figsize=(16, 8))\n",
    "#     plt.subplot(121)\n",
    "#     plt.imshow(mask_members_pil)\n",
    "#     plt.subplot(122)\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()\n",
    "#     # Image.show(image)\n",
    "    \n",
    "# elif mask_members_dir is  None and mask_corrosion_dir is not None:\n",
    "\n",
    "#     plt.figure(figsize=(16, 8))\n",
    "#     plt.subplot(121)\n",
    "#     plt.imshow(mask_corrosion_pil)\n",
    "#     plt.subplot(122)\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()\n",
    "# else:\n",
    "\n",
    "#     plt.figure(figsize=(16, 8))\n",
    "#     plt.subplot(131)\n",
    "#     plt.imshow(mask_corrosion_pil)\n",
    "#     plt.subplot(132)\n",
    "#     plt.imshow(mask_members_pil)\n",
    "#     plt.subplot(133)\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PNDMScheduler\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                ).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPMScheduler\n",
    "from diffusers import DDPMScheduler\n",
    "import os\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "scheduler_ddpm = DDPMScheduler.from_pretrained(os.path.join(config_dir, 'scheduler'))\n",
    "\n",
    "# print(pipeline.scheduler)\n",
    "pipeline.scheduler = scheduler_ddpm\n",
    "# print(pipeline.scheduler)\n",
    "image = pipeline(\n",
    "                   prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                ).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mask_members_dir is not None and mask_corrosion_dir is None:\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    # Image.show(image)\n",
    "    \n",
    "elif mask_members_dir is  None and mask_corrosion_dir is not None:\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "else:\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(144)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDIMScheduler\n",
    "from diffusers import DDIMScheduler\n",
    "import os\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "scheduler_ddim = DDIMScheduler.from_pretrained(os.path.join(config_dir, 'scheduler'))\n",
    "\n",
    "print(pipeline.scheduler)\n",
    "pipeline.scheduler = scheduler_ddim\n",
    "print(pipeline.scheduler)\n",
    "image = pipeline(\n",
    "                   prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                ).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PNDMScheduler\n",
    "from diffusers import PNDMScheduler\n",
    "import os\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "scheduler_pndm = PNDMScheduler.from_pretrained(os.path.join(config_dir, 'scheduler'))\n",
    "\n",
    "print(pipeline.scheduler)\n",
    "pipeline.scheduler = scheduler_pndm\n",
    "print(pipeline.scheduler)\n",
    "image = pipeline(\n",
    "                   prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                ).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***##step by step denoise##***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "control_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_crack_only_generate/checkpoint-2000/controlnet'\n",
    "config_dir = '/home/ubunto/Project/konglx/generate/ControlNet/models/stable-diffusion-v1-5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.ÂàõÂª∫ÂêÑ‰∏™ÈÉ®ÂàÜÁöÑÊ®°Âûã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler, ControlNetModel\n",
    "import os\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(config_dir, subfolder='vae', use_safetensors=None)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(config_dir, subfolder='tokenizer')\n",
    "text_encoder = CLIPTextModel.from_pretrained(config_dir, subfolder='text_encoder', use_safetensors=None)\n",
    "unet = UNet2DConditionModel.from_pretrained(config_dir, subfolder='unet', use_safetensors=None)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(control_dir)\n",
    "controlnet.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.config.scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UniPCMultistepScheduler\n",
    "\n",
    "scheduler_multistep = UniPCMultistepScheduler.from_pretrained(config_dir, subfolder=\"scheduler\")\n",
    "scheduler_multistep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = \"cuda\"\n",
    "vae.to(torch_device)\n",
    "text_encoder.to(torch_device)\n",
    "unet.to(torch_device)\n",
    "controlnet.to(torch_device)\n",
    "unet.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Create embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"crack\"]\n",
    "device = 'cuda'\n",
    "seed = 0\n",
    "height = 512  # default height of Stable Diffusion\n",
    "width = 512  # default width of Stable Diffusion\n",
    "num_inference_steps = 25  # Number of denoising steps\n",
    "guidance_scale = 7.5  # Scale for classifier-free guidance\n",
    "generator = torch.Generator(device=device).manual_seed(seed)  # Seed generator to create the initial latent noise\n",
    "batch_size = len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tokenizer(\n",
    "    prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "prompt_embeds = text_embeddings\n",
    "encoder_hidden_states_control = text_embeddings\n",
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "mask_dir = '/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/crackÔºàÂ§ç‰ª∂Ôºâ/SegmentationClass/DeepCrack_11240-6.png'\n",
    "validation_image = Image.open(mask_dir).convert(\"RGB\")\n",
    "validation_image.size\n",
    "\n",
    "conditioning_image_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(512),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "conditioning_image_pil = validation_image.resize([512, 512])\n",
    "conditioning_pixel_values = torch.stack([conditioning_image_transforms(conditioning_image_pil)])\n",
    "conditioning_pixel_values = conditioning_pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "print(conditioning_pixel_values.shape)\n",
    "controlnet_image = conditioning_pixel_values.to(torch_device)\n",
    "# print(controlnet_image)\n",
    "conditioning_image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You‚Äôll also need to generate the unconditional text embeddings which are the embeddings for the padding token.\n",
    "These need to have the same shape (batch_size and seq_length) as the conditional text_embeddings:\n",
    "'''\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "uncond_embeddings.shape, text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let‚Äôs concatenate the conditional and unconditional embeddings into a batch to avoid doing two forward passes:\n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "# text_embeddings\n",
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Create random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Next, generate some initial random noise as a starting point for the diffusion process. \n",
    "This is the latent representation of the image, and it‚Äôll be gradually denoised. \n",
    "At this point, the latent image is smaller than the final image size but that‚Äôs okay though \n",
    "because the model will transform it into the final 512x512 image dimensions later.\n",
    "'''\n",
    "\n",
    "# The height and width are divided by 8 because the vae model has 3 down-sampling layers.\n",
    "# You can check by running the following:   2 ** (len(vae.config.block_out_channels) - 1) == 8\n",
    "\n",
    "do_classifier_free_guidance = False\n",
    "guess_mode = False\n",
    "\n",
    "latents = torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    "    device=torch_device,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "                    latents,\n",
    "                    2,\n",
    "                    encoder_hidden_states=encoder_hidden_states_control,\n",
    "                    controlnet_cond=controlnet_image,\n",
    "                    return_dict=False,\n",
    "                )\n",
    "len(down_block_res_samples), down_block_res_samples[-1].shape, len(mid_block_res_sample), mid_block_res_sample[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Denoise the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Start by scaling the input with the initial noise distribution, sigma, the noise scale value, \n",
    "which is required for improved schedulers like UniPCMultistepScheduler: \n",
    "'''\n",
    "print(scheduler_multistep.init_noise_sigma)\n",
    "latents = latents * scheduler_multistep.init_noise_sigma\n",
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "The last step is to create the denoising loop that‚Äôll progressively \n",
    "transform the pure noise in latents to an image described by your prompt.\n",
    "Remember, the denoising loop needs to do three things:\n",
    "\n",
    "1.Set the scheduler‚Äôs timesteps to use during denoising.\n",
    "2.Iterate over the timesteps.\n",
    "3.At each timestep, call the UNet model to predict the noise residual and \n",
    "pass it to the scheduler to compute the previous noisy sample.\n",
    "\n",
    "'''\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "scheduler_multistep.set_timesteps(num_inference_steps)\n",
    "\n",
    "for t in tqdm(scheduler_multistep.timesteps):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "    latent_model_input = scheduler_multistep.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler_multistep.step(noise_pred, t, latents).prev_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "pil_latents = transforms.ToPILImage()(latents.squeeze(0))\n",
    "pil_latents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Decode the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and decode the image latents with vae\n",
    "latents_scaled = 1 / vae.config.scaling_factor * latents\n",
    "# pil_latents_scaled = transforms.ToPILImage()(latents.squeeze(0))\n",
    "# pil_latents_scaled = transforms.ToPILImage()(latents_scaled.squeeze(0))\n",
    "with torch.no_grad():\n",
    "    # image = vae.decode(latents).sample\n",
    "    image = vae.decode(latents_scaled).sample\n",
    "print(image.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (image / 2 + 0.5).clamp(0, 1).squeeze()\n",
    "image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n",
    "image = Image.fromarray(image)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "The last step is to create the denoising loop that‚Äôll progressively \n",
    "transform the pure noise in latents to an image described by your prompt.\n",
    "Remember, the denoising loop needs to do three things:\n",
    "\n",
    "1.Set the scheduler‚Äôs timesteps to use during denoising.\n",
    "2.Iterate over the timesteps.\n",
    "3.At each timestep, call the UNet model to predict the noise residual and \n",
    "pass it to the scheduler to compute the previous noisy sample.\n",
    "\n",
    "'''\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "scheduler_multistep.set_timesteps(num_inference_steps)\n",
    "\n",
    "for t in tqdm(scheduler.timesteps):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    # expand the latents if we are doing classifier free guidance\n",
    "    latent_model_input_control = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "    # latent_model_input_control = torch.cat([latents_control] * 2)\n",
    "\n",
    "    latent_model_input_control = scheduler.scale_model_input(latent_model_input_control, timestep=t)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        noise_pred_control = unet(latent_model_input_control, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond_control, noise_pred_text_control = noise_pred_control.chunk(2)\n",
    "    noise_pred_control = noise_pred_uncond_control+ guidance_scale * (noise_pred_text_control - noise_pred_uncond_control)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler.step(noise_pred_control, t, latents).prev_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = any\n",
    "a in 'abc'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
