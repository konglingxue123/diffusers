{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubunto/software/miniconda3/envs/dif/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/ubunto/software/miniconda3/envs/dif/lib/python3.9/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DDPMPipeline, DDIMPipeline, DDIMScheduler\n",
    "from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n",
    "from diffusers.utils import load_image\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import PIL\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ''\n",
    "\n",
    "device = 'cuda'\n",
    "seed = 2024\n",
    "noisy_sample = torch.randn(\n",
    "    1, 4, 64, 64\n",
    ").to(device)\n",
    "# noisy_sample = torch.randn(\n",
    "#     1, 4, 512, 512\n",
    "# ).to('cuda:0')\n",
    "# noisy_sample = torch.randn(\n",
    "#     1, 4, 56, 56\n",
    "# ).to('cuda:0')\n",
    "# prompt.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample(sample, i):\n",
    "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
    "    image_processed = (image_processed + 1.0) * 127.5\n",
    "    image_processed = image_processed.numpy().astype(np.uint8)\n",
    "\n",
    "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
    "    display(f\"Image at step {i}\")\n",
    "    display(image_pil)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 填充腐蚀区域（前提要有members的mask）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def times(x, y):\n",
    "    return x * y\n",
    "type(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# mask_members_np_demo = np.array(Image.open(mask_members_dir))\n",
    "# print(mask_members_np_demo.shape)\n",
    "\n",
    "random.seed(4)\n",
    "\n",
    "########### 定义颜色转换方法 ##############\n",
    "import imgviz\n",
    "\n",
    "def colored_mask(mask, save_path=None):\n",
    "    lbl_pil = Image.fromarray(mask.astype(np.uint8), mode=\"P\")\n",
    "    colormap = imgviz.label_colormap()\n",
    "    # print(colormap)\n",
    "    lbl_pil.putpalette(colormap.flatten())\n",
    "    if save_path is not None:\n",
    "        lbl_pil.save(save_path)\n",
    "\n",
    "    return lbl_pil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "img_dir = '/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/members/images'\n",
    "img_dir_list = os.listdir(img_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'controlnet_conditioning_scale': [1.0, 1.0, 1.0]} are not expected by StableDiffusionControlNetPipeline and will be ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_members_dir is not None and mask_corrosion_dir is not None and mask_depth_dir is not None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  4.90it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.34it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.57it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.56it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.56it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.55it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.56it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.56it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.54it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.53it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.55it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.51it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.53it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.47it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.55it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.52it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.53it/s]\n",
      "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.53it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.52it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.53it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.53it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.36it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.44it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.42it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.42it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.43it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.43it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.44it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.45it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.43it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.41it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.42it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.45it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.46it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.45it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.44it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.45it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.45it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.45it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.44it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.43it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.42it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.42it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.42it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.44it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.44it/s]\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.44it/s]\n",
      "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
      "100%|██████████| 20/20 [00:03<00:00,  6.44it/s]\n"
     ]
    }
   ],
   "source": [
    "config_dir = '/home/ubunto/Project/konglx/generate/ControlNet/models/stable-diffusion-v1-5'\n",
    "control_corrosion_trained_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_corrosion_inpainting_h-256_w-256_2024-07-12_09:10:44_seeds-2024/checkpoint-5700/controlnet'\n",
    "control_members_trained_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_members_inpainting_h-256_w-256_2024-07-12_14:52:34_seeds-2024/checkpoint-3800/controlnet'\n",
    "# control_depth_trained_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_depth_h-512_w-512_2024-07-16_11:25:38_seeds-2023/checkpoint-5700/controlnet'\n",
    "control_depth_trained_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_h-512_w-512_2024-07-16_17:42:27_seeds-2024/checkpoint-2000/controlnet'\n",
    "\n",
    "# controlnet=[controlnet_corrosion,controlnet_members, controlnet_depth]\n",
    "\n",
    "import os\n",
    "# img_dir = '/home/ubunto/Project/konglx/generate/ControlNet-v1-1-nightly/training/corrosion_and_crack/corrosion/JPEGImages'\n",
    "img_dir_list = os.listdir(img_dir)\n",
    "save_dir = '/home/ubunto/Project/konglx/generate/diffusers/datasets/for_testing/org_corrosion_members_depth_generated_0'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "# len(img_dir_list)\n",
    "controlnet_conditioning_scale_list=[1.0,1.0,1.0]\n",
    "\n",
    "control_corrosion_dir = control_corrosion_trained_dir\n",
    "control_members_dir = control_members_trained_dir\n",
    "# control_depth_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_depth_depth_h-512_w-512_2024-07-16_11:25:38_seeds-2023/checkpoint-5700/controlnet'\n",
    "control_depth_dir = control_depth_trained_dir\n",
    "print('mask_members_dir is not None and mask_corrosion_dir is not None and mask_depth_dir is not None')\n",
    "controlnet_corrosion = ControlNetModel.from_pretrained(control_corrosion_dir)\n",
    "controlnet_members = ControlNetModel.from_pretrained(control_members_dir)\n",
    "controlnet_depth = ControlNetModel.from_pretrained(control_depth_dir)\n",
    "pipeline = StableDiffusionControlNetPipeline.from_pretrained(config_dir, controlnet=[controlnet_corrosion,controlnet_members, controlnet_depth], controlnet_conditioning_scale=controlnet_conditioning_scale_list).to(device)\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "\n",
    "\n",
    "for img_name in img_dir_list:\n",
    "    img_name = img_name.split('.')[0]\n",
    "    mask_corrosion_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/corrosion/conditioning_images/{img_name}.png'\n",
    "    mask_members_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/members/conditioning_images/{img_name}.png'\n",
    "    # mask_members_dir = None\n",
    "    mask_depth_dir = f'/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/depth/conditioning_images/{img_name}.png'\n",
    "    if mask_members_dir is not None and mask_corrosion_dir is not None and mask_depth_dir is not None:\n",
    "        try:\n",
    "            mask_corrosion_pil = load_image(mask_corrosion_dir)\n",
    "            mask_members_pil = load_image(mask_members_dir)\n",
    "            mask_depth_pil = load_image(mask_depth_dir)\n",
    "            validation_image = [mask_corrosion_pil, mask_members_pil, mask_depth_pil]\n",
    "            image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                            ).images[0]\n",
    "            image.save(os.path.join(save_dir, f'{img_name}.jpg'))\n",
    "        # else:\n",
    "        #     print(img_name)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "        continue\n",
    "        # plt.figure(figsize=(20, 5))\n",
    "        # plt.subplot(151)\n",
    "        # plt.imshow(mask_corrosion_pil)\n",
    "        # plt.subplot(152)\n",
    "        # plt.imshow(mask_members_pil)\n",
    "        # print(mask_members_pil.mode)\n",
    "        # plt.subplot(153)\n",
    "        # plt.imshow(mask_depth_pil)\n",
    "        # print(mask_depth_pil.mode)\n",
    "        # plt.subplot(154)\n",
    "        # plt.imshow(org_img_pil)\n",
    "        # plt.subplot(155)\n",
    "        # plt.imshow(image)\n",
    "        # plt.show()\n",
    "    # 无members，无corrosion，无depth\n",
    "    else:\n",
    "        print('Input nothing')\n",
    "    # image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_corrosion_np = np.array(mask_corrosion_pil)\n",
    "mask_corrosion_np.min(), mask_corrosion_np.max(), mask_corrosion_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if mask_members_dir is not None and mask_corrosion_dir is None:\n",
    "\n",
    "#     plt.figure(figsize=(16, 8))\n",
    "#     plt.subplot(121)\n",
    "#     plt.imshow(mask_members_pil)\n",
    "#     plt.subplot(122)\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()\n",
    "#     # Image.show(image)\n",
    "    \n",
    "# elif mask_members_dir is  None and mask_corrosion_dir is not None:\n",
    "\n",
    "#     plt.figure(figsize=(16, 8))\n",
    "#     plt.subplot(121)\n",
    "#     plt.imshow(mask_corrosion_pil)\n",
    "#     plt.subplot(122)\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()\n",
    "# else:\n",
    "\n",
    "#     plt.figure(figsize=(16, 8))\n",
    "#     plt.subplot(131)\n",
    "#     plt.imshow(mask_corrosion_pil)\n",
    "#     plt.subplot(132)\n",
    "#     plt.imshow(mask_members_pil)\n",
    "#     plt.subplot(133)\n",
    "#     plt.imshow(image)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PNDMScheduler\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "image = pipeline(prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                ).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPMScheduler\n",
    "from diffusers import DDPMScheduler\n",
    "import os\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "scheduler_ddpm = DDPMScheduler.from_pretrained(os.path.join(config_dir, 'scheduler'))\n",
    "\n",
    "# print(pipeline.scheduler)\n",
    "pipeline.scheduler = scheduler_ddpm\n",
    "# print(pipeline.scheduler)\n",
    "image = pipeline(\n",
    "                   prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                ).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mask_members_dir is not None and mask_corrosion_dir is None:\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "    # Image.show(image)\n",
    "    \n",
    "elif mask_members_dir is  None and mask_corrosion_dir is not None:\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(131)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "else:\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.subplot(141)\n",
    "    plt.imshow(mask_corrosion_pil)\n",
    "    plt.subplot(142)\n",
    "    plt.imshow(mask_members_pil)\n",
    "    plt.subplot(143)\n",
    "    plt.imshow(org_img_pil)\n",
    "    plt.subplot(144)\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDIMScheduler\n",
    "from diffusers import DDIMScheduler\n",
    "import os\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "scheduler_ddim = DDIMScheduler.from_pretrained(os.path.join(config_dir, 'scheduler'))\n",
    "\n",
    "print(pipeline.scheduler)\n",
    "pipeline.scheduler = scheduler_ddim\n",
    "print(pipeline.scheduler)\n",
    "image = pipeline(\n",
    "                   prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                ).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PNDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PNDMScheduler\n",
    "from diffusers import PNDMScheduler\n",
    "import os\n",
    "generator = torch.Generator(device=device).manual_seed(seed)\n",
    "scheduler_pndm = PNDMScheduler.from_pretrained(os.path.join(config_dir, 'scheduler'))\n",
    "\n",
    "print(pipeline.scheduler)\n",
    "pipeline.scheduler = scheduler_pndm\n",
    "print(pipeline.scheduler)\n",
    "image = pipeline(\n",
    "                   prompt, validation_image, num_inference_steps=20, generator=generator\n",
    "                ).images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***##step by step denoise##***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "control_dir = '/home/ubunto/Project/konglx/generate/diffusers/examples/controlnet/controlnet-model_crack_only_generate/checkpoint-2000/controlnet'\n",
    "config_dir = '/home/ubunto/Project/konglx/generate/ControlNet/models/stable-diffusion-v1-5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.创建各个部分的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler, ControlNetModel\n",
    "import os\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(config_dir, subfolder='vae', use_safetensors=None)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(config_dir, subfolder='tokenizer')\n",
    "text_encoder = CLIPTextModel.from_pretrained(config_dir, subfolder='text_encoder', use_safetensors=None)\n",
    "unet = UNet2DConditionModel.from_pretrained(config_dir, subfolder='unet', use_safetensors=None)\n",
    "\n",
    "controlnet = ControlNetModel.from_pretrained(control_dir)\n",
    "controlnet.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.config.scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UniPCMultistepScheduler\n",
    "\n",
    "scheduler_multistep = UniPCMultistepScheduler.from_pretrained(config_dir, subfolder=\"scheduler\")\n",
    "scheduler_multistep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_device = \"cuda\"\n",
    "vae.to(torch_device)\n",
    "text_encoder.to(torch_device)\n",
    "unet.to(torch_device)\n",
    "controlnet.to(torch_device)\n",
    "unet.config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Create embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Create text embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"crack\"]\n",
    "device = 'cuda'\n",
    "seed = 0\n",
    "height = 512  # default height of Stable Diffusion\n",
    "width = 512  # default width of Stable Diffusion\n",
    "num_inference_steps = 25  # Number of denoising steps\n",
    "guidance_scale = 7.5  # Scale for classifier-free guidance\n",
    "generator = torch.Generator(device=device).manual_seed(seed)  # Seed generator to create the initial latent noise\n",
    "batch_size = len(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tokenizer(\n",
    "    prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]\n",
    "\n",
    "prompt_embeds = text_embeddings\n",
    "encoder_hidden_states_control = text_embeddings\n",
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Create image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "mask_dir = '/home/ubunto/Project/konglx/generate/diffusers/datasets/corrosion_and_crack/crack（复件）/SegmentationClass/DeepCrack_11240-6.png'\n",
    "validation_image = Image.open(mask_dir).convert(\"RGB\")\n",
    "validation_image.size\n",
    "\n",
    "conditioning_image_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(512, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "            transforms.CenterCrop(512),\n",
    "            transforms.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "conditioning_image_pil = validation_image.resize([512, 512])\n",
    "conditioning_pixel_values = torch.stack([conditioning_image_transforms(conditioning_image_pil)])\n",
    "conditioning_pixel_values = conditioning_pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "print(conditioning_pixel_values.shape)\n",
    "controlnet_image = conditioning_pixel_values.to(torch_device)\n",
    "# print(controlnet_image)\n",
    "conditioning_image_pil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "You’ll also need to generate the unconditional text embeddings which are the embeddings for the padding token.\n",
    "These need to have the same shape (batch_size and seq_length) as the conditional text_embeddings:\n",
    "'''\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "uncond_input = tokenizer([\"\"] * batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
    "uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]\n",
    "uncond_embeddings.shape, text_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s concatenate the conditional and unconditional embeddings into a batch to avoid doing two forward passes:\n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "# text_embeddings\n",
    "text_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Create random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Next, generate some initial random noise as a starting point for the diffusion process. \n",
    "This is the latent representation of the image, and it’ll be gradually denoised. \n",
    "At this point, the latent image is smaller than the final image size but that’s okay though \n",
    "because the model will transform it into the final 512x512 image dimensions later.\n",
    "'''\n",
    "\n",
    "# The height and width are divided by 8 because the vae model has 3 down-sampling layers.\n",
    "# You can check by running the following:   2 ** (len(vae.config.block_out_channels) - 1) == 8\n",
    "\n",
    "do_classifier_free_guidance = False\n",
    "guess_mode = False\n",
    "\n",
    "latents = torch.randn(\n",
    "    (batch_size, unet.config.in_channels, height // 8, width // 8),\n",
    "    generator=generator,\n",
    "    device=torch_device,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "down_block_res_samples, mid_block_res_sample = controlnet(\n",
    "                    latents,\n",
    "                    2,\n",
    "                    encoder_hidden_states=encoder_hidden_states_control,\n",
    "                    controlnet_cond=controlnet_image,\n",
    "                    return_dict=False,\n",
    "                )\n",
    "len(down_block_res_samples), down_block_res_samples[-1].shape, len(mid_block_res_sample), mid_block_res_sample[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Denoise the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Start by scaling the input with the initial noise distribution, sigma, the noise scale value, \n",
    "which is required for improved schedulers like UniPCMultistepScheduler: \n",
    "'''\n",
    "print(scheduler_multistep.init_noise_sigma)\n",
    "latents = latents * scheduler_multistep.init_noise_sigma\n",
    "latents.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "The last step is to create the denoising loop that’ll progressively \n",
    "transform the pure noise in latents to an image described by your prompt.\n",
    "Remember, the denoising loop needs to do three things:\n",
    "\n",
    "1.Set the scheduler’s timesteps to use during denoising.\n",
    "2.Iterate over the timesteps.\n",
    "3.At each timestep, call the UNet model to predict the noise residual and \n",
    "pass it to the scheduler to compute the previous noisy sample.\n",
    "\n",
    "'''\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "scheduler_multistep.set_timesteps(num_inference_steps)\n",
    "\n",
    "for t in tqdm(scheduler_multistep.timesteps):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    latent_model_input = torch.cat([latents] * 2)\n",
    "\n",
    "    latent_model_input = scheduler_multistep.scale_model_input(latent_model_input, timestep=t)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler_multistep.step(noise_pred, t, latents).prev_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "pil_latents = transforms.ToPILImage()(latents.squeeze(0))\n",
    "pil_latents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.Decode the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale and decode the image latents with vae\n",
    "latents_scaled = 1 / vae.config.scaling_factor * latents\n",
    "# pil_latents_scaled = transforms.ToPILImage()(latents.squeeze(0))\n",
    "# pil_latents_scaled = transforms.ToPILImage()(latents_scaled.squeeze(0))\n",
    "with torch.no_grad():\n",
    "    # image = vae.decode(latents).sample\n",
    "    image = vae.decode(latents_scaled).sample\n",
    "print(image.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (image / 2 + 0.5).clamp(0, 1).squeeze()\n",
    "image = (image.permute(1, 2, 0) * 255).to(torch.uint8).cpu().numpy()\n",
    "image = Image.fromarray(image)\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "The last step is to create the denoising loop that’ll progressively \n",
    "transform the pure noise in latents to an image described by your prompt.\n",
    "Remember, the denoising loop needs to do three things:\n",
    "\n",
    "1.Set the scheduler’s timesteps to use during denoising.\n",
    "2.Iterate over the timesteps.\n",
    "3.At each timestep, call the UNet model to predict the noise residual and \n",
    "pass it to the scheduler to compute the previous noisy sample.\n",
    "\n",
    "'''\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "scheduler_multistep.set_timesteps(num_inference_steps)\n",
    "\n",
    "for t in tqdm(scheduler.timesteps):\n",
    "    # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
    "    # expand the latents if we are doing classifier free guidance\n",
    "    latent_model_input_control = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "    # latent_model_input_control = torch.cat([latents_control] * 2)\n",
    "\n",
    "    latent_model_input_control = scheduler.scale_model_input(latent_model_input_control, timestep=t)\n",
    "\n",
    "    # predict the noise residual\n",
    "    with torch.no_grad():\n",
    "        noise_pred_control = unet(latent_model_input_control, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "    # perform guidance\n",
    "    noise_pred_uncond_control, noise_pred_text_control = noise_pred_control.chunk(2)\n",
    "    noise_pred_control = noise_pred_uncond_control+ guidance_scale * (noise_pred_text_control - noise_pred_uncond_control)\n",
    "\n",
    "    # compute the previous noisy sample x_t -> x_t-1\n",
    "    latents = scheduler.step(noise_pred_control, t, latents).prev_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = any\n",
    "a in 'abc'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dif",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
